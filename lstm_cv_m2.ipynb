{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "774db566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Re-loads all imports every time the cell is ran. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ea1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eb0b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerConsumptionDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        Data is resampled to hourly intervals.\n",
    "        Both 'np.nan' and '?' are converted to 'np.nan'\n",
    "        'Date' and 'Time' columns are merged into 'dt' index\n",
    "        '''\n",
    "\n",
    "        if stage == 'fit' and self.X_train is not None:\n",
    "            return \n",
    "        if stage == 'test' and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:  \n",
    "            return\n",
    "        \n",
    "        path = '/kaggle/input/electric-power-consumption-data-set/household_power_consumption.txt'\n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            path, \n",
    "            sep=';', \n",
    "            parse_dates={'dt' : ['Date', 'Time']}, \n",
    "            infer_datetime_format=True, \n",
    "            low_memory=False, \n",
    "            na_values=['nan','?'], \n",
    "            index_col='dt'\n",
    "        )\n",
    "\n",
    "        df_resample = df.resample('h').mean()\n",
    "\n",
    "        X = df_resample.dropna().copy()\n",
    "        y = X['Global_active_power'].shift(-1).ffill()\n",
    "        self.columns = X.columns\n",
    "\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cce9bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log('train_loss', loss)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss)\n",
    "        return result\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult()\n",
    "        result.log('test_loss', loss)\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e301e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "'''\n",
    "\n",
    "p = dict(\n",
    "    seq_len = 24,\n",
    "    batch_size = 70, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 10,\n",
    "    n_features = 7,\n",
    "    hidden_size = 100,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9fdb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_everything(1)\n",
    "\n",
    "# csv_logger = CSVLogger('./', name='lstm', version='0'),\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=p['max_epochs'],\n",
    "#     logger=csv_logger,\n",
    "# )\n",
    "\n",
    "# model = LSTMRegressor(\n",
    "#     n_features = p['n_features'],\n",
    "#     hidden_size = p['hidden_size'],\n",
    "#     seq_len = p['seq_len'],\n",
    "#     batch_size = p['batch_size'],\n",
    "#     criterion = p['criterion'],\n",
    "#     num_layers = p['num_layers'],\n",
    "#     dropout = p['dropout'],\n",
    "#     learning_rate = p['learning_rate']\n",
    "# )\n",
    "\n",
    "# dm = PowerConsumptionDataModule(\n",
    "#     seq_len = p['seq_len'],\n",
    "#     batch_size = p['batch_size']\n",
    "# )\n",
    "\n",
    "# trainer.fit(model)\n",
    "# trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20151317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = pd.read_csv('./lstm/0/metrics.csv')\n",
    "# train_loss = metrics[['train_loss', 'step', 'epoch']][~np.isnan(metrics['train_loss'])]\n",
    "# val_loss = metrics[['val_loss', 'epoch']][~np.isnan(metrics['val_loss'])]\n",
    "# test_loss = metrics['test_loss'].iloc[-1]\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "# axes[0].set_title('Train loss per batch')\n",
    "# axes[0].plot(train_loss['step'], train_loss['train_loss'])\n",
    "# axes[1].set_title('Validation loss per epoch')\n",
    "# axes[1].plot(val_loss['epoch'], val_loss['val_loss'], color='orange')\n",
    "# plt.show(block = True)\n",
    "\n",
    "# print('MSE:')\n",
    "# print(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\n",
    "# print(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\n",
    "# print(f'Test loss:  {test_loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
