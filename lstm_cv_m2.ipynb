{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774db566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... | hidden... | learni... | weight... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.5128  \u001b[0m | \u001b[0m0.1873   \u001b[0m | \u001b[0m245.0    \u001b[0m | \u001b[0m0.007347 \u001b[0m | \u001b[0m0.0006027\u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-0.5131  \u001b[0m | \u001b[0m0.07801  \u001b[0m | \u001b[0m66.94    \u001b[0m | \u001b[0m0.000675 \u001b[0m | \u001b[0m0.0008675\u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-0.5096  \u001b[0m | \u001b[95m0.3006   \u001b[0m | \u001b[95m190.6    \u001b[0m | \u001b[95m0.0003038\u001b[0m | \u001b[95m0.0009702\u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.5176  \u001b[0m | \u001b[0m0.4162   \u001b[0m | \u001b[0m79.56    \u001b[0m | \u001b[0m0.0019   \u001b[0m | \u001b[0m0.0001916\u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m-0.5042  \u001b[0m | \u001b[95m0.1521   \u001b[0m | \u001b[95m149.5    \u001b[0m | \u001b[95m0.004376 \u001b[0m | \u001b[95m0.0002983\u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m-0.5037  \u001b[0m | \u001b[95m0.2182   \u001b[0m | \u001b[95m162.3    \u001b[0m | \u001b[95m0.005269 \u001b[0m | \u001b[95m0.0003376\u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.5059  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m32.0     \u001b[0m | \u001b[0m0.007962 \u001b[0m | \u001b[0m0.0007266\u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.5133  \u001b[0m | \u001b[0m0.1011   \u001b[0m | \u001b[0m161.1    \u001b[0m | \u001b[0m0.00362  \u001b[0m | \u001b[0m0.0006121\u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.5081  \u001b[0m | \u001b[0m0.4854   \u001b[0m | \u001b[0m191.0    \u001b[0m | \u001b[0m0.009939 \u001b[0m | \u001b[0m0.0003802\u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.5037  \u001b[0m | \u001b[0m0.2634   \u001b[0m | \u001b[0m162.9    \u001b[0m | \u001b[0m0.005371 \u001b[0m | \u001b[0m0.0001136\u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.5045  \u001b[0m | \u001b[0m0.2694   \u001b[0m | \u001b[0m150.7    \u001b[0m | \u001b[0m0.006583 \u001b[0m | \u001b[0m0.000379 \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.5096  \u001b[0m | \u001b[0m0.4174   \u001b[0m | \u001b[0m148.0    \u001b[0m | \u001b[0m0.009427 \u001b[0m | \u001b[0m0.0004597\u001b[0m |\n",
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.5036  \u001b[0m | \u001b[95m0.01531  \u001b[0m | \u001b[95m152.2    \u001b[0m | \u001b[95m0.009254 \u001b[0m | \u001b[95m0.0006946\u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.5119  \u001b[0m | \u001b[0m0.4801   \u001b[0m | \u001b[0m153.5    \u001b[0m | \u001b[0m0.002237 \u001b[0m | \u001b[0m0.0009668\u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.507   \u001b[0m | \u001b[0m0.4855   \u001b[0m | \u001b[0m151.7    \u001b[0m | \u001b[0m0.005339 \u001b[0m | \u001b[0m0.0007121\u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.506   \u001b[0m | \u001b[0m0.004451 \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m0.005169 \u001b[0m | \u001b[0m0.0003817\u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.5106  \u001b[0m | \u001b[0m0.4222   \u001b[0m | \u001b[0m33.29    \u001b[0m | \u001b[0m0.008511 \u001b[0m | \u001b[0m0.0003696\u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.5136  \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m165.4    \u001b[0m | \u001b[0m0.009768 \u001b[0m | \u001b[0m0.0005364\u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.5137  \u001b[0m | \u001b[0m0.4868   \u001b[0m | \u001b[0m150.0    \u001b[0m | \u001b[0m0.006993 \u001b[0m | \u001b[0m0.0003546\u001b[0m |\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.5007  \u001b[0m | \u001b[95m0.1823   \u001b[0m | \u001b[95m96.23    \u001b[0m | \u001b[95m0.006953 \u001b[0m | \u001b[95m0.0006707\u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.5092  \u001b[0m | \u001b[0m0.4934   \u001b[0m | \u001b[0m167.5    \u001b[0m | \u001b[0m0.009015 \u001b[0m | \u001b[0m0.0007595\u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.5109  \u001b[0m | \u001b[0m0.357    \u001b[0m | \u001b[0m199.4    \u001b[0m | \u001b[0m0.0005288\u001b[0m | \u001b[0m0.0004087\u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.5042  \u001b[0m | \u001b[0m0.4227   \u001b[0m | \u001b[0m192.8    \u001b[0m | \u001b[0m0.003024 \u001b[0m | \u001b[0m0.0005453\u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.5038  \u001b[0m | \u001b[0m0.0422   \u001b[0m | \u001b[0m213.3    \u001b[0m | \u001b[0m0.006946 \u001b[0m | \u001b[0m0.0006091\u001b[0m |\n",
      "| \u001b[95m25       \u001b[0m | \u001b[95m-0.4953  \u001b[0m | \u001b[95m0.3963   \u001b[0m | \u001b[95m250.2    \u001b[0m | \u001b[95m0.007312 \u001b[0m | \u001b[95m4.827e-05\u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.5069  \u001b[0m | \u001b[0m0.4036   \u001b[0m | \u001b[0m250.2    \u001b[0m | \u001b[0m0.004976 \u001b[0m | \u001b[0m0.0005941\u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.5117  \u001b[0m | \u001b[0m0.02921  \u001b[0m | \u001b[0m60.86    \u001b[0m | \u001b[0m0.005549 \u001b[0m | \u001b[0m0.0008709\u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.5081  \u001b[0m | \u001b[0m0.4737   \u001b[0m | \u001b[0m167.2    \u001b[0m | \u001b[0m0.00177  \u001b[0m | \u001b[0m0.0004458\u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.5111  \u001b[0m | \u001b[0m0.2084   \u001b[0m | \u001b[0m96.24    \u001b[0m | \u001b[0m0.007964 \u001b[0m | \u001b[0m0.0005545\u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.5029  \u001b[0m | \u001b[0m0.03071  \u001b[0m | \u001b[0m147.6    \u001b[0m | \u001b[0m0.004328 \u001b[0m | \u001b[0m0.0009534\u001b[0m |\n",
      "=========================================================================\n",
      "{'target': -0.4952677607536316, 'params': {'dropout_prob': 0.3962914362699862, 'hidden_size': 250.18524275247105, 'learning_rate': 0.00731207923918078, 'weight_decay': 4.827393850836503e-05}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_and_validate() missing 4 required positional arguments: 'hidden_size', 'learning_rate', 'weight_decay', and 'dropout_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m X_train, X_val \u001b[39m=\u001b[39m X[train_indices], X[val_indices]\n\u001b[1;32m    142\u001b[0m y_train, y_val \u001b[39m=\u001b[39m y[train_indices], y[val_indices]\n\u001b[0;32m--> 144\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device)\n\u001b[1;32m    145\u001b[0m train_losses_all\u001b[39m.\u001b[39mappend(train_losses)\n\u001b[1;32m    146\u001b[0m val_losses_all\u001b[39m.\u001b[39mappend(val_losses)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_and_validate() missing 4 required positional arguments: 'hidden_size', 'learning_rate', 'weight_decay', and 'dropout_prob'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "num_layers = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate sinusoidal time series data\n",
    "def generate_sin_wave(freq, num_samples, sample_rate):\n",
    "    x = np.linspace(0, num_samples / sample_rate, num_samples)\n",
    "    y = np.sin(2 * np.pi * freq * x)\n",
    "    return y\n",
    "\n",
    "# Create input-target pairs for the LSTM model\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        seq = data[i : (i + seq_length)]\n",
    "        X.append(seq)\n",
    "        target = data[i + seq_length]\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define the LSTM model with dropout\n",
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Generate sinusoidal time series data\n",
    "freq = 2\n",
    "num_samples = 200\n",
    "sample_rate = 10\n",
    "data = generate_sin_wave(freq, num_samples, sample_rate)\n",
    "\n",
    "# Prepare the data for the LSTM model\n",
    "seq_length = 20\n",
    "X, y = create_sequences(data, seq_length)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device, hidden_size, learning_rate, weight_decay, dropout_prob):\n",
    "    hidden_size = int(hidden_size)\n",
    "    input_size = X_train.shape[2]\n",
    "    output_size = y_train.shape[1]\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    model = TimeSeriesLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob=dropout_prob).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Create a function to optimize using Bayesian optimization\n",
    "def optimize_lstm(hidden_size, learning_rate, weight_decay, dropout_prob):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_epochs = 5\n",
    "    k_folds = 5\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    val_losses_all = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "        _, val_losses = train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device, hidden_size, learning_rate, weight_decay, dropout_prob)\n",
    "        val_losses_all.append(val_losses)\n",
    "\n",
    "    # Return the mean validation loss across all folds\n",
    "    return -np.mean([losses[-1] for losses in val_losses_all])\n",
    "\n",
    "# Define the bounds for hyperparameters\n",
    "pbounds = {\n",
    "    \"hidden_size\": (32, 256),\n",
    "    \"learning_rate\": (1e-4, 1e-2),\n",
    "    \"weight_decay\": (1e-5, 1e-3),\n",
    "    \"dropout_prob\": (0.0, 0.5),\n",
    "}\n",
    "\n",
    "# Create the Bayesian optimization object\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_lstm,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(optimizer.max)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_epochs = 5\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "train_losses_all = []\n",
    "val_losses_all = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    train_losses, val_losses = train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device)\n",
    "    train_losses_all.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "\n",
    "    print(f\"Fold {fold + 1}: Final Training Loss: {train_losses[-1]:.4f}, Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(k_folds):\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses_all[i], label=f\"Training Loss (Fold {i + 1})\")\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses_all[i], label=f\"Validation Loss (Fold {i + 1})\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dfc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.11.2)",
   "language": "python",
   "name": "3.11.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
