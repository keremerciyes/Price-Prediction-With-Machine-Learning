{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774db566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generate sinusoidal time series data\n",
    "def generate_sin_wave(freq, num_samples, sample_rate):\n",
    "    x = np.linspace(0, num_samples / sample_rate, num_samples)\n",
    "    y = np.sin(2 * np.pi * freq * x)\n",
    "    return y\n",
    "\n",
    "# Create input-target pairs for the LSTM model\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        seq = data[i : (i + seq_length)]\n",
    "        X.append(seq)\n",
    "        target = data[i + seq_length]\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define the LSTM model with dropout\n",
    "class TimeSeriesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(TimeSeriesLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Generate sinusoidal time series data\n",
    "freq = 2\n",
    "num_samples = 200\n",
    "sample_rate = 10\n",
    "data = generate_sin_wave(freq, num_samples, sample_rate)\n",
    "\n",
    "# Prepare the data for the LSTM model\n",
    "seq_length = 20\n",
    "X, y = create_sequences(data, seq_length)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device, hidden_size, learning_rate, weight_decay, dropout_prob):\n",
    "    hidden_size = int(hidden_size)\n",
    "    input_size = X_train.shape[2]\n",
    "    output_size = y_train.shape[1]\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "    model = TimeSeriesLSTM(input_size, hidden_size, num_layers, output_size, dropout_prob=dropout_prob).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Create a function to optimize using Bayesian optimization\n",
    "def optimize_lstm(hidden_size, learning_rate, weight_decay, dropout_prob):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_epochs = 100\n",
    "    k_folds = 5\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    val_losses_all = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "        _, val_losses = train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device, hidden_size, learning_rate, weight_decay, dropout_prob)\n",
    "        val_losses_all.append(val_losses)\n",
    "\n",
    "    # Return the mean validation loss across all folds\n",
    "    return -np.mean([losses[-1] for losses in val_losses_all])\n",
    "\n",
    "# Define the bounds for hyperparameters\n",
    "pbounds = {\n",
    "    \"hidden_size\": (32, 256),\n",
    "    \"learning_rate\": (1e-4, 1e-2),\n",
    "    \"weight_decay\": (1e-5, 1e-3),\n",
    "    \"dropout_prob\": (0.0, 0.5),\n",
    "}\n",
    "\n",
    "# Create the Bayesian optimization object\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_lstm,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(optimizer.max)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_epochs = 100\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "train_losses_all = []\n",
    "val_losses_all = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "    train_losses, val_losses = train_and_validate(X_train, y_train, X_val, y_val, num_epochs, device)\n",
    "    train_losses_all.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "\n",
    "    print(f\"Fold {fold + 1}: Final Training Loss: {train_losses[-1]:.4f}, Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(k_folds):\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses_all[i], label=f\"Training Loss (Fold {i + 1})\")\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses_all[i], label=f\"Validation Loss (Fold {i + 1})\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab0181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: | "
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge bayesian-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dfc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
