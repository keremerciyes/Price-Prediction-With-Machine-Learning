{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Passengers<br>Month=%{x}<br>Passengers=%{y}<extra></extra>",
         "legendgroup": "Passengers",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Passengers",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "1949-01-01T00:00:00",
          "1949-02-01T00:00:00",
          "1949-03-01T00:00:00",
          "1949-04-01T00:00:00",
          "1949-05-01T00:00:00",
          "1949-06-01T00:00:00",
          "1949-07-01T00:00:00",
          "1949-08-01T00:00:00",
          "1949-09-01T00:00:00",
          "1949-10-01T00:00:00",
          "1949-11-01T00:00:00",
          "1949-12-01T00:00:00",
          "1950-01-01T00:00:00",
          "1950-02-01T00:00:00",
          "1950-03-01T00:00:00",
          "1950-04-01T00:00:00",
          "1950-05-01T00:00:00",
          "1950-06-01T00:00:00",
          "1950-07-01T00:00:00",
          "1950-08-01T00:00:00",
          "1950-09-01T00:00:00",
          "1950-10-01T00:00:00",
          "1950-11-01T00:00:00",
          "1950-12-01T00:00:00",
          "1951-01-01T00:00:00",
          "1951-02-01T00:00:00",
          "1951-03-01T00:00:00",
          "1951-04-01T00:00:00",
          "1951-05-01T00:00:00",
          "1951-06-01T00:00:00",
          "1951-07-01T00:00:00",
          "1951-08-01T00:00:00",
          "1951-09-01T00:00:00",
          "1951-10-01T00:00:00",
          "1951-11-01T00:00:00",
          "1951-12-01T00:00:00",
          "1952-01-01T00:00:00",
          "1952-02-01T00:00:00",
          "1952-03-01T00:00:00",
          "1952-04-01T00:00:00",
          "1952-05-01T00:00:00",
          "1952-06-01T00:00:00",
          "1952-07-01T00:00:00",
          "1952-08-01T00:00:00",
          "1952-09-01T00:00:00",
          "1952-10-01T00:00:00",
          "1952-11-01T00:00:00",
          "1952-12-01T00:00:00",
          "1953-01-01T00:00:00",
          "1953-02-01T00:00:00",
          "1953-03-01T00:00:00",
          "1953-04-01T00:00:00",
          "1953-05-01T00:00:00",
          "1953-06-01T00:00:00",
          "1953-07-01T00:00:00",
          "1953-08-01T00:00:00",
          "1953-09-01T00:00:00",
          "1953-10-01T00:00:00",
          "1953-11-01T00:00:00",
          "1953-12-01T00:00:00",
          "1954-01-01T00:00:00",
          "1954-02-01T00:00:00",
          "1954-03-01T00:00:00",
          "1954-04-01T00:00:00",
          "1954-05-01T00:00:00",
          "1954-06-01T00:00:00",
          "1954-07-01T00:00:00",
          "1954-08-01T00:00:00",
          "1954-09-01T00:00:00",
          "1954-10-01T00:00:00",
          "1954-11-01T00:00:00",
          "1954-12-01T00:00:00",
          "1955-01-01T00:00:00",
          "1955-02-01T00:00:00",
          "1955-03-01T00:00:00",
          "1955-04-01T00:00:00",
          "1955-05-01T00:00:00",
          "1955-06-01T00:00:00",
          "1955-07-01T00:00:00",
          "1955-08-01T00:00:00",
          "1955-09-01T00:00:00",
          "1955-10-01T00:00:00",
          "1955-11-01T00:00:00",
          "1955-12-01T00:00:00",
          "1956-01-01T00:00:00",
          "1956-02-01T00:00:00",
          "1956-03-01T00:00:00",
          "1956-04-01T00:00:00",
          "1956-05-01T00:00:00",
          "1956-06-01T00:00:00",
          "1956-07-01T00:00:00",
          "1956-08-01T00:00:00",
          "1956-09-01T00:00:00",
          "1956-10-01T00:00:00",
          "1956-11-01T00:00:00",
          "1956-12-01T00:00:00",
          "1957-01-01T00:00:00",
          "1957-02-01T00:00:00",
          "1957-03-01T00:00:00",
          "1957-04-01T00:00:00",
          "1957-05-01T00:00:00",
          "1957-06-01T00:00:00",
          "1957-07-01T00:00:00",
          "1957-08-01T00:00:00",
          "1957-09-01T00:00:00",
          "1957-10-01T00:00:00",
          "1957-11-01T00:00:00",
          "1957-12-01T00:00:00",
          "1958-01-01T00:00:00",
          "1958-02-01T00:00:00",
          "1958-03-01T00:00:00",
          "1958-04-01T00:00:00",
          "1958-05-01T00:00:00",
          "1958-06-01T00:00:00",
          "1958-07-01T00:00:00",
          "1958-08-01T00:00:00",
          "1958-09-01T00:00:00",
          "1958-10-01T00:00:00",
          "1958-11-01T00:00:00",
          "1958-12-01T00:00:00",
          "1959-01-01T00:00:00",
          "1959-02-01T00:00:00",
          "1959-03-01T00:00:00",
          "1959-04-01T00:00:00",
          "1959-05-01T00:00:00",
          "1959-06-01T00:00:00",
          "1959-07-01T00:00:00",
          "1959-08-01T00:00:00",
          "1959-09-01T00:00:00",
          "1959-10-01T00:00:00",
          "1959-11-01T00:00:00",
          "1959-12-01T00:00:00",
          "1960-01-01T00:00:00",
          "1960-02-01T00:00:00",
          "1960-03-01T00:00:00",
          "1960-04-01T00:00:00",
          "1960-05-01T00:00:00",
          "1960-06-01T00:00:00",
          "1960-07-01T00:00:00",
          "1960-08-01T00:00:00",
          "1960-09-01T00:00:00",
          "1960-10-01T00:00:00",
          "1960-11-01T00:00:00",
          "1960-12-01T00:00:00"
         ],
         "xaxis": "x",
         "y": [
          112,
          118,
          132,
          129,
          121,
          135,
          148,
          148,
          136,
          119,
          104,
          118,
          115,
          126,
          141,
          135,
          125,
          149,
          170,
          170,
          158,
          133,
          114,
          140,
          145,
          150,
          178,
          163,
          172,
          178,
          199,
          199,
          184,
          162,
          146,
          166,
          171,
          180,
          193,
          181,
          183,
          218,
          230,
          242,
          209,
          191,
          172,
          194,
          196,
          196,
          236,
          235,
          229,
          243,
          264,
          272,
          237,
          211,
          180,
          201,
          204,
          188,
          235,
          227,
          234,
          264,
          302,
          293,
          259,
          229,
          203,
          229,
          242,
          233,
          267,
          269,
          270,
          315,
          364,
          347,
          312,
          274,
          237,
          278,
          284,
          277,
          317,
          313,
          318,
          374,
          413,
          405,
          355,
          306,
          271,
          306,
          315,
          301,
          356,
          348,
          355,
          422,
          465,
          467,
          404,
          347,
          305,
          336,
          340,
          318,
          362,
          348,
          363,
          435,
          491,
          505,
          404,
          359,
          310,
          337,
          360,
          342,
          406,
          396,
          420,
          472,
          548,
          559,
          463,
          407,
          362,
          405,
          417,
          391,
          419,
          461,
          472,
          535,
          622,
          606,
          508,
          461,
          390,
          432
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "orientation": "h",
         "title": {
          "text": ""
         },
         "tracegroupgap": 0,
         "y": 1.02
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "font": {
           "size": 18
          },
          "xaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          },
          "yaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Month"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Passengers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set fraction: 0.24475524475524477\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # How to use PyTorch LSTMs for time series regression\n",
    "\n",
    "# %% [markdown]\n",
    "# # Data\n",
    "\n",
    "# %% [markdown]\n",
    "# 1. Download the Air Passengers data.\n",
    "# 2. Load the Air Passengers data into a DataFrame.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data-sets/air_passengers.csv\", index_col=\"Month\", parse_dates=True)\n",
    "df.rename(columns={'#Passengers': 'Passengers'}, inplace=True)\n",
    "\n",
    "# %%\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "plot_template = dict(\n",
    "    layout=go.Layout({\n",
    "        \"font_size\": 18,\n",
    "        \"xaxis_title_font_size\": 24,\n",
    "        \"yaxis_title_font_size\": 24})\n",
    ")\n",
    "\n",
    "fig = px.line(df, labels=dict(index=\"Date\", value=\"Passengers\"))\n",
    "fig.update_layout(\n",
    "  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Create the target variable\n",
    "\n",
    "# %%\n",
    "forecast_lead = 1\n",
    "target = f\"lead{forecast_lead}\"\n",
    "\n",
    "df[target] = df[\"Passengers\"].shift(-forecast_lead)\n",
    "df = df.iloc[:-forecast_lead]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Create a hold-out test set and preprocess the data\n",
    "\n",
    "# %%\n",
    "test_start = \"1958-01-01\"\n",
    "\n",
    "df_train = df.loc[:test_start].copy()\n",
    "df_test = df.loc[test_start:].copy()\n",
    "\n",
    "print(\"Test set fraction:\", len(df_test) / len(df))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Standardize the features and target, based on the training set\n",
    "\n",
    "# %%\n",
    "target_mean = df_train[target].mean()\n",
    "target_stdev = df_train[target].std()\n",
    "\n",
    "df_train[\"Passengers\"] = (df_train[\"Passengers\"] - target_mean) / target_stdev\n",
    "df_test[\"Passengers\"] = (df_test[\"Passengers\"] - target_mean) / target_stdev\n",
    "\n",
    "df_train[target] = (df_train[target] - target_mean) / target_stdev\n",
    "df_test[target] = (df_test[target] - target_mean) / target_stdev\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Create datasets that PyTorch `DataLoader` can work with\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, sequence_length=5):\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[self.target].values).float()\n",
    "        self.X = torch.tensor(dataframe['Passengers'].values).float().unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]\n",
    "        \n",
    "# Continue the rest of the code as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([4, 3, 1])\n",
      "Target shape: torch.Size([4])\n",
      "Untrained test\n",
      "--------\n",
      "Test loss: 6.016740865177578\n",
      "\n",
      "Epoch 0\n",
      "---------\n",
      "Train loss: 1.0110322308859654\n",
      "Test loss: 5.241092880566915\n",
      "\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 0.8342921555574451\n",
      "Test loss: 4.372982925838894\n",
      "\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 0.6266789635909455\n",
      "Test loss: 3.103095041380988\n",
      "\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 0.390941378593977\n",
      "Test loss: 1.7885334077808592\n",
      "\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 0.20999515183003886\n",
      "Test loss: 1.0753844968146749\n",
      "\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 0.18094851846607135\n",
      "Test loss: 1.107842404809263\n",
      "\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 0.17907020727372064\n",
      "Test loss: 1.2174660621417894\n",
      "\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 0.1737264888943173\n",
      "Test loss: 1.1193751986655924\n",
      "\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 0.1740726484651012\n",
      "Test loss: 1.1198402245839436\n",
      "\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 0.1675821524362878\n",
      "Test loss: 1.1360574956569407\n",
      "\n",
      "Epoch 10\n",
      "---------\n",
      "Train loss: 0.1652771629659193\n",
      "Test loss: 1.1015375351740255\n",
      "\n",
      "Epoch 11\n",
      "---------\n",
      "Train loss: 0.16437772223226993\n",
      "Test loss: 1.1556203472945425\n",
      "\n",
      "Epoch 12\n",
      "---------\n",
      "Train loss: 0.16121434403716453\n",
      "Test loss: 1.1152752257055707\n",
      "\n",
      "Epoch 13\n",
      "---------\n",
      "Train loss: 0.16145253871634072\n",
      "Test loss: 1.111576658156183\n",
      "\n",
      "Epoch 14\n",
      "---------\n",
      "Train loss: 0.15854029324171798\n",
      "Test loss: 1.0188187207612727\n",
      "\n",
      "Epoch 15\n",
      "---------\n",
      "Train loss: 0.15556860228285327\n",
      "Test loss: 1.0687250701917543\n",
      "\n",
      "Epoch 16\n",
      "---------\n",
      "Train loss: 0.15531853185633995\n",
      "Test loss: 0.9945556525554922\n",
      "\n",
      "Epoch 17\n",
      "---------\n",
      "Train loss: 0.16941468344469154\n",
      "Test loss: 1.0191125439272986\n",
      "\n",
      "Epoch 18\n",
      "---------\n",
      "Train loss: 0.14577725237800873\n",
      "Test loss: 1.0179084291060765\n",
      "\n",
      "Epoch 19\n",
      "---------\n",
      "Train loss: 0.14395075452713563\n",
      "Test loss: 0.9967845777670542\n",
      "\n",
      "Epoch 20\n",
      "---------\n",
      "Train loss: 0.13955172545476152\n",
      "Test loss: 0.9116342407133844\n",
      "\n",
      "Epoch 21\n",
      "---------\n",
      "Train loss: 0.14345100135376146\n",
      "Test loss: 0.9177959478563733\n",
      "\n",
      "Epoch 22\n",
      "---------\n",
      "Train loss: 0.13777657252337253\n",
      "Test loss: 0.879197407927778\n",
      "\n",
      "Epoch 23\n",
      "---------\n",
      "Train loss: 0.1327418651697891\n",
      "Test loss: 0.8417993427978622\n",
      "\n",
      "Epoch 24\n",
      "---------\n",
      "Train loss: 0.12741445612859284\n",
      "Test loss: 0.8563580786188444\n",
      "\n",
      "Epoch 25\n",
      "---------\n",
      "Train loss: 0.13859047028901322\n",
      "Test loss: 0.78254534304142\n",
      "\n",
      "Epoch 26\n",
      "---------\n",
      "Train loss: 0.12731744276451146\n",
      "Test loss: 0.7580220128099123\n",
      "\n",
      "Epoch 27\n",
      "---------\n",
      "Train loss: 0.12517081658422416\n",
      "Test loss: 0.8102351700266203\n",
      "\n",
      "Epoch 28\n",
      "---------\n",
      "Train loss: 0.11763331147709064\n",
      "Test loss: 0.7565695353680186\n",
      "\n",
      "Epoch 29\n",
      "---------\n",
      "Train loss: 0.12214708506196205\n",
      "Test loss: 0.6321694105863571\n",
      "\n",
      "Epoch 30\n",
      "---------\n",
      "Train loss: 0.108113763512977\n",
      "Test loss: 0.6805026282866796\n",
      "\n",
      "Epoch 31\n",
      "---------\n",
      "Train loss: 0.10689053501586646\n",
      "Test loss: 0.6281609584887823\n",
      "\n",
      "Epoch 32\n",
      "---------\n",
      "Train loss: 0.10239837078344342\n",
      "Test loss: 0.6603353081478013\n",
      "\n",
      "Epoch 33\n",
      "---------\n",
      "Train loss: 0.10300803157900061\n",
      "Test loss: 0.6300052718983756\n",
      "\n",
      "Epoch 34\n",
      "---------\n",
      "Train loss: 0.1000619612833751\n",
      "Test loss: 0.6734387692477968\n",
      "\n",
      "Epoch 35\n",
      "---------\n",
      "Train loss: 0.10517003593434181\n",
      "Test loss: 0.6246950286957953\n",
      "\n",
      "Epoch 36\n",
      "---------\n",
      "Train loss: 0.10261032916605473\n",
      "Test loss: 0.5738468451632394\n",
      "\n",
      "Epoch 37\n",
      "---------\n",
      "Train loss: 0.09524547621341688\n",
      "Test loss: 0.6053906662596596\n",
      "\n",
      "Epoch 38\n",
      "---------\n",
      "Train loss: 0.09174921602243558\n",
      "Test loss: 0.5994438247548209\n",
      "\n",
      "Epoch 39\n",
      "---------\n",
      "Train loss: 0.08804801498938884\n",
      "Test loss: 0.5547367897298601\n",
      "\n",
      "Epoch 40\n",
      "---------\n",
      "Train loss: 0.08905987266916782\n",
      "Test loss: 0.5245968807074759\n",
      "\n",
      "Epoch 41\n",
      "---------\n",
      "Train loss: 0.08665156324527093\n",
      "Test loss: 0.5240273127953211\n",
      "\n",
      "Epoch 42\n",
      "---------\n",
      "Train loss: 0.08830924712986286\n",
      "Test loss: 0.5514618986182742\n",
      "\n",
      "Epoch 43\n",
      "---------\n",
      "Train loss: 0.09764228712966931\n",
      "Test loss: 0.4877849875224961\n",
      "\n",
      "Epoch 44\n",
      "---------\n",
      "Train loss: 0.08338147291215137\n",
      "Test loss: 0.4891906976699829\n",
      "\n",
      "Epoch 45\n",
      "---------\n",
      "Train loss: 0.08211608733316618\n",
      "Test loss: 0.503828801214695\n",
      "\n",
      "Epoch 46\n",
      "---------\n",
      "Train loss: 0.08956650691938453\n",
      "Test loss: 0.4385969299409125\n",
      "\n",
      "Epoch 47\n",
      "---------\n",
      "Train loss: 0.0816518162776317\n",
      "Test loss: 0.4620264114605056\n",
      "\n",
      "Epoch 48\n",
      "---------\n",
      "Train loss: 0.08231662972165006\n",
      "Test loss: 0.4611094246308009\n",
      "\n",
      "Epoch 49\n",
      "---------\n",
      "Train loss: 0.08116858040115663\n",
      "Test loss: 0.5106127634644508\n",
      "\n",
      "Epoch 50\n",
      "---------\n",
      "Train loss: 0.08105592136936528\n",
      "Test loss: 0.4922473877668381\n",
      "\n",
      "Epoch 51\n",
      "---------\n",
      "Train loss: 0.0801902087405324\n",
      "Test loss: 0.5225219950079918\n",
      "\n",
      "Epoch 52\n",
      "---------\n",
      "Train loss: 0.07531915752250436\n",
      "Test loss: 0.43408046745591694\n",
      "\n",
      "Epoch 53\n",
      "---------\n",
      "Train loss: 0.0759331237724317\n",
      "Test loss: 0.4761446590224902\n",
      "\n",
      "Epoch 54\n",
      "---------\n",
      "Train loss: 0.09635429275970507\n",
      "Test loss: 0.47953252163198257\n",
      "\n",
      "Epoch 55\n",
      "---------\n",
      "Train loss: 0.07995496959691602\n",
      "Test loss: 0.39564187990294564\n",
      "\n",
      "Epoch 56\n",
      "---------\n",
      "Train loss: 0.07343115449683475\n",
      "Test loss: 0.5065071715248955\n",
      "\n",
      "Epoch 57\n",
      "---------\n",
      "Train loss: 0.0763220576502915\n",
      "Test loss: 0.499618224799633\n",
      "\n",
      "Epoch 58\n",
      "---------\n",
      "Train loss: 0.0766798111310761\n",
      "Test loss: 0.4354226456748115\n",
      "\n",
      "Epoch 59\n",
      "---------\n",
      "Train loss: 0.07381799484470061\n",
      "Test loss: 0.5416479590866301\n",
      "\n",
      "Epoch 60\n",
      "---------\n",
      "Train loss: 0.07377519717972193\n",
      "Test loss: 0.46354320562548107\n",
      "\n",
      "Epoch 61\n",
      "---------\n",
      "Train loss: 0.07292921039541918\n",
      "Test loss: 0.4757886578639348\n",
      "\n",
      "Epoch 62\n",
      "---------\n",
      "Train loss: 0.07601547823287547\n",
      "Test loss: 0.5070012129015393\n",
      "\n",
      "Epoch 63\n",
      "---------\n",
      "Train loss: 0.07270824064367584\n",
      "Test loss: 0.41926511211527717\n",
      "\n",
      "Epoch 64\n",
      "---------\n",
      "Train loss: 0.07705174469655114\n",
      "Test loss: 0.44700324204232955\n",
      "\n",
      "Epoch 65\n",
      "---------\n",
      "Train loss: 0.07056273164094559\n",
      "Test loss: 0.4376055631372664\n",
      "\n",
      "Epoch 66\n",
      "---------\n",
      "Train loss: 0.07058534821096275\n",
      "Test loss: 0.39109047998984653\n",
      "\n",
      "Epoch 67\n",
      "---------\n",
      "Train loss: 0.07463952280314905\n",
      "Test loss: 0.4959224479066001\n",
      "\n",
      "Epoch 68\n",
      "---------\n",
      "Train loss: 0.06845937445593465\n",
      "Test loss: 0.403017847902245\n",
      "\n",
      "Epoch 69\n",
      "---------\n",
      "Train loss: 0.07140076290150839\n",
      "Test loss: 0.4078502804040909\n",
      "\n",
      "Epoch 70\n",
      "---------\n",
      "Train loss: 0.07098229295973267\n",
      "Test loss: 0.49938436183664536\n",
      "\n",
      "Epoch 71\n",
      "---------\n",
      "Train loss: 0.0714778729182269\n",
      "Test loss: 0.498362617360221\n",
      "\n",
      "Epoch 72\n",
      "---------\n",
      "Train loss: 0.06839292286895216\n",
      "Test loss: 0.4208928081724379\n",
      "\n",
      "Epoch 73\n",
      "---------\n",
      "Train loss: 0.06839505651233983\n",
      "Test loss: 0.4204004771179623\n",
      "\n",
      "Epoch 74\n",
      "---------\n",
      "Train loss: 0.06809158754809427\n",
      "Test loss: 0.4094383335775799\n",
      "\n",
      "Epoch 75\n",
      "---------\n",
      "Train loss: 0.06885903361346989\n",
      "Test loss: 0.4235762920644548\n",
      "\n",
      "Epoch 76\n",
      "---------\n",
      "Train loss: 0.07091568939254753\n",
      "Test loss: 0.44660985966523487\n",
      "\n",
      "Epoch 77\n",
      "---------\n",
      "Train loss: 0.07284078231480505\n",
      "Test loss: 0.43309282925393844\n",
      "\n",
      "Epoch 78\n",
      "---------\n",
      "Train loss: 0.07253186654166452\n",
      "Test loss: 0.4394070886903339\n",
      "\n",
      "Epoch 79\n",
      "---------\n",
      "Train loss: 0.06895907479338348\n",
      "Test loss: 0.4477349254820082\n",
      "\n",
      "Epoch 80\n",
      "---------\n",
      "Train loss: 0.06998081343148702\n",
      "Test loss: 0.4624301294485728\n",
      "\n",
      "Epoch 81\n",
      "---------\n",
      "Train loss: 0.06697090708517603\n",
      "Test loss: 0.4483078039354748\n",
      "\n",
      "Epoch 82\n",
      "---------\n",
      "Train loss: 0.07081415537478668\n",
      "Test loss: 0.4263758924272325\n",
      "\n",
      "Epoch 83\n",
      "---------\n",
      "Train loss: 0.06987038737029902\n",
      "Test loss: 0.4710242913828956\n",
      "\n",
      "Epoch 84\n",
      "---------\n",
      "Train loss: 0.07097724911623768\n",
      "Test loss: 0.4857442196872499\n",
      "\n",
      "Epoch 85\n",
      "---------\n",
      "Train loss: 0.06811433172385607\n",
      "Test loss: 0.45087817973560756\n",
      "\n",
      "Epoch 86\n",
      "---------\n",
      "Train loss: 0.06737026260400723\n",
      "Test loss: 0.47192464106612736\n",
      "\n",
      "Epoch 87\n",
      "---------\n",
      "Train loss: 0.07173727772065572\n",
      "Test loss: 0.45923107034630245\n",
      "\n",
      "Epoch 88\n",
      "---------\n",
      "Train loss: 0.0689103858271015\n",
      "Test loss: 0.4706526497999827\n",
      "\n",
      "Epoch 89\n",
      "---------\n",
      "Train loss: 0.07443913639456566\n",
      "Test loss: 0.5091082718637254\n",
      "\n",
      "Epoch 90\n",
      "---------\n",
      "Train loss: 0.07003671722486615\n",
      "Test loss: 0.44025807744926876\n",
      "\n",
      "Epoch 91\n",
      "---------\n",
      "Train loss: 0.07271071705534789\n",
      "Test loss: 0.4492529531319936\n",
      "\n",
      "Epoch 92\n",
      "---------\n",
      "Train loss: 0.07461472833529115\n",
      "Test loss: 0.4073234217034446\n",
      "\n",
      "Epoch 93\n",
      "---------\n",
      "Train loss: 0.0694292819160702\n",
      "Test loss: 0.40634529458151925\n",
      "\n",
      "Epoch 94\n",
      "---------\n",
      "Train loss: 0.0710980283495571\n",
      "Test loss: 0.39831630223327213\n",
      "\n",
      "Epoch 95\n",
      "---------\n",
      "Train loss: 0.06725008922096874\n",
      "Test loss: 0.4500835140546163\n",
      "\n",
      "Epoch 96\n",
      "---------\n",
      "Train loss: 0.0680883670325524\n",
      "Test loss: 0.42824477785163456\n",
      "\n",
      "Epoch 97\n",
      "---------\n",
      "Train loss: 0.07593286908896905\n",
      "Test loss: 0.4704824967516793\n",
      "\n",
      "Epoch 98\n",
      "---------\n",
      "Train loss: 0.07386633487684387\n",
      "Test loss: 0.42845556802219814\n",
      "\n",
      "Epoch 99\n",
      "---------\n",
      "Train loss: 0.07148401547289852\n",
      "Test loss: 0.5359422912200292\n",
      "\n",
      "Epoch 100\n",
      "---------\n",
      "Train loss: 0.06739369287554707\n",
      "Test loss: 0.43367693656020695\n",
      "\n",
      "Epoch 101\n",
      "---------\n",
      "Train loss: 0.07626045089481133\n",
      "Test loss: 0.5183474040693707\n",
      "\n",
      "Epoch 102\n",
      "---------\n",
      "Train loss: 0.06768162711523473\n",
      "Test loss: 0.478059611386723\n",
      "\n",
      "Epoch 103\n",
      "---------\n",
      "Train loss: 0.07077834948099085\n",
      "Test loss: 0.4750853743818071\n",
      "\n",
      "Epoch 104\n",
      "---------\n",
      "Train loss: 0.06856144089916986\n",
      "Test loss: 0.4598108066452874\n",
      "\n",
      "Epoch 105\n",
      "---------\n",
      "Train loss: 0.07191646401770413\n",
      "Test loss: 0.3990598950121138\n",
      "\n",
      "Epoch 106\n",
      "---------\n",
      "Train loss: 0.06948406373638656\n",
      "Test loss: 0.4154475149181154\n",
      "\n",
      "Epoch 107\n",
      "---------\n",
      "Train loss: 0.06786417375717844\n",
      "Test loss: 0.45036564932929146\n",
      "\n",
      "Epoch 108\n",
      "---------\n",
      "Train loss: 0.06804143597504922\n",
      "Test loss: 0.44301369455125594\n",
      "\n",
      "Epoch 109\n",
      "---------\n",
      "Train loss: 0.06609203837277684\n",
      "Test loss: 0.4307812733782662\n",
      "\n",
      "Epoch 110\n",
      "---------\n",
      "Train loss: 0.06750603715356972\n",
      "Test loss: 0.43285893897215527\n",
      "\n",
      "Epoch 111\n",
      "---------\n",
      "Train loss: 0.06792548707952457\n",
      "Test loss: 0.4401704685555564\n",
      "\n",
      "Epoch 112\n",
      "---------\n",
      "Train loss: 0.06685645944838013\n",
      "Test loss: 0.4081955850124359\n",
      "\n",
      "Epoch 113\n",
      "---------\n",
      "Train loss: 0.06737523402158072\n",
      "Test loss: 0.44483256340026855\n",
      "\n",
      "Epoch 114\n",
      "---------\n",
      "Train loss: 0.06672169921720135\n",
      "Test loss: 0.4417520397239261\n",
      "\n",
      "Epoch 115\n",
      "---------\n",
      "Train loss: 0.06582440634208199\n",
      "Test loss: 0.43834564917617375\n",
      "\n",
      "Epoch 116\n",
      "---------\n",
      "Train loss: 0.06810945412144065\n",
      "Test loss: 0.45853575070699054\n",
      "\n",
      "Epoch 117\n",
      "---------\n",
      "Train loss: 0.06727202144351654\n",
      "Test loss: 0.4414787193139394\n",
      "\n",
      "Epoch 118\n",
      "---------\n",
      "Train loss: 0.06626608029806189\n",
      "Test loss: 0.45026543239752453\n",
      "\n",
      "Epoch 119\n",
      "---------\n",
      "Train loss: 0.06637853314168751\n",
      "Test loss: 0.429690549770991\n",
      "\n",
      "Epoch 120\n",
      "---------\n",
      "Train loss: 0.07165147204484258\n",
      "Test loss: 0.45189738604757523\n",
      "\n",
      "Epoch 121\n",
      "---------\n",
      "Train loss: 0.07034686912915536\n",
      "Test loss: 0.4619721919298172\n",
      "\n",
      "Epoch 122\n",
      "---------\n",
      "Train loss: 0.069247781897762\n",
      "Test loss: 0.44688570333851707\n",
      "\n",
      "Epoch 123\n",
      "---------\n",
      "Train loss: 0.06675911469834059\n",
      "Test loss: 0.46374160713619655\n",
      "\n",
      "Epoch 124\n",
      "---------\n",
      "Train loss: 0.06789155683613249\n",
      "Test loss: 0.43744612899091506\n",
      "\n",
      "Epoch 125\n",
      "---------\n",
      "Train loss: 0.07144261963133301\n",
      "Test loss: 0.4510779662264718\n",
      "\n",
      "Epoch 126\n",
      "---------\n",
      "Train loss: 0.06965284893522039\n",
      "Test loss: 0.424195784661505\n",
      "\n",
      "Epoch 127\n",
      "---------\n",
      "Train loss: 0.06693321460625157\n",
      "Test loss: 0.4753446661763721\n",
      "\n",
      "Epoch 128\n",
      "---------\n",
      "Train loss: 0.07505532964465342\n",
      "Test loss: 0.42215550276968217\n",
      "\n",
      "Epoch 129\n",
      "---------\n",
      "Train loss: 0.06630731309841005\n",
      "Test loss: 0.5770792812108994\n",
      "\n",
      "Epoch 130\n",
      "---------\n",
      "Train loss: 0.0656640279921703\n",
      "Test loss: 0.4106595483091142\n",
      "\n",
      "Epoch 131\n",
      "---------\n",
      "Train loss: 0.06704706133210234\n",
      "Test loss: 0.460433895389239\n",
      "\n",
      "Epoch 132\n",
      "---------\n",
      "Train loss: 0.07031553883903793\n",
      "Test loss: 0.4735453608963225\n",
      "\n",
      "Epoch 133\n",
      "---------\n",
      "Train loss: 0.06741421483457088\n",
      "Test loss: 0.4341013530890147\n",
      "\n",
      "Epoch 134\n",
      "---------\n",
      "Train loss: 0.06721515088741269\n",
      "Test loss: 0.4748917784955766\n",
      "\n",
      "Epoch 135\n",
      "---------\n",
      "Train loss: 0.06689387154100197\n",
      "Test loss: 0.4344285825888316\n",
      "\n",
      "Epoch 136\n",
      "---------\n",
      "Train loss: 0.0708914183113458\n",
      "Test loss: 0.4358738445573383\n",
      "\n",
      "Epoch 137\n",
      "---------\n",
      "Train loss: 0.06817206214847309\n",
      "Test loss: 0.4509158002005683\n",
      "\n",
      "Epoch 138\n",
      "---------\n",
      "Train loss: 0.06585824497597059\n",
      "Test loss: 0.44224026799201965\n",
      "\n",
      "Epoch 139\n",
      "---------\n",
      "Train loss: 0.07033859003734376\n",
      "Test loss: 0.4004630595445633\n",
      "\n",
      "Epoch 140\n",
      "---------\n",
      "Train loss: 0.06966830051637121\n",
      "Test loss: 0.4690733419524299\n",
      "\n",
      "Epoch 141\n",
      "---------\n",
      "Train loss: 0.06593515145193253\n",
      "Test loss: 0.37437064448992413\n",
      "\n",
      "Epoch 142\n",
      "---------\n",
      "Train loss: 0.06676076781254128\n",
      "Test loss: 0.43667618102497524\n",
      "\n",
      "Epoch 143\n",
      "---------\n",
      "Train loss: 0.06585050439200911\n",
      "Test loss: 0.41737143364217544\n",
      "\n",
      "Epoch 144\n",
      "---------\n",
      "Train loss: 0.06599909093763147\n",
      "Test loss: 0.4597538693083657\n",
      "\n",
      "Epoch 145\n",
      "---------\n",
      "Train loss: 0.06646671912832451\n",
      "Test loss: 0.4406130678123898\n",
      "\n",
      "Epoch 146\n",
      "---------\n",
      "Train loss: 0.07192096556536853\n",
      "Test loss: 0.46059682137436336\n",
      "\n",
      "Epoch 147\n",
      "---------\n",
      "Train loss: 0.07253701372870378\n",
      "Test loss: 0.4282509254084693\n",
      "\n",
      "Epoch 148\n",
      "---------\n",
      "Train loss: 0.06585319616715424\n",
      "Test loss: 0.4574689318736394\n",
      "\n",
      "Epoch 149\n",
      "---------\n",
      "Train loss: 0.06859679767096948\n",
      "Test loss: 0.4601907978455226\n",
      "\n",
      "Epoch 150\n",
      "---------\n",
      "Train loss: 0.06742774187919817\n",
      "Test loss: 0.5024226970142789\n",
      "\n",
      "Epoch 151\n",
      "---------\n",
      "Train loss: 0.06909218769786614\n",
      "Test loss: 0.39492061734199524\n",
      "\n",
      "Epoch 152\n",
      "---------\n",
      "Train loss: 0.06847767861160849\n",
      "Test loss: 0.4435847070482042\n",
      "\n",
      "Epoch 153\n",
      "---------\n",
      "Train loss: 0.07082619025771107\n",
      "Test loss: 0.39855921268463135\n",
      "\n",
      "Epoch 154\n",
      "---------\n",
      "Train loss: 0.06758721023132759\n",
      "Test loss: 0.4371645798285802\n",
      "\n",
      "Epoch 155\n",
      "---------\n",
      "Train loss: 0.06482396624368038\n",
      "Test loss: 0.3901367634534836\n",
      "\n",
      "Epoch 156\n",
      "---------\n",
      "Train loss: 0.0682540163397789\n",
      "Test loss: 0.40197374257776475\n",
      "\n",
      "Epoch 157\n",
      "---------\n",
      "Train loss: 0.06610391658198621\n",
      "Test loss: 0.4211156749063068\n",
      "\n",
      "Epoch 158\n",
      "---------\n",
      "Train loss: 0.07176870287262968\n",
      "Test loss: 0.43166031771235996\n",
      "\n",
      "Epoch 159\n",
      "---------\n",
      "Train loss: 0.06910752258928758\n",
      "Test loss: 0.49580035275883144\n",
      "\n",
      "Epoch 160\n",
      "---------\n",
      "Train loss: 0.06632676554311599\n",
      "Test loss: 0.43049058980411953\n",
      "\n",
      "Epoch 161\n",
      "---------\n",
      "Train loss: 0.06496626616353751\n",
      "Test loss: 0.42487848301728565\n",
      "\n",
      "Epoch 162\n",
      "---------\n",
      "Train loss: 0.07085295899638108\n",
      "Test loss: 0.4423225935962465\n",
      "\n",
      "Epoch 163\n",
      "---------\n",
      "Train loss: 0.06733813359876097\n",
      "Test loss: 0.4751250545183818\n",
      "\n",
      "Epoch 164\n",
      "---------\n",
      "Train loss: 0.06940406657356236\n",
      "Test loss: 0.4409746560785506\n",
      "\n",
      "Epoch 165\n",
      "---------\n",
      "Train loss: 0.0648313426605195\n",
      "Test loss: 0.4299055023325814\n",
      "\n",
      "Epoch 166\n",
      "---------\n",
      "Train loss: 0.06914337717794947\n",
      "Test loss: 0.47008876005808514\n",
      "\n",
      "Epoch 167\n",
      "---------\n",
      "Train loss: 0.06694197681333337\n",
      "Test loss: 0.48281412985589767\n",
      "\n",
      "Epoch 168\n",
      "---------\n",
      "Train loss: 0.06776070302086216\n",
      "Test loss: 0.4096234854724672\n",
      "\n",
      "Epoch 169\n",
      "---------\n",
      "Train loss: 0.06678946569029774\n",
      "Test loss: 0.4438081880410512\n",
      "\n",
      "Epoch 170\n",
      "---------\n",
      "Train loss: 0.06590449893181878\n",
      "Test loss: 0.4143256064918306\n",
      "\n",
      "Epoch 171\n",
      "---------\n",
      "Train loss: 0.06615995871730515\n",
      "Test loss: 0.44053111142582363\n",
      "\n",
      "Epoch 172\n",
      "---------\n",
      "Train loss: 0.07101549304622624\n",
      "Test loss: 0.3992974989944034\n",
      "\n",
      "Epoch 173\n",
      "---------\n",
      "Train loss: 0.06560863505728776\n",
      "Test loss: 0.4656309501992332\n",
      "\n",
      "Epoch 174\n",
      "---------\n",
      "Train loss: 0.07178718144340175\n",
      "Test loss: 0.4724900755617354\n",
      "\n",
      "Epoch 175\n",
      "---------\n",
      "Train loss: 0.0659751897411687\n",
      "Test loss: 0.3956159187687768\n",
      "\n",
      "Epoch 176\n",
      "---------\n",
      "Train loss: 0.06530387828076657\n",
      "Test loss: 0.46875186098946464\n",
      "\n",
      "Epoch 177\n",
      "---------\n",
      "Train loss: 0.06766680651344359\n",
      "Test loss: 0.45610305666923523\n",
      "\n",
      "Epoch 178\n",
      "---------\n",
      "Train loss: 0.06748634093258131\n",
      "Test loss: 0.3677134182718065\n",
      "\n",
      "Epoch 179\n",
      "---------\n",
      "Train loss: 0.07218375213311187\n",
      "Test loss: 0.4245242344008552\n",
      "\n",
      "Epoch 180\n",
      "---------\n",
      "Train loss: 0.06905793495077107\n",
      "Test loss: 0.40545998679267037\n",
      "\n",
      "Epoch 181\n",
      "---------\n",
      "Train loss: 0.06691307621076703\n",
      "Test loss: 0.42923929293950397\n",
      "\n",
      "Epoch 182\n",
      "---------\n",
      "Train loss: 0.07129936542228929\n",
      "Test loss: 0.48886721167299485\n",
      "\n",
      "Epoch 183\n",
      "---------\n",
      "Train loss: 0.06478399909766656\n",
      "Test loss: 0.427909544772572\n",
      "\n",
      "Epoch 184\n",
      "---------\n",
      "Train loss: 0.0659334522242716\n",
      "Test loss: 0.4399651288986206\n",
      "\n",
      "Epoch 185\n",
      "---------\n",
      "Train loss: 0.06608624754673136\n",
      "Test loss: 0.45187436872058445\n",
      "\n",
      "Epoch 186\n",
      "---------\n",
      "Train loss: 0.0653996887350721\n",
      "Test loss: 0.4474011841747496\n",
      "\n",
      "Epoch 187\n",
      "---------\n",
      "Train loss: 0.06525044981390238\n",
      "Test loss: 0.4674844592809677\n",
      "\n",
      "Epoch 188\n",
      "---------\n",
      "Train loss: 0.06553390511544421\n",
      "Test loss: 0.41991458502080703\n",
      "\n",
      "Epoch 189\n",
      "---------\n",
      "Train loss: 0.07226604392885097\n",
      "Test loss: 0.4046676605939865\n",
      "\n",
      "Epoch 190\n",
      "---------\n",
      "Train loss: 0.06864296232483216\n",
      "Test loss: 0.4227241377035777\n",
      "\n",
      "Epoch 191\n",
      "---------\n",
      "Train loss: 0.06427465289432023\n",
      "Test loss: 0.474218691388766\n",
      "\n",
      "Epoch 192\n",
      "---------\n",
      "Train loss: 0.06829540394911808\n",
      "Test loss: 0.42065808839268154\n",
      "\n",
      "Epoch 193\n",
      "---------\n",
      "Train loss: 0.06675099272147886\n",
      "Test loss: 0.42668161458439297\n",
      "\n",
      "Epoch 194\n",
      "---------\n",
      "Train loss: 0.06799660237239939\n",
      "Test loss: 0.48342087533738876\n",
      "\n",
      "Epoch 195\n",
      "---------\n",
      "Train loss: 0.06752523540386132\n",
      "Test loss: 0.43344298170672524\n",
      "\n",
      "Epoch 196\n",
      "---------\n",
      "Train loss: 0.0719920493263219\n",
      "Test loss: 0.42172617216904956\n",
      "\n",
      "Epoch 197\n",
      "---------\n",
      "Train loss: 0.0731148145028523\n",
      "Test loss: 0.46033432914151085\n",
      "\n",
      "Epoch 198\n",
      "---------\n",
      "Train loss: 0.06722950775708471\n",
      "Test loss: 0.409906342625618\n",
      "\n",
      "Epoch 199\n",
      "---------\n",
      "Train loss: 0.06629270394997937\n",
      "Test loss: 0.4466735108031167\n",
      "\n",
      "Epoch 200\n",
      "---------\n",
      "Train loss: 0.06517721137164958\n",
      "Test loss: 0.4215722895330853\n",
      "\n",
      "Epoch 201\n",
      "---------\n",
      "Train loss: 0.06596287599365626\n",
      "Test loss: 0.43533576528231305\n",
      "\n",
      "Epoch 202\n",
      "---------\n",
      "Train loss: 0.06593758745917253\n",
      "Test loss: 0.467279573281606\n",
      "\n",
      "Epoch 203\n",
      "---------\n",
      "Train loss: 0.06554455386607774\n",
      "Test loss: 0.44153930412398446\n",
      "\n",
      "Epoch 204\n",
      "---------\n",
      "Train loss: 0.07257015946587282\n",
      "Test loss: 0.43639853099981946\n",
      "\n",
      "Epoch 205\n",
      "---------\n",
      "Train loss: 0.0721817598511864\n",
      "Test loss: 0.4254681368668874\n",
      "\n",
      "Epoch 206\n",
      "---------\n",
      "Train loss: 0.0677538280641394\n",
      "Test loss: 0.4764791958861881\n",
      "\n",
      "Epoch 207\n",
      "---------\n",
      "Train loss: 0.06503924660916839\n",
      "Test loss: 0.4394631087779999\n",
      "\n",
      "Epoch 208\n",
      "---------\n",
      "Train loss: 0.06721976811864547\n",
      "Test loss: 0.4014842626121309\n",
      "\n",
      "Epoch 209\n",
      "---------\n",
      "Train loss: 0.06715437736628312\n",
      "Test loss: 0.4443143837981754\n",
      "\n",
      "Epoch 210\n",
      "---------\n",
      "Train loss: 0.06492604958891336\n",
      "Test loss: 0.47875552541679806\n",
      "\n",
      "Epoch 211\n",
      "---------\n",
      "Train loss: 0.06560577808081039\n",
      "Test loss: 0.4298633502589332\n",
      "\n",
      "Epoch 212\n",
      "---------\n",
      "Train loss: 0.07297367888635822\n",
      "Test loss: 0.43793606758117676\n",
      "\n",
      "Epoch 213\n",
      "---------\n",
      "Train loss: 0.06820090228159513\n",
      "Test loss: 0.41577096780141193\n",
      "\n",
      "Epoch 214\n",
      "---------\n",
      "Train loss: 0.06483324497405972\n",
      "Test loss: 0.45139527486430275\n",
      "\n",
      "Epoch 215\n",
      "---------\n",
      "Train loss: 0.06927904894109815\n",
      "Test loss: 0.4253961079650455\n",
      "\n",
      "Epoch 216\n",
      "---------\n",
      "Train loss: 0.06477176317379676\n",
      "Test loss: 0.4313443601131439\n",
      "\n",
      "Epoch 217\n",
      "---------\n",
      "Train loss: 0.06677483893664819\n",
      "Test loss: 0.44808434777789646\n",
      "\n",
      "Epoch 218\n",
      "---------\n",
      "Train loss: 0.06810829395960484\n",
      "Test loss: 0.44669311576419407\n",
      "\n",
      "Epoch 219\n",
      "---------\n",
      "Train loss: 0.06394497470215096\n",
      "Test loss: 0.4432397302654054\n",
      "\n",
      "Epoch 220\n",
      "---------\n",
      "Train loss: 0.06872267522183913\n",
      "Test loss: 0.4653429041306178\n",
      "\n",
      "Epoch 221\n",
      "---------\n",
      "Train loss: 0.06752702201317463\n",
      "Test loss: 0.403552586833636\n",
      "\n",
      "Epoch 222\n",
      "---------\n",
      "Train loss: 0.06534629541316203\n",
      "Test loss: 0.45673902332782745\n",
      "\n",
      "Epoch 223\n",
      "---------\n",
      "Train loss: 0.06463682150933892\n",
      "Test loss: 0.4490973833534453\n",
      "\n",
      "Epoch 224\n",
      "---------\n",
      "Train loss: 0.06576133965115462\n",
      "Test loss: 0.43844541245036656\n",
      "\n",
      "Epoch 225\n",
      "---------\n",
      "Train loss: 0.06998478631222886\n",
      "Test loss: 0.4722319973839654\n",
      "\n",
      "Epoch 226\n",
      "---------\n",
      "Train loss: 0.06913862734966512\n",
      "Test loss: 0.40293550822469926\n",
      "\n",
      "Epoch 227\n",
      "---------\n",
      "Train loss: 0.06593505522635366\n",
      "Test loss: 0.42931536502308315\n",
      "\n",
      "Epoch 228\n",
      "---------\n",
      "Train loss: 0.06502612148012434\n",
      "Test loss: 0.42272282640139264\n",
      "\n",
      "Epoch 229\n",
      "---------\n",
      "Train loss: 0.06500499815280948\n",
      "Test loss: 0.4074403974745009\n",
      "\n",
      "Epoch 230\n",
      "---------\n",
      "Train loss: 0.06739910379318255\n",
      "Test loss: 0.4226852473285463\n",
      "\n",
      "Epoch 231\n",
      "---------\n",
      "Train loss: 0.06439213098825089\n",
      "Test loss: 0.4734305358595318\n",
      "\n",
      "Epoch 232\n",
      "---------\n",
      "Train loss: 0.06503290549985\n",
      "Test loss: 0.4360364344384935\n",
      "\n",
      "Epoch 233\n",
      "---------\n",
      "Train loss: 0.06765924537155245\n",
      "Test loss: 0.4272678660021888\n",
      "\n",
      "Epoch 234\n",
      "---------\n",
      "Train loss: 0.0648437890756343\n",
      "Test loss: 0.4288732045226627\n",
      "\n",
      "Epoch 235\n",
      "---------\n",
      "Train loss: 0.06497090098202664\n",
      "Test loss: 0.46644822094175553\n",
      "\n",
      "Epoch 236\n",
      "---------\n",
      "Train loss: 0.06758206843265466\n",
      "Test loss: 0.4446151355902354\n",
      "\n",
      "Epoch 237\n",
      "---------\n",
      "Train loss: 0.06596488066549812\n",
      "Test loss: 0.4518518000841141\n",
      "\n",
      "Epoch 238\n",
      "---------\n",
      "Train loss: 0.07004936431933727\n",
      "Test loss: 0.42110616465409595\n",
      "\n",
      "Epoch 239\n",
      "---------\n",
      "Train loss: 0.06605698299660746\n",
      "Test loss: 0.47940129538377124\n",
      "\n",
      "Epoch 240\n",
      "---------\n",
      "Train loss: 0.07259646666768406\n",
      "Test loss: 0.42332150207625496\n",
      "\n",
      "Epoch 241\n",
      "---------\n",
      "Train loss: 0.06666032113051708\n",
      "Test loss: 0.4427248686552048\n",
      "\n",
      "Epoch 242\n",
      "---------\n",
      "Train loss: 0.06518528494052589\n",
      "Test loss: 0.4435028185447057\n",
      "\n",
      "Epoch 243\n",
      "---------\n",
      "Train loss: 0.06441420615218314\n",
      "Test loss: 0.43930043942398495\n",
      "\n",
      "Epoch 244\n",
      "---------\n",
      "Train loss: 0.06474540322752935\n",
      "Test loss: 0.40538308024406433\n",
      "\n",
      "Epoch 245\n",
      "---------\n",
      "Train loss: 0.06345386422071897\n",
      "Test loss: 0.41643188065952724\n",
      "\n",
      "Epoch 246\n",
      "---------\n",
      "Train loss: 0.06469776571196105\n",
      "Test loss: 0.4450518124633365\n",
      "\n",
      "Epoch 247\n",
      "---------\n",
      "Train loss: 0.06421118831661131\n",
      "Test loss: 0.4296211550633113\n",
      "\n",
      "Epoch 248\n",
      "---------\n",
      "Train loss: 0.06424541212618351\n",
      "Test loss: 0.4456400126218796\n",
      "\n",
      "Epoch 249\n",
      "---------\n",
      "Train loss: 0.0641000671312213\n",
      "Test loss: 0.4417191313372718\n",
      "\n",
      "Epoch 250\n",
      "---------\n",
      "Train loss: 0.06434391868866182\n",
      "Test loss: 0.4185870521598392\n",
      "\n",
      "Epoch 251\n",
      "---------\n",
      "Train loss: 0.06559035952003407\n",
      "Test loss: 0.42671628461943734\n",
      "\n",
      "Epoch 252\n",
      "---------\n",
      "Train loss: 0.064702920308004\n",
      "Test loss: 0.4526720775498284\n",
      "\n",
      "Epoch 253\n",
      "---------\n",
      "Train loss: 0.0686036547745711\n",
      "Test loss: 0.4217774007055495\n",
      "\n",
      "Epoch 254\n",
      "---------\n",
      "Train loss: 0.06591868377290666\n",
      "Test loss: 0.45462122393978965\n",
      "\n",
      "Epoch 255\n",
      "---------\n",
      "Train loss: 0.06434718747290649\n",
      "Test loss: 0.44380395776695675\n",
      "\n",
      "Epoch 256\n",
      "---------\n",
      "Train loss: 0.06754882074892521\n",
      "Test loss: 0.4462004353602727\n",
      "\n",
      "Epoch 257\n",
      "---------\n",
      "Train loss: 0.06320355453291475\n",
      "Test loss: 0.43406055205398136\n",
      "\n",
      "Epoch 258\n",
      "---------\n",
      "Train loss: 0.06380768910665731\n",
      "Test loss: 0.4321068409416411\n",
      "\n",
      "Epoch 259\n",
      "---------\n",
      "Train loss: 0.06583017237218362\n",
      "Test loss: 0.4090232782893711\n",
      "\n",
      "Epoch 260\n",
      "---------\n",
      "Train loss: 0.06339537977848002\n",
      "Test loss: 0.4453975624508328\n",
      "\n",
      "Epoch 261\n",
      "---------\n",
      "Train loss: 0.0660906879763518\n",
      "Test loss: 0.4109155535697937\n",
      "\n",
      "Epoch 262\n",
      "---------\n",
      "Train loss: 0.06855343841016293\n",
      "Test loss: 0.3860787832074695\n",
      "\n",
      "Epoch 263\n",
      "---------\n",
      "Train loss: 0.06412765107649777\n",
      "Test loss: 0.41623571183946395\n",
      "\n",
      "Epoch 264\n",
      "---------\n",
      "Train loss: 0.06634008871125323\n",
      "Test loss: 0.44097424215740627\n",
      "\n",
      "Epoch 265\n",
      "---------\n",
      "Train loss: 0.06300581898540258\n",
      "Test loss: 0.43368007408248055\n",
      "\n",
      "Epoch 266\n",
      "---------\n",
      "Train loss: 0.06661057625231999\n",
      "Test loss: 0.4386458843946457\n",
      "\n",
      "Epoch 267\n",
      "---------\n",
      "Train loss: 0.0633545826255743\n",
      "Test loss: 0.3898935963710149\n",
      "\n",
      "Epoch 268\n",
      "---------\n",
      "Train loss: 0.06725344838508006\n",
      "Test loss: 0.45189892252286273\n",
      "\n",
      "Epoch 269\n",
      "---------\n",
      "Train loss: 0.07038690402571644\n",
      "Test loss: 0.4222533024019665\n",
      "\n",
      "Epoch 270\n",
      "---------\n",
      "Train loss: 0.0640607720477939\n",
      "Test loss: 0.4387752529647615\n",
      "\n",
      "Epoch 271\n",
      "---------\n",
      "Train loss: 0.06382030496440295\n",
      "Test loss: 0.442932160364257\n",
      "\n",
      "Epoch 272\n",
      "---------\n",
      "Train loss: 0.06516576704702207\n",
      "Test loss: 0.44088270101282334\n",
      "\n",
      "Epoch 273\n",
      "---------\n",
      "Train loss: 0.06387881301010825\n",
      "Test loss: 0.3685717499918408\n",
      "\n",
      "Epoch 274\n",
      "---------\n",
      "Train loss: 0.06749568367376924\n",
      "Test loss: 0.417484818233384\n",
      "\n",
      "Epoch 275\n",
      "---------\n",
      "Train loss: 0.06579128622875682\n",
      "Test loss: 0.41929592854446834\n",
      "\n",
      "Epoch 276\n",
      "---------\n",
      "Train loss: 0.0647450263079788\n",
      "Test loss: 0.4153407977686988\n",
      "\n",
      "Epoch 277\n",
      "---------\n",
      "Train loss: 0.06283198214181798\n",
      "Test loss: 0.4310120642185211\n",
      "\n",
      "Epoch 278\n",
      "---------\n",
      "Train loss: 0.0798690527611013\n",
      "Test loss: 0.456456344988611\n",
      "\n",
      "Epoch 279\n",
      "---------\n",
      "Train loss: 0.07161303375115884\n",
      "Test loss: 0.3782673130432765\n",
      "\n",
      "Epoch 280\n",
      "---------\n",
      "Train loss: 0.07010719100279468\n",
      "Test loss: 0.4655189663171768\n",
      "\n",
      "Epoch 281\n",
      "---------\n",
      "Train loss: 0.07003022500846003\n",
      "Test loss: 0.45172584387991166\n",
      "\n",
      "Epoch 282\n",
      "---------\n",
      "Train loss: 0.0647060378354841\n",
      "Test loss: 0.42866867946253884\n",
      "\n",
      "Epoch 283\n",
      "---------\n",
      "Train loss: 0.06320298925441291\n",
      "Test loss: 0.42655232383145225\n",
      "\n",
      "Epoch 284\n",
      "---------\n",
      "Train loss: 0.06586250789197427\n",
      "Test loss: 0.4103611442777846\n",
      "\n",
      "Epoch 285\n",
      "---------\n",
      "Train loss: 0.06261631636880338\n",
      "Test loss: 0.4258974211083518\n",
      "\n",
      "Epoch 286\n",
      "---------\n",
      "Train loss: 0.06426989671308547\n",
      "Test loss: 0.4189971453613705\n",
      "\n",
      "Epoch 287\n",
      "---------\n",
      "Train loss: 0.06304759956297598\n",
      "Test loss: 0.39963780840237934\n",
      "\n",
      "Epoch 288\n",
      "---------\n",
      "Train loss: 0.06225640529009979\n",
      "Test loss: 0.4352194600635105\n",
      "\n",
      "Epoch 289\n",
      "---------\n",
      "Train loss: 0.06271038660114366\n",
      "Test loss: 0.41070785125096637\n",
      "\n",
      "Epoch 290\n",
      "---------\n",
      "Train loss: 0.062372482947206924\n",
      "Test loss: 0.4191554519865248\n",
      "\n",
      "Epoch 291\n",
      "---------\n",
      "Train loss: 0.06339686649984547\n",
      "Test loss: 0.4319288796848721\n",
      "\n",
      "Epoch 292\n",
      "---------\n",
      "Train loss: 0.06523762147740594\n",
      "Test loss: 0.410304918885231\n",
      "\n",
      "Epoch 293\n",
      "---------\n",
      "Train loss: 0.06775218491176409\n",
      "Test loss: 0.4675542298290465\n",
      "\n",
      "Epoch 294\n",
      "---------\n",
      "Train loss: 0.06491761628006186\n",
      "Test loss: 0.3902046349313524\n",
      "\n",
      "Epoch 295\n",
      "---------\n",
      "Train loss: 0.07098907844296523\n",
      "Test loss: 0.46721592876646256\n",
      "\n",
      "Epoch 296\n",
      "---------\n",
      "Train loss: 0.07151847219626818\n",
      "Test loss: 0.3973801102903154\n",
      "\n",
      "Epoch 297\n",
      "---------\n",
      "Train loss: 0.06276919423336429\n",
      "Test loss: 0.427568440635999\n",
      "\n",
      "Epoch 298\n",
      "---------\n",
      "Train loss: 0.06246769639463829\n",
      "Test loss: 0.40568013820383286\n",
      "\n",
      "Epoch 299\n",
      "---------\n",
      "Train loss: 0.07070645859598049\n",
      "Test loss: 0.39924051860968274\n",
      "\n",
      "Epoch 300\n",
      "---------\n",
      "Train loss: 0.07048842181185526\n",
      "Test loss: 0.4541076372067134\n",
      "\n",
      "Epoch 301\n",
      "---------\n",
      "Train loss: 0.06534478643776051\n",
      "Test loss: 0.42644382185406154\n",
      "\n",
      "Epoch 302\n",
      "---------\n",
      "Train loss: 0.06279608372798455\n",
      "Test loss: 0.4303121864795685\n",
      "\n",
      "Epoch 303\n",
      "---------\n",
      "Train loss: 0.06633965401644153\n",
      "Test loss: 0.4114794019195769\n",
      "\n",
      "Epoch 304\n",
      "---------\n",
      "Train loss: 0.06338158968303885\n",
      "Test loss: 0.40946701334582436\n",
      "\n",
      "Epoch 305\n",
      "---------\n",
      "Train loss: 0.06829343636387161\n",
      "Test loss: 0.39998360971609753\n",
      "\n",
      "Epoch 306\n",
      "---------\n",
      "Train loss: 0.06238898909318128\n",
      "Test loss: 0.39979029032919144\n",
      "\n",
      "Epoch 307\n",
      "---------\n",
      "Train loss: 0.06407131261325308\n",
      "Test loss: 0.4004303465286891\n",
      "\n",
      "Epoch 308\n",
      "---------\n",
      "Train loss: 0.06217986052589757\n",
      "Test loss: 0.4297935913006465\n",
      "\n",
      "Epoch 309\n",
      "---------\n",
      "Train loss: 0.06249561120889017\n",
      "Test loss: 0.42081259025467765\n",
      "\n",
      "Epoch 310\n",
      "---------\n",
      "Train loss: 0.06555304443463683\n",
      "Test loss: 0.4254491809341643\n",
      "\n",
      "Epoch 311\n",
      "---------\n",
      "Train loss: 0.06410290766507387\n",
      "Test loss: 0.39978396230273777\n",
      "\n",
      "Epoch 312\n",
      "---------\n",
      "Train loss: 0.07345777706775282\n",
      "Test loss: 0.4165978464815352\n",
      "\n",
      "Epoch 313\n",
      "---------\n",
      "Train loss: 0.06959610893889996\n",
      "Test loss: 0.37377674380938214\n",
      "\n",
      "Epoch 314\n",
      "---------\n",
      "Train loss: 0.06302254990441725\n",
      "Test loss: 0.44020550615257686\n",
      "\n",
      "Epoch 315\n",
      "---------\n",
      "Train loss: 0.06405468973597246\n",
      "Test loss: 0.39507823023531174\n",
      "\n",
      "Epoch 316\n",
      "---------\n",
      "Train loss: 0.06167798407841474\n",
      "Test loss: 0.4124776952796512\n",
      "\n",
      "Epoch 317\n",
      "---------\n",
      "Train loss: 0.06286218117124268\n",
      "Test loss: 0.42046453058719635\n",
      "\n",
      "Epoch 318\n",
      "---------\n",
      "Train loss: 0.06405465833709709\n",
      "Test loss: 0.4123936692873637\n",
      "\n",
      "Epoch 319\n",
      "---------\n",
      "Train loss: 0.06459182878357492\n",
      "Test loss: 0.41540858811802334\n",
      "\n",
      "Epoch 320\n",
      "---------\n",
      "Train loss: 0.0635493761593742\n",
      "Test loss: 0.43301571905612946\n",
      "\n",
      "Epoch 321\n",
      "---------\n",
      "Train loss: 0.06231299280937362\n",
      "Test loss: 0.4282571060789956\n",
      "\n",
      "Epoch 322\n",
      "---------\n",
      "Train loss: 0.06181194800384609\n",
      "Test loss: 0.39062608116202885\n",
      "\n",
      "Epoch 323\n",
      "---------\n",
      "Train loss: 0.06213484696179096\n",
      "Test loss: 0.4091787106460995\n",
      "\n",
      "Epoch 324\n",
      "---------\n",
      "Train loss: 0.06135048700629601\n",
      "Test loss: 0.4216279122564528\n",
      "\n",
      "Epoch 325\n",
      "---------\n",
      "Train loss: 0.06290815637579986\n",
      "Test loss: 0.40314645568529767\n",
      "\n",
      "Epoch 326\n",
      "---------\n",
      "Train loss: 0.06579607936354089\n",
      "Test loss: 0.42400336099995506\n",
      "\n",
      "Epoch 327\n",
      "---------\n",
      "Train loss: 0.06369688475803871\n",
      "Test loss: 0.40575232605139416\n",
      "\n",
      "Epoch 328\n",
      "---------\n",
      "Train loss: 0.06837845241118755\n",
      "Test loss: 0.41060053143236375\n",
      "\n",
      "Epoch 329\n",
      "---------\n",
      "Train loss: 0.07050308811345271\n",
      "Test loss: 0.4005461980899175\n",
      "\n",
      "Epoch 330\n",
      "---------\n",
      "Train loss: 0.06555508762331945\n",
      "Test loss: 0.39665625327163273\n",
      "\n",
      "Epoch 331\n",
      "---------\n",
      "Train loss: 0.06714105024002492\n",
      "Test loss: 0.44795136981540257\n",
      "\n",
      "Epoch 332\n",
      "---------\n",
      "Train loss: 0.06315527833066881\n",
      "Test loss: 0.4090764804018868\n",
      "\n",
      "Epoch 333\n",
      "---------\n",
      "Train loss: 0.06318356901673335\n",
      "Test loss: 0.42727157639132607\n",
      "\n",
      "Epoch 334\n",
      "---------\n",
      "Train loss: 0.06142402523463326\n",
      "Test loss: 0.4352422704299291\n",
      "\n",
      "Epoch 335\n",
      "---------\n",
      "Train loss: 0.06592007127723523\n",
      "Test loss: 0.4122113552358415\n",
      "\n",
      "Epoch 336\n",
      "---------\n",
      "Train loss: 0.06198154208582959\n",
      "Test loss: 0.4147261877854665\n",
      "\n",
      "Epoch 337\n",
      "---------\n",
      "Train loss: 0.06210312293842435\n",
      "Test loss: 0.4573386460542679\n",
      "\n",
      "Epoch 338\n",
      "---------\n",
      "Train loss: 0.060476163934384077\n",
      "Test loss: 0.40452828506628674\n",
      "\n",
      "Epoch 339\n",
      "---------\n",
      "Train loss: 0.06267004852582302\n",
      "Test loss: 0.4201049721903271\n",
      "\n",
      "Epoch 340\n",
      "---------\n",
      "Train loss: 0.062377890405644267\n",
      "Test loss: 0.4354612347152498\n",
      "\n",
      "Epoch 341\n",
      "---------\n",
      "Train loss: 0.0651264620080058\n",
      "Test loss: 0.43778930604457855\n",
      "\n",
      "Epoch 342\n",
      "---------\n",
      "Train loss: 0.06437954605124625\n",
      "Test loss: 0.4332667142152786\n",
      "\n",
      "Epoch 343\n",
      "---------\n",
      "Train loss: 0.05960951695617821\n",
      "Test loss: 0.4070236649778154\n",
      "\n",
      "Epoch 344\n",
      "---------\n",
      "Train loss: 0.06533696176484227\n",
      "Test loss: 0.39745334453052944\n",
      "\n",
      "Epoch 345\n",
      "---------\n",
      "Train loss: 0.06392194909442749\n",
      "Test loss: 0.4329826384782791\n",
      "\n",
      "Epoch 346\n",
      "---------\n",
      "Train loss: 0.06232869964359062\n",
      "Test loss: 0.40668167008294\n",
      "\n",
      "Epoch 347\n",
      "---------\n",
      "Train loss: 0.060493149711484354\n",
      "Test loss: 0.40323499507374233\n",
      "\n",
      "Epoch 348\n",
      "---------\n",
      "Train loss: 0.062094853658761294\n",
      "Test loss: 0.3960502694050471\n",
      "\n",
      "Epoch 349\n",
      "---------\n",
      "Train loss: 0.06487232050858438\n",
      "Test loss: 0.37401063326332307\n",
      "\n",
      "Epoch 350\n",
      "---------\n",
      "Train loss: 0.06217619267824505\n",
      "Test loss: 0.4214169515503777\n",
      "\n",
      "Epoch 351\n",
      "---------\n",
      "Train loss: 0.06238045593324516\n",
      "Test loss: 0.446105925573243\n",
      "\n",
      "Epoch 352\n",
      "---------\n",
      "Train loss: 0.06004513116412064\n",
      "Test loss: 0.39671267569065094\n",
      "\n",
      "Epoch 353\n",
      "---------\n",
      "Train loss: 0.0609833734509136\n",
      "Test loss: 0.42820098996162415\n",
      "\n",
      "Epoch 354\n",
      "---------\n",
      "Train loss: 0.05984260865287589\n",
      "Test loss: 0.4189252836836709\n",
      "\n",
      "Epoch 355\n",
      "---------\n",
      "Train loss: 0.06402317531007741\n",
      "Test loss: 0.4049343847566181\n",
      "\n",
      "Epoch 356\n",
      "---------\n",
      "Train loss: 0.062375764562083144\n",
      "Test loss: 0.4178127812014686\n",
      "\n",
      "Epoch 357\n",
      "---------\n",
      "Train loss: 0.06163996338312115\n",
      "Test loss: 0.438486710190773\n",
      "\n",
      "Epoch 358\n",
      "---------\n",
      "Train loss: 0.05908100455001529\n",
      "Test loss: 0.4147230022483402\n",
      "\n",
      "Epoch 359\n",
      "---------\n",
      "Train loss: 0.06108199266184654\n",
      "Test loss: 0.4119030808409055\n",
      "\n",
      "Epoch 360\n",
      "---------\n",
      "Train loss: 0.061794236643306376\n",
      "Test loss: 0.41816729886664283\n",
      "\n",
      "Epoch 361\n",
      "---------\n",
      "Train loss: 0.06327779810609561\n",
      "Test loss: 0.4105726348029243\n",
      "\n",
      "Epoch 362\n",
      "---------\n",
      "Train loss: 0.06024538153516395\n",
      "Test loss: 0.39962615817785263\n",
      "\n",
      "Epoch 363\n",
      "---------\n",
      "Train loss: 0.06061482402895178\n",
      "Test loss: 0.4312925504313575\n",
      "\n",
      "Epoch 364\n",
      "---------\n",
      "Train loss: 0.06290648323816381\n",
      "Test loss: 0.4013771911462148\n",
      "\n",
      "Epoch 365\n",
      "---------\n",
      "Train loss: 0.06028669144559119\n",
      "Test loss: 0.42717986636691624\n",
      "\n",
      "Epoch 366\n",
      "---------\n",
      "Train loss: 0.059711631090717025\n",
      "Test loss: 0.3939630116025607\n",
      "\n",
      "Epoch 367\n",
      "---------\n",
      "Train loss: 0.059010019209901135\n",
      "Test loss: 0.44848623043960995\n",
      "\n",
      "Epoch 368\n",
      "---------\n",
      "Train loss: 0.061785170841696005\n",
      "Test loss: 0.43463895387119716\n",
      "\n",
      "Epoch 369\n",
      "---------\n",
      "Train loss: 0.060786292996324064\n",
      "Test loss: 0.4006972519887818\n",
      "\n",
      "Epoch 370\n",
      "---------\n",
      "Train loss: 0.06119382627574461\n",
      "Test loss: 0.4185831762022442\n",
      "\n",
      "Epoch 371\n",
      "---------\n",
      "Train loss: 0.06343090398669508\n",
      "Test loss: 0.43536121563778984\n",
      "\n",
      "Epoch 372\n",
      "---------\n",
      "Train loss: 0.0627258945855179\n",
      "Test loss: 0.43504105673895943\n",
      "\n",
      "Epoch 373\n",
      "---------\n",
      "Train loss: 0.05993628844485751\n",
      "Test loss: 0.4308294985029433\n",
      "\n",
      "Epoch 374\n",
      "---------\n",
      "Train loss: 0.05965903531094747\n",
      "Test loss: 0.42238570666975445\n",
      "\n",
      "Epoch 375\n",
      "---------\n",
      "Train loss: 0.058679640414181576\n",
      "Test loss: 0.4154654261138704\n",
      "\n",
      "Epoch 376\n",
      "---------\n",
      "Train loss: 0.05945697729475796\n",
      "Test loss: 0.39736195156971615\n",
      "\n",
      "Epoch 377\n",
      "---------\n",
      "Train loss: 0.05879352112034602\n",
      "Test loss: 0.41713666915893555\n",
      "\n",
      "Epoch 378\n",
      "---------\n",
      "Train loss: 0.05864277724841876\n",
      "Test loss: 0.43502359506156707\n",
      "\n",
      "Epoch 379\n",
      "---------\n",
      "Train loss: 0.05798192353021087\n",
      "Test loss: 0.40789123210642075\n",
      "\n",
      "Epoch 380\n",
      "---------\n",
      "Train loss: 0.05825692145299399\n",
      "Test loss: 0.44008753034803605\n",
      "\n",
      "Epoch 381\n",
      "---------\n",
      "Train loss: 0.05842969114642723\n",
      "Test loss: 0.42834606601132286\n",
      "\n",
      "Epoch 382\n",
      "---------\n",
      "Train loss: 0.0599960678124002\n",
      "Test loss: 0.3972703127397431\n",
      "\n",
      "Epoch 383\n",
      "---------\n",
      "Train loss: 0.059220797482079694\n",
      "Test loss: 0.41426654739512336\n",
      "\n",
      "Epoch 384\n",
      "---------\n",
      "Train loss: 0.058798566477240195\n",
      "Test loss: 0.40895526359478634\n",
      "\n",
      "Epoch 385\n",
      "---------\n",
      "Train loss: 0.061586852451520305\n",
      "Test loss: 0.43063700530264115\n",
      "\n",
      "Epoch 386\n",
      "---------\n",
      "Train loss: 0.06350876098232609\n",
      "Test loss: 0.4217476447423299\n",
      "\n",
      "Epoch 387\n",
      "---------\n",
      "Train loss: 0.058137466894420085\n",
      "Test loss: 0.40604248229000306\n",
      "\n",
      "Epoch 388\n",
      "---------\n",
      "Train loss: 0.06196997028642467\n",
      "Test loss: 0.43190237879753113\n",
      "\n",
      "Epoch 389\n",
      "---------\n",
      "Train loss: 0.06264963034274322\n",
      "Test loss: 0.3972969866461224\n",
      "\n",
      "Epoch 390\n",
      "---------\n",
      "Train loss: 0.058434883930853436\n",
      "Test loss: 0.40962861229976016\n",
      "\n",
      "Epoch 391\n",
      "---------\n",
      "Train loss: 0.05778324926671173\n",
      "Test loss: 0.4258168712258339\n",
      "\n",
      "Epoch 392\n",
      "---------\n",
      "Train loss: 0.06114115279966167\n",
      "Test loss: 0.40059948960940045\n",
      "\n",
      "Epoch 393\n",
      "---------\n",
      "Train loss: 0.05948860564136079\n",
      "Test loss: 0.42322347727086806\n",
      "\n",
      "Epoch 394\n",
      "---------\n",
      "Train loss: 0.05875657096372119\n",
      "Test loss: 0.4194878803359138\n",
      "\n",
      "Epoch 395\n",
      "---------\n",
      "Train loss: 0.05838707195860999\n",
      "Test loss: 0.3910706415772438\n",
      "\n",
      "Epoch 396\n",
      "---------\n",
      "Train loss: 0.05932911259255239\n",
      "Test loss: 0.405022506084707\n",
      "\n",
      "Epoch 397\n",
      "---------\n",
      "Train loss: 0.05878198688982853\n",
      "Test loss: 0.4066624657975303\n",
      "\n",
      "Epoch 398\n",
      "---------\n",
      "Train loss: 0.05798794764892331\n",
      "Test loss: 0.39675749093294144\n",
      "\n",
      "Epoch 399\n",
      "---------\n",
      "Train loss: 0.061408568905400376\n",
      "Test loss: 0.43863724668820697\n",
      "\n",
      "Epoch 400\n",
      "---------\n",
      "Train loss: 0.06027390242421201\n",
      "Test loss: 0.40479404893186355\n",
      "\n",
      "Epoch 401\n",
      "---------\n",
      "Train loss: 0.05696427830012648\n",
      "Test loss: 0.4385898212591807\n",
      "\n",
      "Epoch 402\n",
      "---------\n",
      "Train loss: 0.05830897996202111\n",
      "Test loss: 0.38995105607642067\n",
      "\n",
      "Epoch 403\n",
      "---------\n",
      "Train loss: 0.061144928414640684\n",
      "Test loss: 0.4206310709317525\n",
      "\n",
      "Epoch 404\n",
      "---------\n",
      "Train loss: 0.06316257680633239\n",
      "Test loss: 0.4314015640152825\n",
      "\n",
      "Epoch 405\n",
      "---------\n",
      "Train loss: 0.05694788351788053\n",
      "Test loss: 0.42864395926396054\n",
      "\n",
      "Epoch 406\n",
      "---------\n",
      "Train loss: 0.05910110852814147\n",
      "Test loss: 0.39869799547725254\n",
      "\n",
      "Epoch 407\n",
      "---------\n",
      "Train loss: 0.05782833751956267\n",
      "Test loss: 0.4278981180654632\n",
      "\n",
      "Epoch 408\n",
      "---------\n",
      "Train loss: 0.06270809970530015\n",
      "Test loss: 0.4307833131816652\n",
      "\n",
      "Epoch 409\n",
      "---------\n",
      "Train loss: 0.06062813222940479\n",
      "Test loss: 0.41297326816452873\n",
      "\n",
      "Epoch 410\n",
      "---------\n",
      "Train loss: 0.06379139855770129\n",
      "Test loss: 0.4277348029944632\n",
      "\n",
      "Epoch 411\n",
      "---------\n",
      "Train loss: 0.0615000450883859\n",
      "Test loss: 0.3974815093808704\n",
      "\n",
      "Epoch 412\n",
      "---------\n",
      "Train loss: 0.056958929768630435\n",
      "Test loss: 0.4323854049046834\n",
      "\n",
      "Epoch 413\n",
      "---------\n",
      "Train loss: 0.05908159874213327\n",
      "Test loss: 0.4032706841826439\n",
      "\n",
      "Epoch 414\n",
      "---------\n",
      "Train loss: 0.058269072989267964\n",
      "Test loss: 0.42653482490115696\n",
      "\n",
      "Epoch 415\n",
      "---------\n",
      "Train loss: 0.05990698028888021\n",
      "Test loss: 0.4276882807413737\n",
      "\n",
      "Epoch 416\n",
      "---------\n",
      "Train loss: 0.056350042964498116\n",
      "Test loss: 0.40003304762972725\n",
      "\n",
      "Epoch 417\n",
      "---------\n",
      "Train loss: 0.05698838400920587\n",
      "Test loss: 0.41946250283055836\n",
      "\n",
      "Epoch 418\n",
      "---------\n",
      "Train loss: 0.06013722551454391\n",
      "Test loss: 0.3901417131225268\n",
      "\n",
      "Epoch 419\n",
      "---------\n",
      "Train loss: 0.05697110855336567\n",
      "Test loss: 0.41800445980495876\n",
      "\n",
      "Epoch 420\n",
      "---------\n",
      "Train loss: 0.058149753171684485\n",
      "Test loss: 0.40271347926722634\n",
      "\n",
      "Epoch 421\n",
      "---------\n",
      "Train loss: 0.05743849488706993\n",
      "Test loss: 0.43043723122941124\n",
      "\n",
      "Epoch 422\n",
      "---------\n",
      "Train loss: 0.0573207365565135\n",
      "Test loss: 0.41125986228386563\n",
      "\n",
      "Epoch 423\n",
      "---------\n",
      "Train loss: 0.056068332872590484\n",
      "Test loss: 0.40252458221382564\n",
      "\n",
      "Epoch 424\n",
      "---------\n",
      "Train loss: 0.0640563793214304\n",
      "Test loss: 0.417577794028653\n",
      "\n",
      "Epoch 425\n",
      "---------\n",
      "Train loss: 0.057192222728709954\n",
      "Test loss: 0.4158842960993449\n",
      "\n",
      "Epoch 426\n",
      "---------\n",
      "Train loss: 0.05742034681939653\n",
      "Test loss: 0.40114489363299477\n",
      "\n",
      "Epoch 427\n",
      "---------\n",
      "Train loss: 0.057786280217572185\n",
      "Test loss: 0.4024727452132437\n",
      "\n",
      "Epoch 428\n",
      "---------\n",
      "Train loss: 0.0605281394839819\n",
      "Test loss: 0.3725013807415962\n",
      "\n",
      "Epoch 429\n",
      "---------\n",
      "Train loss: 0.0565189383258777\n",
      "Test loss: 0.41238444132937324\n",
      "\n",
      "Epoch 430\n",
      "---------\n",
      "Train loss: 0.056869867078993205\n",
      "Test loss: 0.40545058333211476\n",
      "\n",
      "Epoch 431\n",
      "---------\n",
      "Train loss: 0.05560483661247417\n",
      "Test loss: 0.42412155369917554\n",
      "\n",
      "Epoch 432\n",
      "---------\n",
      "Train loss: 0.05642003565195149\n",
      "Test loss: 0.42586834728717804\n",
      "\n",
      "Epoch 433\n",
      "---------\n",
      "Train loss: 0.06039721259315099\n",
      "Test loss: 0.40821607162555057\n",
      "\n",
      "Epoch 434\n",
      "---------\n",
      "Train loss: 0.05902339067376618\n",
      "Test loss: 0.4412454441189766\n",
      "\n",
      "Epoch 435\n",
      "---------\n",
      "Train loss: 0.05669125335823212\n",
      "Test loss: 0.4130796798401409\n",
      "\n",
      "Epoch 436\n",
      "---------\n",
      "Train loss: 0.05668900857147362\n",
      "Test loss: 0.4087240282032225\n",
      "\n",
      "Epoch 437\n",
      "---------\n",
      "Train loss: 0.05692183424253017\n",
      "Test loss: 0.41839974787500167\n",
      "\n",
      "Epoch 438\n",
      "---------\n",
      "Train loss: 0.06370647963402527\n",
      "Test loss: 0.4040479370289379\n",
      "\n",
      "Epoch 439\n",
      "---------\n",
      "Train loss: 0.060190490330569446\n",
      "Test loss: 0.38218828125132454\n",
      "\n",
      "Epoch 440\n",
      "---------\n",
      "Train loss: 0.05897165306045541\n",
      "Test loss: 0.41950879825486076\n",
      "\n",
      "Epoch 441\n",
      "---------\n",
      "Train loss: 0.060868466655457656\n",
      "Test loss: 0.401683842142423\n",
      "\n",
      "Epoch 442\n",
      "---------\n",
      "Train loss: 0.0565323275513947\n",
      "Test loss: 0.42475317501359516\n",
      "\n",
      "Epoch 443\n",
      "---------\n",
      "Train loss: 0.05821828657229032\n",
      "Test loss: 0.4106569207376904\n",
      "\n",
      "Epoch 444\n",
      "---------\n",
      "Train loss: 0.06067549690072026\n",
      "Test loss: 0.40415658470657134\n",
      "\n",
      "Epoch 445\n",
      "---------\n",
      "Train loss: 0.055927051829972436\n",
      "Test loss: 0.39651219960716033\n",
      "\n",
      "Epoch 446\n",
      "---------\n",
      "Train loss: 0.056855504843822145\n",
      "Test loss: 0.4169807765218947\n",
      "\n",
      "Epoch 447\n",
      "---------\n",
      "Train loss: 0.05543633990159184\n",
      "Test loss: 0.41949740383360123\n",
      "\n",
      "Epoch 448\n",
      "---------\n",
      "Train loss: 0.063540396746248\n",
      "Test loss: 0.3923339123527209\n",
      "\n",
      "Epoch 449\n",
      "---------\n",
      "Train loss: 0.056951894058978984\n",
      "Test loss: 0.4046524051162932\n",
      "\n",
      "Epoch 450\n",
      "---------\n",
      "Train loss: 0.05835522782789277\n",
      "Test loss: 0.4135485233532058\n",
      "\n",
      "Epoch 451\n",
      "---------\n",
      "Train loss: 0.056949367779972296\n",
      "Test loss: 0.4069611272878117\n",
      "\n",
      "Epoch 452\n",
      "---------\n",
      "Train loss: 0.05878421968580889\n",
      "Test loss: 0.41363310399982667\n",
      "\n",
      "Epoch 453\n",
      "---------\n",
      "Train loss: 0.05661962068240557\n",
      "Test loss: 0.4059358537197113\n",
      "\n",
      "Epoch 454\n",
      "---------\n",
      "Train loss: 0.05738876330932336\n",
      "Test loss: 0.395408281021648\n",
      "\n",
      "Epoch 455\n",
      "---------\n",
      "Train loss: 0.05858976543614907\n",
      "Test loss: 0.40149713473187554\n",
      "\n",
      "Epoch 456\n",
      "---------\n",
      "Train loss: 0.058087883201161664\n",
      "Test loss: 0.4248754100667106\n",
      "\n",
      "Epoch 457\n",
      "---------\n",
      "Train loss: 0.05572744082227083\n",
      "Test loss: 0.4146649059322145\n",
      "\n",
      "Epoch 458\n",
      "---------\n",
      "Train loss: 0.055962993148049076\n",
      "Test loss: 0.38438273552391267\n",
      "\n",
      "Epoch 459\n",
      "---------\n",
      "Train loss: 0.05910053574812731\n",
      "Test loss: 0.3993321168753836\n",
      "\n",
      "Epoch 460\n",
      "---------\n",
      "Train loss: 0.05594884594237166\n",
      "Test loss: 0.4013129514124658\n",
      "\n",
      "Epoch 461\n",
      "---------\n",
      "Train loss: 0.056370368460193276\n",
      "Test loss: 0.40807052122222054\n",
      "\n",
      "Epoch 462\n",
      "---------\n",
      "Train loss: 0.055087100165630024\n",
      "Test loss: 0.40204208095868427\n",
      "\n",
      "Epoch 463\n",
      "---------\n",
      "Train loss: 0.057683213015219996\n",
      "Test loss: 0.3959210332896974\n",
      "\n",
      "Epoch 464\n",
      "---------\n",
      "Train loss: 0.05554439970624766\n",
      "Test loss: 0.42175380637248355\n",
      "\n",
      "Epoch 465\n",
      "---------\n",
      "Train loss: 0.05544739660607385\n",
      "Test loss: 0.39935628324747086\n",
      "\n",
      "Epoch 466\n",
      "---------\n",
      "Train loss: 0.05639866283828659\n",
      "Test loss: 0.3931047585275438\n",
      "\n",
      "Epoch 467\n",
      "---------\n",
      "Train loss: 0.06277155161036976\n",
      "Test loss: 0.4030053499672148\n",
      "\n",
      "Epoch 468\n",
      "---------\n",
      "Train loss: 0.05639485494508075\n",
      "Test loss: 0.42869364470243454\n",
      "\n",
      "Epoch 469\n",
      "---------\n",
      "Train loss: 0.054805988213047385\n",
      "Test loss: 0.3988368900285827\n",
      "\n",
      "Epoch 470\n",
      "---------\n",
      "Train loss: 0.058021838926444094\n",
      "Test loss: 0.3879193572534455\n",
      "\n",
      "Epoch 471\n",
      "---------\n",
      "Train loss: 0.05702516769192049\n",
      "Test loss: 0.3925413183040089\n",
      "\n",
      "Epoch 472\n",
      "---------\n",
      "Train loss: 0.05517120066256861\n",
      "Test loss: 0.4039892430106799\n",
      "\n",
      "Epoch 473\n",
      "---------\n",
      "Train loss: 0.05513747418964548\n",
      "Test loss: 0.3985915266805225\n",
      "\n",
      "Epoch 474\n",
      "---------\n",
      "Train loss: 0.05507321579248777\n",
      "Test loss: 0.4077947429484791\n",
      "\n",
      "Epoch 475\n",
      "---------\n",
      "Train loss: 0.059652685320803096\n",
      "Test loss: 0.4094533522923787\n",
      "\n",
      "Epoch 476\n",
      "---------\n",
      "Train loss: 0.06356779074329617\n",
      "Test loss: 0.3998930909567409\n",
      "\n",
      "Epoch 477\n",
      "---------\n",
      "Train loss: 0.054045063110866716\n",
      "Test loss: 0.36244020528263515\n",
      "\n",
      "Epoch 478\n",
      "---------\n",
      "Train loss: 0.06496954414927002\n",
      "Test loss: 0.4103633504774835\n",
      "\n",
      "Epoch 479\n",
      "---------\n",
      "Train loss: 0.05556749493760955\n",
      "Test loss: 0.4349485569530063\n",
      "\n",
      "Epoch 480\n",
      "---------\n",
      "Train loss: 0.0552270823557462\n",
      "Test loss: 0.3897308160861333\n",
      "\n",
      "Epoch 481\n",
      "---------\n",
      "Train loss: 0.05658740370667407\n",
      "Test loss: 0.40353023923105663\n",
      "\n",
      "Epoch 482\n",
      "---------\n",
      "Train loss: 0.05515004884052489\n",
      "Test loss: 0.40306271115938824\n",
      "\n",
      "Epoch 483\n",
      "---------\n",
      "Train loss: 0.06369756098969706\n",
      "Test loss: 0.38720522489812637\n",
      "\n",
      "Epoch 484\n",
      "---------\n",
      "Train loss: 0.05526731974844422\n",
      "Test loss: 0.3958898319138421\n",
      "\n",
      "Epoch 485\n",
      "---------\n",
      "Train loss: 0.0558063736285216\n",
      "Test loss: 0.38339731925063664\n",
      "\n",
      "Epoch 486\n",
      "---------\n",
      "Train loss: 0.05820219716822196\n",
      "Test loss: 0.4029821985297733\n",
      "\n",
      "Epoch 487\n",
      "---------\n",
      "Train loss: 0.05567291867919266\n",
      "Test loss: 0.4210355406006177\n",
      "\n",
      "Epoch 488\n",
      "---------\n",
      "Train loss: 0.056855040602386\n",
      "Test loss: 0.40374159730143017\n",
      "\n",
      "Epoch 489\n",
      "---------\n",
      "Train loss: 0.057956248721373935\n",
      "Test loss: 0.3939727279875014\n",
      "\n",
      "Epoch 490\n",
      "---------\n",
      "Train loss: 0.055952246666752865\n",
      "Test loss: 0.3854578071170383\n",
      "\n",
      "Epoch 491\n",
      "---------\n",
      "Train loss: 0.05478450480664573\n",
      "Test loss: 0.38560473173856735\n",
      "\n",
      "Epoch 492\n",
      "---------\n",
      "Train loss: 0.05722689886377858\n",
      "Test loss: 0.3977412068181568\n",
      "\n",
      "Epoch 493\n",
      "---------\n",
      "Train loss: 0.0569187729020736\n",
      "Test loss: 0.3981957394215796\n",
      "\n",
      "Epoch 494\n",
      "---------\n",
      "Train loss: 0.059330037146407576\n",
      "Test loss: 0.3854874653948678\n",
      "\n",
      "Epoch 495\n",
      "---------\n",
      "Train loss: 0.059805051140886335\n",
      "Test loss: 0.3886631222234832\n",
      "\n",
      "Epoch 496\n",
      "---------\n",
      "Train loss: 0.05985982942261866\n",
      "Test loss: 0.3948694144686063\n",
      "\n",
      "Epoch 497\n",
      "---------\n",
      "Train loss: 0.05527836895946946\n",
      "Test loss: 0.40002236515283585\n",
      "\n",
      "Epoch 498\n",
      "---------\n",
      "Train loss: 0.057693602849862406\n",
      "Test loss: 0.41234948734442395\n",
      "\n",
      "Epoch 499\n",
      "---------\n",
      "Train loss: 0.05455294325552781\n",
      "Test loss: 0.4039909483657943\n",
      "\n",
      "Epoch 500\n",
      "---------\n",
      "Train loss: 0.055594751079167636\n",
      "Test loss: 0.40163855585787034\n",
      "\n",
      "Epoch 501\n",
      "---------\n",
      "Train loss: 0.054094091980784596\n",
      "Test loss: 0.40265119075775146\n",
      "\n",
      "Epoch 502\n",
      "---------\n",
      "Train loss: 0.05974672274065337\n",
      "Test loss: 0.4152071815398004\n",
      "\n",
      "Epoch 503\n",
      "---------\n",
      "Train loss: 0.058855966930942874\n",
      "Test loss: 0.4064306318759918\n",
      "\n",
      "Epoch 504\n",
      "---------\n",
      "Train loss: 0.05379551451187581\n",
      "Test loss: 0.3911830170287026\n",
      "\n",
      "Epoch 505\n",
      "---------\n",
      "Train loss: 0.05379209753924182\n",
      "Test loss: 0.40657124999496674\n",
      "\n",
      "Epoch 506\n",
      "---------\n",
      "Train loss: 0.05687657929956913\n",
      "Test loss: 0.38655318402581745\n",
      "\n",
      "Epoch 507\n",
      "---------\n",
      "Train loss: 0.05462405860557088\n",
      "Test loss: 0.40477287852101856\n",
      "\n",
      "Epoch 508\n",
      "---------\n",
      "Train loss: 0.05372884909073556\n",
      "Test loss: 0.3950640128718482\n",
      "\n",
      "Epoch 509\n",
      "---------\n",
      "Train loss: 0.05335448094618706\n",
      "Test loss: 0.38198357489373946\n",
      "\n",
      "Epoch 510\n",
      "---------\n",
      "Train loss: 0.05520757656112047\n",
      "Test loss: 0.41937314305040574\n",
      "\n",
      "Epoch 511\n",
      "---------\n",
      "Train loss: 0.06112453607576234\n",
      "Test loss: 0.40655269970496494\n",
      "\n",
      "Epoch 512\n",
      "---------\n",
      "Train loss: 0.05413870776205191\n",
      "Test loss: 0.4123171957002746\n",
      "\n",
      "Epoch 513\n",
      "---------\n",
      "Train loss: 0.05371385322203943\n",
      "Test loss: 0.41749072074890137\n",
      "\n",
      "Epoch 514\n",
      "---------\n",
      "Train loss: 0.05373524347253676\n",
      "Test loss: 0.38043949918614495\n",
      "\n",
      "Epoch 515\n",
      "---------\n",
      "Train loss: 0.054722181820709793\n",
      "Test loss: 0.42029252068863976\n",
      "\n",
      "Epoch 516\n",
      "---------\n",
      "Train loss: 0.056026746735109816\n",
      "Test loss: 0.3966968564523591\n",
      "\n",
      "Epoch 517\n",
      "---------\n",
      "Train loss: 0.05669954763392785\n",
      "Test loss: 0.4060819504989518\n",
      "\n",
      "Epoch 518\n",
      "---------\n",
      "Train loss: 0.05544867221864739\n",
      "Test loss: 0.40882446120182675\n",
      "\n",
      "Epoch 519\n",
      "---------\n",
      "Train loss: 0.053908436509248404\n",
      "Test loss: 0.38376806759172016\n",
      "\n",
      "Epoch 520\n",
      "---------\n",
      "Train loss: 0.054248254193225875\n",
      "Test loss: 0.39053592334191006\n",
      "\n",
      "Epoch 521\n",
      "---------\n",
      "Train loss: 0.05330821553070564\n",
      "Test loss: 0.39203455713060165\n",
      "\n",
      "Epoch 522\n",
      "---------\n",
      "Train loss: 0.05418230687999832\n",
      "Test loss: 0.3997955711351501\n",
      "\n",
      "Epoch 523\n",
      "---------\n",
      "Train loss: 0.05927798717415759\n",
      "Test loss: 0.3992445882823732\n",
      "\n",
      "Epoch 524\n",
      "---------\n",
      "Train loss: 0.05454895838699615\n",
      "Test loss: 0.3929554786947038\n",
      "\n",
      "Epoch 525\n",
      "---------\n",
      "Train loss: 0.05468261070616011\n",
      "Test loss: 0.41159000827206504\n",
      "\n",
      "Epoch 526\n",
      "---------\n",
      "Train loss: 0.05346960873950073\n",
      "Test loss: 0.40970853964487713\n",
      "\n",
      "Epoch 527\n",
      "---------\n",
      "Train loss: 0.05312738725998705\n",
      "Test loss: 0.399817045364115\n",
      "\n",
      "Epoch 528\n",
      "---------\n",
      "Train loss: 0.05421728960105351\n",
      "Test loss: 0.39246487451924217\n",
      "\n",
      "Epoch 529\n",
      "---------\n",
      "Train loss: 0.053857399649651985\n",
      "Test loss: 0.40912170128689873\n",
      "\n",
      "Epoch 530\n",
      "---------\n",
      "Train loss: 0.05312300779457603\n",
      "Test loss: 0.4034777995612886\n",
      "\n",
      "Epoch 531\n",
      "---------\n",
      "Train loss: 0.052409957512281835\n",
      "Test loss: 0.3906252317958408\n",
      "\n",
      "Epoch 532\n",
      "---------\n",
      "Train loss: 0.05649973837924855\n",
      "Test loss: 0.40307388703028363\n",
      "\n",
      "Epoch 533\n",
      "---------\n",
      "Train loss: 0.05364228043845287\n",
      "Test loss: 0.3847466434041659\n",
      "\n",
      "Epoch 534\n",
      "---------\n",
      "Train loss: 0.05382237939297089\n",
      "Test loss: 0.40130285090870327\n",
      "\n",
      "Epoch 535\n",
      "---------\n",
      "Train loss: 0.054749233348827274\n",
      "Test loss: 0.41177788294023937\n",
      "\n",
      "Epoch 536\n",
      "---------\n",
      "Train loss: 0.05314570129849017\n",
      "Test loss: 0.40690944923294914\n",
      "\n",
      "Epoch 537\n",
      "---------\n",
      "Train loss: 0.055942804868599136\n",
      "Test loss: 0.3957608217994372\n",
      "\n",
      "Epoch 538\n",
      "---------\n",
      "Train loss: 0.054151379370263646\n",
      "Test loss: 0.3807775254050891\n",
      "\n",
      "Epoch 539\n",
      "---------\n",
      "Train loss: 0.05401510522434754\n",
      "Test loss: 0.38409273078044254\n",
      "\n",
      "Epoch 540\n",
      "---------\n",
      "Train loss: 0.05362311551081283\n",
      "Test loss: 0.4102534270948834\n",
      "\n",
      "Epoch 541\n",
      "---------\n",
      "Train loss: 0.05242064756540848\n",
      "Test loss: 0.40792691707611084\n",
      "\n",
      "Epoch 542\n",
      "---------\n",
      "Train loss: 0.052903405170322264\n",
      "Test loss: 0.39424265838331646\n",
      "\n",
      "Epoch 543\n",
      "---------\n",
      "Train loss: 0.060570208073061495\n",
      "Test loss: 0.3924318667915132\n",
      "\n",
      "Epoch 544\n",
      "---------\n",
      "Train loss: 0.052134322318514545\n",
      "Test loss: 0.398397554953893\n",
      "\n",
      "Epoch 545\n",
      "---------\n",
      "Train loss: 0.05300171171048922\n",
      "Test loss: 0.4063413292169571\n",
      "\n",
      "Epoch 546\n",
      "---------\n",
      "Train loss: 0.052735709334228886\n",
      "Test loss: 0.39965266982714337\n",
      "\n",
      "Epoch 547\n",
      "---------\n",
      "Train loss: 0.0537140317194696\n",
      "Test loss: 0.4030857566330168\n",
      "\n",
      "Epoch 548\n",
      "---------\n",
      "Train loss: 0.060000095749273896\n",
      "Test loss: 0.40015339354674023\n",
      "\n",
      "Epoch 549\n",
      "---------\n",
      "Train loss: 0.05308276042342186\n",
      "Test loss: 0.3997742889655961\n",
      "\n",
      "Epoch 550\n",
      "---------\n",
      "Train loss: 0.05476545280544087\n",
      "Test loss: 0.3979086677233378\n",
      "\n",
      "Epoch 551\n",
      "---------\n",
      "Train loss: 0.05246260276856317\n",
      "Test loss: 0.41283001254002255\n",
      "\n",
      "Epoch 552\n",
      "---------\n",
      "Train loss: 0.060238399576129656\n",
      "Test loss: 0.3902411601609654\n",
      "\n",
      "Epoch 553\n",
      "---------\n",
      "Train loss: 0.05350628129339644\n",
      "Test loss: 0.3954888234535853\n",
      "\n",
      "Epoch 554\n",
      "---------\n",
      "Train loss: 0.055025906135727255\n",
      "Test loss: 0.40050730605920154\n",
      "\n",
      "Epoch 555\n",
      "---------\n",
      "Train loss: 0.05375991462330733\n",
      "Test loss: 0.4161631622248226\n",
      "\n",
      "Epoch 556\n",
      "---------\n",
      "Train loss: 0.057643682862232835\n",
      "Test loss: 0.3970450419518683\n",
      "\n",
      "Epoch 557\n",
      "---------\n",
      "Train loss: 0.054473080925942795\n",
      "Test loss: 0.384853769507673\n",
      "\n",
      "Epoch 558\n",
      "---------\n",
      "Train loss: 0.06015535701798009\n",
      "Test loss: 0.408019299308459\n",
      "\n",
      "Epoch 559\n",
      "---------\n",
      "Train loss: 0.05642561336779701\n",
      "Test loss: 0.40050388624270755\n",
      "\n",
      "Epoch 560\n",
      "---------\n",
      "Train loss: 0.05272416231621589\n",
      "Test loss: 0.4239322460359997\n",
      "\n",
      "Epoch 561\n",
      "---------\n",
      "Train loss: 0.05318056071908878\n",
      "Test loss: 0.40508129447698593\n",
      "\n",
      "Epoch 562\n",
      "---------\n",
      "Train loss: 0.05209061105103631\n",
      "Test loss: 0.4171750959422853\n",
      "\n",
      "Epoch 563\n",
      "---------\n",
      "Train loss: 0.05444292577781847\n",
      "Test loss: 0.41098296642303467\n",
      "\n",
      "Epoch 564\n",
      "---------\n",
      "Train loss: 0.052224326986886026\n",
      "Test loss: 0.39641113579273224\n",
      "\n",
      "Epoch 565\n",
      "---------\n",
      "Train loss: 0.05465038130725069\n",
      "Test loss: 0.4033704714642631\n",
      "\n",
      "Epoch 566\n",
      "---------\n",
      "Train loss: 0.05303920477828277\n",
      "Test loss: 0.40578452828857636\n",
      "\n",
      "Epoch 567\n",
      "---------\n",
      "Train loss: 0.05368403697918568\n",
      "Test loss: 0.38784174621105194\n",
      "\n",
      "Epoch 568\n",
      "---------\n",
      "Train loss: 0.05663662488638822\n",
      "Test loss: 0.4282763534122043\n",
      "\n",
      "Epoch 569\n",
      "---------\n",
      "Train loss: 0.05418745526445231\n",
      "Test loss: 0.3959091562363837\n",
      "\n",
      "Epoch 570\n",
      "---------\n",
      "Train loss: 0.053819759915183694\n",
      "Test loss: 0.4081372328930431\n",
      "\n",
      "Epoch 571\n",
      "---------\n",
      "Train loss: 0.053444086790217886\n",
      "Test loss: 0.40575433025757474\n",
      "\n",
      "Epoch 572\n",
      "---------\n",
      "Train loss: 0.0558152928216649\n",
      "Test loss: 0.41883814417653614\n",
      "\n",
      "Epoch 573\n",
      "---------\n",
      "Train loss: 0.05359306951452579\n",
      "Test loss: 0.40073755714628434\n",
      "\n",
      "Epoch 574\n",
      "---------\n",
      "Train loss: 0.05452040752529034\n",
      "Test loss: 0.4430672509802712\n",
      "\n",
      "Epoch 575\n",
      "---------\n",
      "Train loss: 0.05281674760460321\n",
      "Test loss: 0.42022471461031174\n",
      "\n",
      "Epoch 576\n",
      "---------\n",
      "Train loss: 0.05195380263363144\n",
      "Test loss: 0.40487589024835163\n",
      "\n",
      "Epoch 577\n",
      "---------\n",
      "Train loss: 0.054660264813459696\n",
      "Test loss: 0.41242730617523193\n",
      "\n",
      "Epoch 578\n",
      "---------\n",
      "Train loss: 0.05288404572222914\n",
      "Test loss: 0.4076226055622101\n",
      "\n",
      "Epoch 579\n",
      "---------\n",
      "Train loss: 0.053522504633292556\n",
      "Test loss: 0.4082561060786247\n",
      "\n",
      "Epoch 580\n",
      "---------\n",
      "Train loss: 0.053351411379740706\n",
      "Test loss: 0.4052318086226781\n",
      "\n",
      "Epoch 581\n",
      "---------\n",
      "Train loss: 0.0548398458299094\n",
      "Test loss: 0.409885011613369\n",
      "\n",
      "Epoch 582\n",
      "---------\n",
      "Train loss: 0.053313810453151485\n",
      "Test loss: 0.4045187036196391\n",
      "\n",
      "Epoch 583\n",
      "---------\n",
      "Train loss: 0.054198115931025574\n",
      "Test loss: 0.4041064638230536\n",
      "\n",
      "Epoch 584\n",
      "---------\n",
      "Train loss: 0.05843141422207866\n",
      "Test loss: 0.40839144670301014\n",
      "\n",
      "Epoch 585\n",
      "---------\n",
      "Train loss: 0.05371304638434334\n",
      "Test loss: 0.41610702292786705\n",
      "\n",
      "Epoch 586\n",
      "---------\n",
      "Train loss: 0.055088027679760544\n",
      "Test loss: 0.39021312197049457\n",
      "\n",
      "Epoch 587\n",
      "---------\n",
      "Train loss: 0.053070056006877815\n",
      "Test loss: 0.4039670667714543\n",
      "\n",
      "Epoch 588\n",
      "---------\n",
      "Train loss: 0.052670070552267134\n",
      "Test loss: 0.4056394158138169\n",
      "\n",
      "Epoch 589\n",
      "---------\n",
      "Train loss: 0.05245657756209506\n",
      "Test loss: 0.39280174175898236\n",
      "\n",
      "Epoch 590\n",
      "---------\n",
      "Train loss: 0.05361685180105269\n",
      "Test loss: 0.40039245784282684\n",
      "\n",
      "Epoch 591\n",
      "---------\n",
      "Train loss: 0.0510646617081615\n",
      "Test loss: 0.4062522542145517\n",
      "\n",
      "Epoch 592\n",
      "---------\n",
      "Train loss: 0.05193862581758627\n",
      "Test loss: 0.3969159498810768\n",
      "\n",
      "Epoch 593\n",
      "---------\n",
      "Train loss: 0.053046279902836044\n",
      "Test loss: 0.41692256430784863\n",
      "\n",
      "Epoch 594\n",
      "---------\n",
      "Train loss: 0.055825914455843825\n",
      "Test loss: 0.4170987183849017\n",
      "\n",
      "Epoch 595\n",
      "---------\n",
      "Train loss: 0.051785964385739396\n",
      "Test loss: 0.3929188201824824\n",
      "\n",
      "Epoch 596\n",
      "---------\n",
      "Train loss: 0.05336327483278832\n",
      "Test loss: 0.4230370538102256\n",
      "\n",
      "Epoch 597\n",
      "---------\n",
      "Train loss: 0.05168139803156789\n",
      "Test loss: 0.3928165551688936\n",
      "\n",
      "Epoch 598\n",
      "---------\n",
      "Train loss: 0.05542108998633921\n",
      "Test loss: 0.408795995844735\n",
      "\n",
      "Epoch 599\n",
      "---------\n",
      "Train loss: 0.05200073657345326\n",
      "Test loss: 0.3878322160906262\n",
      "\n",
      "Epoch 600\n",
      "---------\n",
      "Train loss: 0.052989977644756436\n",
      "Test loss: 0.39582665844096077\n",
      "\n",
      "Epoch 601\n",
      "---------\n",
      "Train loss: 0.05228671972041151\n",
      "Test loss: 0.4109126304586728\n",
      "\n",
      "Epoch 602\n",
      "---------\n",
      "Train loss: 0.05188641493441537\n",
      "Test loss: 0.40225547138187623\n",
      "\n",
      "Epoch 603\n",
      "---------\n",
      "Train loss: 0.05397084312114332\n",
      "Test loss: 0.41459038936429554\n",
      "\n",
      "Epoch 604\n",
      "---------\n",
      "Train loss: 0.05660451160344694\n",
      "Test loss: 0.413364187710815\n",
      "\n",
      "Epoch 605\n",
      "---------\n",
      "Train loss: 0.056810194260573814\n",
      "Test loss: 0.4072062174479167\n",
      "\n",
      "Epoch 606\n",
      "---------\n",
      "Train loss: 0.054022295882792344\n",
      "Test loss: 0.4071706955631574\n",
      "\n",
      "Epoch 607\n",
      "---------\n",
      "Train loss: 0.05202688516250679\n",
      "Test loss: 0.4065419054693646\n",
      "\n",
      "Epoch 608\n",
      "---------\n",
      "Train loss: 0.05247403052635491\n",
      "Test loss: 0.40711694127983517\n",
      "\n",
      "Epoch 609\n",
      "---------\n",
      "Train loss: 0.05540526147732245\n",
      "Test loss: 0.39192135218116975\n",
      "\n",
      "Epoch 610\n",
      "---------\n",
      "Train loss: 0.05446769686282745\n",
      "Test loss: 0.4346623205476337\n",
      "\n",
      "Epoch 611\n",
      "---------\n",
      "Train loss: 0.05183480749838054\n",
      "Test loss: 0.40925726625654435\n",
      "\n",
      "Epoch 612\n",
      "---------\n",
      "Train loss: 0.05063580176543577\n",
      "Test loss: 0.39854175514645046\n",
      "\n",
      "Epoch 613\n",
      "---------\n",
      "Train loss: 0.05232932396964835\n",
      "Test loss: 0.4156675628489918\n",
      "\n",
      "Epoch 614\n",
      "---------\n",
      "Train loss: 0.051063400395962945\n",
      "Test loss: 0.41094203458891976\n",
      "\n",
      "Epoch 615\n",
      "---------\n",
      "Train loss: 0.051100277300325354\n",
      "Test loss: 0.4037424276272456\n",
      "\n",
      "Epoch 616\n",
      "---------\n",
      "Train loss: 0.05179491945143257\n",
      "Test loss: 0.4059090009993977\n",
      "\n",
      "Epoch 617\n",
      "---------\n",
      "Train loss: 0.054332065129918714\n",
      "Test loss: 0.4051337225569619\n",
      "\n",
      "Epoch 618\n",
      "---------\n",
      "Train loss: 0.052416166506840715\n",
      "Test loss: 0.4111108374264505\n",
      "\n",
      "Epoch 619\n",
      "---------\n",
      "Train loss: 0.0553503020367186\n",
      "Test loss: 0.3985496668352021\n",
      "\n",
      "Epoch 620\n",
      "---------\n",
      "Train loss: 0.0552099347114563\n",
      "Test loss: 0.3994372934103012\n",
      "\n",
      "Epoch 621\n",
      "---------\n",
      "Train loss: 0.0536581084098933\n",
      "Test loss: 0.41503794822427964\n",
      "\n",
      "Epoch 622\n",
      "---------\n",
      "Train loss: 0.051868170350124795\n",
      "Test loss: 0.41252676645914715\n",
      "\n",
      "Epoch 623\n",
      "---------\n",
      "Train loss: 0.05342085160581129\n",
      "Test loss: 0.39907366616858375\n",
      "\n",
      "Epoch 624\n",
      "---------\n",
      "Train loss: 0.05104919599502215\n",
      "Test loss: 0.40381290598048103\n",
      "\n",
      "Epoch 625\n",
      "---------\n",
      "Train loss: 0.05273535976552272\n",
      "Test loss: 0.42154497984382844\n",
      "\n",
      "Epoch 626\n",
      "---------\n",
      "Train loss: 0.052144603714168625\n",
      "Test loss: 0.41880960679716533\n",
      "\n",
      "Epoch 627\n",
      "---------\n",
      "Train loss: 0.052309775774899335\n",
      "Test loss: 0.41729455275668037\n",
      "\n",
      "Epoch 628\n",
      "---------\n",
      "Train loss: 0.05083747798926197\n",
      "Test loss: 0.4067564399705993\n",
      "\n",
      "Epoch 629\n",
      "---------\n",
      "Train loss: 0.05212800873310438\n",
      "Test loss: 0.4298591497871611\n",
      "\n",
      "Epoch 630\n",
      "---------\n",
      "Train loss: 0.056350259542731304\n",
      "Test loss: 0.4068169842163722\n",
      "\n",
      "Epoch 631\n",
      "---------\n",
      "Train loss: 0.055100757479002435\n",
      "Test loss: 0.41772714008887607\n",
      "\n",
      "Epoch 632\n",
      "---------\n",
      "Train loss: 0.05239793921022543\n",
      "Test loss: 0.436811947160297\n",
      "\n",
      "Epoch 633\n",
      "---------\n",
      "Train loss: 0.05341590234976528\n",
      "Test loss: 0.41019104503922993\n",
      "\n",
      "Epoch 634\n",
      "---------\n",
      "Train loss: 0.051528712801103084\n",
      "Test loss: 0.40937142156892353\n",
      "\n",
      "Epoch 635\n",
      "---------\n",
      "Train loss: 0.05246711168105581\n",
      "Test loss: 0.4123801622125838\n",
      "\n",
      "Epoch 636\n",
      "---------\n",
      "Train loss: 0.052473767061850855\n",
      "Test loss: 0.40862150324715507\n",
      "\n",
      "Epoch 637\n",
      "---------\n",
      "Train loss: 0.056146601314789484\n",
      "Test loss: 0.4024074085884624\n",
      "\n",
      "Epoch 638\n",
      "---------\n",
      "Train loss: 0.05492258273131613\n",
      "Test loss: 0.41885195589727825\n",
      "\n",
      "Epoch 639\n",
      "---------\n",
      "Train loss: 0.05232793578345861\n",
      "Test loss: 0.4228679421875212\n",
      "\n",
      "Epoch 640\n",
      "---------\n",
      "Train loss: 0.05219978879072836\n",
      "Test loss: 0.38940183487203384\n",
      "\n",
      "Epoch 641\n",
      "---------\n",
      "Train loss: 0.05199317193806304\n",
      "Test loss: 0.40942157639397514\n",
      "\n",
      "Epoch 642\n",
      "---------\n",
      "Train loss: 0.05294850328937173\n",
      "Test loss: 0.4221888863378101\n",
      "\n",
      "Epoch 643\n",
      "---------\n",
      "Train loss: 0.05106487135448593\n",
      "Test loss: 0.43203770617643994\n",
      "\n",
      "Epoch 644\n",
      "---------\n",
      "Train loss: 0.05060180073737034\n",
      "Test loss: 0.4169340680042903\n",
      "\n",
      "Epoch 645\n",
      "---------\n",
      "Train loss: 0.05069018424754696\n",
      "Test loss: 0.42074765927261776\n",
      "\n",
      "Epoch 646\n",
      "---------\n",
      "Train loss: 0.05007704946079424\n",
      "Test loss: 0.4197108629677031\n",
      "\n",
      "Epoch 647\n",
      "---------\n",
      "Train loss: 0.05114237388209274\n",
      "Test loss: 0.40483617617024314\n",
      "\n",
      "Epoch 648\n",
      "---------\n",
      "Train loss: 0.05049329636884587\n",
      "Test loss: 0.41275188823541004\n",
      "\n",
      "Epoch 649\n",
      "---------\n",
      "Train loss: 0.05417316960769573\n",
      "Test loss: 0.4106409135791991\n",
      "\n",
      "Epoch 650\n",
      "---------\n",
      "Train loss: 0.05209373832414193\n",
      "Test loss: 0.39736838307645583\n",
      "\n",
      "Epoch 651\n",
      "---------\n",
      "Train loss: 0.050247215532830784\n",
      "Test loss: 0.4128526689277755\n",
      "\n",
      "Epoch 652\n",
      "---------\n",
      "Train loss: 0.05975001326961709\n",
      "Test loss: 0.4124851980143123\n",
      "\n",
      "Epoch 653\n",
      "---------\n",
      "Train loss: 0.051272305651634396\n",
      "Test loss: 0.4074082233839565\n",
      "\n",
      "Epoch 654\n",
      "---------\n",
      "Train loss: 0.051353073346295526\n",
      "Test loss: 0.4205301155646642\n",
      "\n",
      "Epoch 655\n",
      "---------\n",
      "Train loss: 0.05105565048454862\n",
      "Test loss: 0.426066770321793\n",
      "\n",
      "Epoch 656\n",
      "---------\n",
      "Train loss: 0.05341648016058441\n",
      "Test loss: 0.4305620715022087\n",
      "\n",
      "Epoch 657\n",
      "---------\n",
      "Train loss: 0.050651503707350845\n",
      "Test loss: 0.41992411182986367\n",
      "\n",
      "Epoch 658\n",
      "---------\n",
      "Train loss: 0.05219250884173172\n",
      "Test loss: 0.4240064471960068\n",
      "\n",
      "Epoch 659\n",
      "---------\n",
      "Train loss: 0.049039754076927365\n",
      "Test loss: 0.4116751609577073\n",
      "\n",
      "Epoch 660\n",
      "---------\n",
      "Train loss: 0.05007157413222428\n",
      "Test loss: 0.42206738889217377\n",
      "\n",
      "Epoch 661\n",
      "---------\n",
      "Train loss: 0.04984435397532901\n",
      "Test loss: 0.4160766303539276\n",
      "\n",
      "Epoch 662\n",
      "---------\n",
      "Train loss: 0.05585550975852779\n",
      "Test loss: 0.42096851766109467\n",
      "\n",
      "Epoch 663\n",
      "---------\n",
      "Train loss: 0.05462755986289786\n",
      "Test loss: 0.4155961506896549\n",
      "\n",
      "Epoch 664\n",
      "---------\n",
      "Train loss: 0.05377603069479976\n",
      "Test loss: 0.4265577793121338\n",
      "\n",
      "Epoch 665\n",
      "---------\n",
      "Train loss: 0.049797839156651334\n",
      "Test loss: 0.42266299906704163\n",
      "\n",
      "Epoch 666\n",
      "---------\n",
      "Train loss: 0.050135742141199965\n",
      "Test loss: 0.42631546076801086\n",
      "\n",
      "Epoch 667\n",
      "---------\n",
      "Train loss: 0.049177132312530195\n",
      "Test loss: 0.41496911810504067\n",
      "\n",
      "Epoch 668\n",
      "---------\n",
      "Train loss: 0.05288153811956623\n",
      "Test loss: 0.43071437378724414\n",
      "\n",
      "Epoch 669\n",
      "---------\n",
      "Train loss: 0.05062525012596909\n",
      "Test loss: 0.4106939484675725\n",
      "\n",
      "Epoch 670\n",
      "---------\n",
      "Train loss: 0.05203814897686243\n",
      "Test loss: 0.415663911236657\n",
      "\n",
      "Epoch 671\n",
      "---------\n",
      "Train loss: 0.049262887403269166\n",
      "Test loss: 0.42725496076875263\n",
      "\n",
      "Epoch 672\n",
      "---------\n",
      "Train loss: 0.052573681177039235\n",
      "Test loss: 0.42612407108147937\n",
      "\n",
      "Epoch 673\n",
      "---------\n",
      "Train loss: 0.05012559834202485\n",
      "Test loss: 0.4054766587085194\n",
      "\n",
      "Epoch 674\n",
      "---------\n",
      "Train loss: 0.050690845253744295\n",
      "Test loss: 0.4311198501123322\n",
      "\n",
      "Epoch 675\n",
      "---------\n",
      "Train loss: 0.04971727013720998\n",
      "Test loss: 0.42520029346148175\n",
      "\n",
      "Epoch 676\n",
      "---------\n",
      "Train loss: 0.0523213346688343\n",
      "Test loss: 0.4133416141072909\n",
      "\n",
      "Epoch 677\n",
      "---------\n",
      "Train loss: 0.049667218167866976\n",
      "Test loss: 0.41669779767592746\n",
      "\n",
      "Epoch 678\n",
      "---------\n",
      "Train loss: 0.050670016290886064\n",
      "Test loss: 0.4373934351735645\n",
      "\n",
      "Epoch 679\n",
      "---------\n",
      "Train loss: 0.049428184516727924\n",
      "Test loss: 0.39807919495635563\n",
      "\n",
      "Epoch 680\n",
      "---------\n",
      "Train loss: 0.05617862755233156\n",
      "Test loss: 0.4059630201922523\n",
      "\n",
      "Epoch 681\n",
      "---------\n",
      "Train loss: 0.04922184662671368\n",
      "Test loss: 0.42676262226369643\n",
      "\n",
      "Epoch 682\n",
      "---------\n",
      "Train loss: 0.048492652511413326\n",
      "Test loss: 0.4170538816187117\n",
      "\n",
      "Epoch 683\n",
      "---------\n",
      "Train loss: 0.04914844072897852\n",
      "Test loss: 0.415839706444078\n",
      "\n",
      "Epoch 684\n",
      "---------\n",
      "Train loss: 0.05409744594778333\n",
      "Test loss: 0.4140276610851288\n",
      "\n",
      "Epoch 685\n",
      "---------\n",
      "Train loss: 0.05275568753547434\n",
      "Test loss: 0.4129670378234651\n",
      "\n",
      "Epoch 686\n",
      "---------\n",
      "Train loss: 0.05232828999448559\n",
      "Test loss: 0.42048799494902295\n",
      "\n",
      "Epoch 687\n",
      "---------\n",
      "Train loss: 0.05040446189897401\n",
      "Test loss: 0.4236196296082603\n",
      "\n",
      "Epoch 688\n",
      "---------\n",
      "Train loss: 0.04909760228890393\n",
      "Test loss: 0.4231313483582603\n",
      "\n",
      "Epoch 689\n",
      "---------\n",
      "Train loss: 0.05149652023932764\n",
      "Test loss: 0.4218468806809849\n",
      "\n",
      "Epoch 690\n",
      "---------\n",
      "Train loss: 0.051217332548860996\n",
      "Test loss: 0.40342579202519524\n",
      "\n",
      "Epoch 691\n",
      "---------\n",
      "Train loss: 0.0509095952979156\n",
      "Test loss: 0.41742802038788795\n",
      "\n",
      "Epoch 692\n",
      "---------\n",
      "Train loss: 0.05297706735187343\n",
      "Test loss: 0.4178715749747223\n",
      "\n",
      "Epoch 693\n",
      "---------\n",
      "Train loss: 0.05540850948143218\n",
      "Test loss: 0.4380625444981787\n",
      "\n",
      "Epoch 694\n",
      "---------\n",
      "Train loss: 0.05009954337895449\n",
      "Test loss: 0.4341496038768027\n",
      "\n",
      "Epoch 695\n",
      "---------\n",
      "Train loss: 0.04876776209649896\n",
      "Test loss: 0.4089323514037662\n",
      "\n",
      "Epoch 696\n",
      "---------\n",
      "Train loss: 0.0493505520436364\n",
      "Test loss: 0.42040270153019166\n",
      "\n",
      "Epoch 697\n",
      "---------\n",
      "Train loss: 0.049362630915961096\n",
      "Test loss: 0.4316948850949605\n",
      "\n",
      "Epoch 698\n",
      "---------\n",
      "Train loss: 0.049045809278530736\n",
      "Test loss: 0.4444776044951545\n",
      "\n",
      "Epoch 699\n",
      "---------\n",
      "Train loss: 0.05137236165215394\n",
      "Test loss: 0.4257659473352962\n",
      "\n",
      "Epoch 700\n",
      "---------\n",
      "Train loss: 0.048802509488138766\n",
      "Test loss: 0.4222221208943261\n",
      "\n",
      "Epoch 701\n",
      "---------\n",
      "Train loss: 0.049330205315657495\n",
      "Test loss: 0.45220813155174255\n",
      "\n",
      "Epoch 702\n",
      "---------\n",
      "Train loss: 0.04939750778222723\n",
      "Test loss: 0.43243065228064853\n",
      "\n",
      "Epoch 703\n",
      "---------\n",
      "Train loss: 0.04854259715648368\n",
      "Test loss: 0.4195619738764233\n",
      "\n",
      "Epoch 704\n",
      "---------\n",
      "Train loss: 0.051382689229545316\n",
      "Test loss: 0.4150516962011655\n",
      "\n",
      "Epoch 705\n",
      "---------\n",
      "Train loss: 0.050846008534011035\n",
      "Test loss: 0.42900576690832776\n",
      "\n",
      "Epoch 706\n",
      "---------\n",
      "Train loss: 0.05541550013835409\n",
      "Test loss: 0.41133151782883537\n",
      "\n",
      "Epoch 707\n",
      "---------\n",
      "Train loss: 0.04826950920479638\n",
      "Test loss: 0.4279824611213472\n",
      "\n",
      "Epoch 708\n",
      "---------\n",
      "Train loss: 0.04931677265891007\n",
      "Test loss: 0.4270053141646915\n",
      "\n",
      "Epoch 709\n",
      "---------\n",
      "Train loss: 0.05161126939180706\n",
      "Test loss: 0.4342196649975247\n",
      "\n",
      "Epoch 710\n",
      "---------\n",
      "Train loss: 0.04903949339807566\n",
      "Test loss: 0.4238176602456305\n",
      "\n",
      "Epoch 711\n",
      "---------\n",
      "Train loss: 0.04956394766590425\n",
      "Test loss: 0.4307728691233529\n",
      "\n",
      "Epoch 712\n",
      "---------\n",
      "Train loss: 0.047954968575920374\n",
      "Test loss: 0.4279928372965919\n",
      "\n",
      "Epoch 713\n",
      "---------\n",
      "Train loss: 0.05222375863896949\n",
      "Test loss: 0.4442630832393964\n",
      "\n",
      "Epoch 714\n",
      "---------\n",
      "Train loss: 0.04939960327309174\n",
      "Test loss: 0.41958689606852\n",
      "\n",
      "Epoch 715\n",
      "---------\n",
      "Train loss: 0.04821144143794039\n",
      "Test loss: 0.43210089206695557\n",
      "\n",
      "Epoch 716\n",
      "---------\n",
      "Train loss: 0.047518914698490074\n",
      "Test loss: 0.43361014790005153\n",
      "\n",
      "Epoch 717\n",
      "---------\n",
      "Train loss: 0.04858723636763378\n",
      "Test loss: 0.43651119122902554\n",
      "\n",
      "Epoch 718\n",
      "---------\n",
      "Train loss: 0.047420494043365125\n",
      "Test loss: 0.4356404220064481\n",
      "\n",
      "Epoch 719\n",
      "---------\n",
      "Train loss: 0.049150138145445714\n",
      "Test loss: 0.42751119782527286\n",
      "\n",
      "Epoch 720\n",
      "---------\n",
      "Train loss: 0.04806471784000418\n",
      "Test loss: 0.4324978076749378\n",
      "\n",
      "Epoch 721\n",
      "---------\n",
      "Train loss: 0.05197879906544196\n",
      "Test loss: 0.45173009236653644\n",
      "\n",
      "Epoch 722\n",
      "---------\n",
      "Train loss: 0.050125699082855135\n",
      "Test loss: 0.4463330101635721\n",
      "\n",
      "Epoch 723\n",
      "---------\n",
      "Train loss: 0.049405695288442075\n",
      "Test loss: 0.42972125195794636\n",
      "\n",
      "Epoch 724\n",
      "---------\n",
      "Train loss: 0.04816915071569383\n",
      "Test loss: 0.42044106705321205\n",
      "\n",
      "Epoch 725\n",
      "---------\n",
      "Train loss: 0.05424084218351969\n",
      "Test loss: 0.438951313495636\n",
      "\n",
      "Epoch 726\n",
      "---------\n",
      "Train loss: 0.052906148468277285\n",
      "Test loss: 0.4215782582759857\n",
      "\n",
      "Epoch 727\n",
      "---------\n",
      "Train loss: 0.05046021778668676\n",
      "Test loss: 0.4251698801914851\n",
      "\n",
      "Epoch 728\n",
      "---------\n",
      "Train loss: 0.048716549262670536\n",
      "Test loss: 0.44162677890724605\n",
      "\n",
      "Epoch 729\n",
      "---------\n",
      "Train loss: 0.04843303417354556\n",
      "Test loss: 0.431064173579216\n",
      "\n",
      "Epoch 730\n",
      "---------\n",
      "Train loss: 0.0481005012656429\n",
      "Test loss: 0.44497548209296334\n",
      "\n",
      "Epoch 731\n",
      "---------\n",
      "Train loss: 0.048490655433852226\n",
      "Test loss: 0.43636321938700146\n",
      "\n",
      "Epoch 732\n",
      "---------\n",
      "Train loss: 0.05005000164133629\n",
      "Test loss: 0.4366035709778468\n",
      "\n",
      "Epoch 733\n",
      "---------\n",
      "Train loss: 0.04814638962437\n",
      "Test loss: 0.43971944351991016\n",
      "\n",
      "Epoch 734\n",
      "---------\n",
      "Train loss: 0.04712678973529754\n",
      "Test loss: 0.43308601114485\n",
      "\n",
      "Epoch 735\n",
      "---------\n",
      "Train loss: 0.04990866330419002\n",
      "Test loss: 0.4417474294702212\n",
      "\n",
      "Epoch 736\n",
      "---------\n",
      "Train loss: 0.04873850274764534\n",
      "Test loss: 0.43439923061264885\n",
      "\n",
      "Epoch 737\n",
      "---------\n",
      "Train loss: 0.049407251445310454\n",
      "Test loss: 0.42947196546528077\n",
      "\n",
      "Epoch 738\n",
      "---------\n",
      "Train loss: 0.04904473203766559\n",
      "Test loss: 0.44202466640207505\n",
      "\n",
      "Epoch 739\n",
      "---------\n",
      "Train loss: 0.04901007288468203\n",
      "Test loss: 0.4177754173676173\n",
      "\n",
      "Epoch 740\n",
      "---------\n",
      "Train loss: 0.04766490490042737\n",
      "Test loss: 0.4560369931989246\n",
      "\n",
      "Epoch 741\n",
      "---------\n",
      "Train loss: 0.04767156411045497\n",
      "Test loss: 0.4360055915183491\n",
      "\n",
      "Epoch 742\n",
      "---------\n",
      "Train loss: 0.052642180068817525\n",
      "Test loss: 0.4445606420437495\n",
      "\n",
      "Epoch 743\n",
      "---------\n",
      "Train loss: 0.049367860674725046\n",
      "Test loss: 0.44487106055021286\n",
      "\n",
      "Epoch 744\n",
      "---------\n",
      "Train loss: 0.048394492927140424\n",
      "Test loss: 0.4309052866366174\n",
      "\n",
      "Epoch 745\n",
      "---------\n",
      "Train loss: 0.05054419590825481\n",
      "Test loss: 0.4534837214483155\n",
      "\n",
      "Epoch 746\n",
      "---------\n",
      "Train loss: 0.051573516501645954\n",
      "Test loss: 0.4495840511388249\n",
      "\n",
      "Epoch 747\n",
      "---------\n",
      "Train loss: 0.04679401989726882\n",
      "Test loss: 0.4472879221041997\n",
      "\n",
      "Epoch 748\n",
      "---------\n",
      "Train loss: 0.04887990826474769\n",
      "Test loss: 0.4451936168803109\n",
      "\n",
      "Epoch 749\n",
      "---------\n",
      "Train loss: 0.049427904321679046\n",
      "Test loss: 0.44214091698328656\n",
      "\n",
      "Epoch 750\n",
      "---------\n",
      "Train loss: 0.046673683798871934\n",
      "Test loss: 0.44535693029562634\n",
      "\n",
      "Epoch 751\n",
      "---------\n",
      "Train loss: 0.04629477758966719\n",
      "Test loss: 0.4357144551144706\n",
      "\n",
      "Epoch 752\n",
      "---------\n",
      "Train loss: 0.048978413821065\n",
      "Test loss: 0.4471222112576167\n",
      "\n",
      "Epoch 753\n",
      "---------\n",
      "Train loss: 0.04932514792640826\n",
      "Test loss: 0.4488988510436482\n",
      "\n",
      "Epoch 754\n",
      "---------\n",
      "Train loss: 0.05008801891069327\n",
      "Test loss: 0.44256704383426243\n",
      "\n",
      "Epoch 755\n",
      "---------\n",
      "Train loss: 0.048819021155525534\n",
      "Test loss: 0.44268163210815853\n",
      "\n",
      "Epoch 756\n",
      "---------\n",
      "Train loss: 0.04916350605864344\n",
      "Test loss: 0.44866111212306553\n",
      "\n",
      "Epoch 757\n",
      "---------\n",
      "Train loss: 0.04762373653440071\n",
      "Test loss: 0.4330712813470099\n",
      "\n",
      "Epoch 758\n",
      "---------\n",
      "Train loss: 0.048184379702433944\n",
      "Test loss: 0.4382111488117112\n",
      "\n",
      "Epoch 759\n",
      "---------\n",
      "Train loss: 0.04734476702287793\n",
      "Test loss: 0.44976821541786194\n",
      "\n",
      "Epoch 760\n",
      "---------\n",
      "Train loss: 0.04866691239710365\n",
      "Test loss: 0.44006358914905125\n",
      "\n",
      "Epoch 761\n",
      "---------\n",
      "Train loss: 0.04782982098861664\n",
      "Test loss: 0.4431403585606151\n",
      "\n",
      "Epoch 762\n",
      "---------\n",
      "Train loss: 0.045255858490626064\n",
      "Test loss: 0.4429212096664641\n",
      "\n",
      "Epoch 763\n",
      "---------\n",
      "Train loss: 0.04674254806845316\n",
      "Test loss: 0.4621701290210088\n",
      "\n",
      "Epoch 764\n",
      "---------\n",
      "Train loss: 0.045299719428710104\n",
      "Test loss: 0.43667589128017426\n",
      "\n",
      "Epoch 765\n",
      "---------\n",
      "Train loss: 0.04733980416287003\n",
      "Test loss: 0.4457429912355211\n",
      "\n",
      "Epoch 766\n",
      "---------\n",
      "Train loss: 0.04548201818917213\n",
      "Test loss: 0.43697776397069293\n",
      "\n",
      "Epoch 767\n",
      "---------\n",
      "Train loss: 0.04936520453442687\n",
      "Test loss: 0.4556457797686259\n",
      "\n",
      "Epoch 768\n",
      "---------\n",
      "Train loss: 0.04995202231553516\n",
      "Test loss: 0.45632125561436016\n",
      "\n",
      "Epoch 769\n",
      "---------\n",
      "Train loss: 0.047220618780037124\n",
      "Test loss: 0.46303710920943153\n",
      "\n",
      "Epoch 770\n",
      "---------\n",
      "Train loss: 0.04601943755863301\n",
      "Test loss: 0.43216891090075177\n",
      "\n",
      "Epoch 771\n",
      "---------\n",
      "Train loss: 0.04508636700588146\n",
      "Test loss: 0.45171671940220726\n",
      "\n",
      "Epoch 772\n",
      "---------\n",
      "Train loss: 0.04552896986673919\n",
      "Test loss: 0.45381519115633434\n",
      "\n",
      "Epoch 773\n",
      "---------\n",
      "Train loss: 0.044981922750594094\n",
      "Test loss: 0.46354763623740936\n",
      "\n",
      "Epoch 774\n",
      "---------\n",
      "Train loss: 0.045152808838923066\n",
      "Test loss: 0.4338325452473428\n",
      "\n",
      "Epoch 775\n",
      "---------\n",
      "Train loss: 0.04535891396725284\n",
      "Test loss: 0.45031193892161053\n",
      "\n",
      "Epoch 776\n",
      "---------\n",
      "Train loss: 0.048182588386615474\n",
      "Test loss: 0.4770897614459197\n",
      "\n",
      "Epoch 777\n",
      "---------\n",
      "Train loss: 0.04743274760299495\n",
      "Test loss: 0.4705290173490842\n",
      "\n",
      "Epoch 778\n",
      "---------\n",
      "Train loss: 0.04929188903354641\n",
      "Test loss: 0.4601854433616002\n",
      "\n",
      "Epoch 779\n",
      "---------\n",
      "Train loss: 0.04683086851478687\n",
      "Test loss: 0.4445807900693681\n",
      "\n",
      "Epoch 780\n",
      "---------\n",
      "Train loss: 0.04527781018259702\n",
      "Test loss: 0.45508695145448047\n",
      "\n",
      "Epoch 781\n",
      "---------\n",
      "Train loss: 0.04755051311803982\n",
      "Test loss: 0.4544862086574237\n",
      "\n",
      "Epoch 782\n",
      "---------\n",
      "Train loss: 0.04580138626456736\n",
      "Test loss: 0.43623604791031945\n",
      "\n",
      "Epoch 783\n",
      "---------\n",
      "Train loss: 0.04553355340405168\n",
      "Test loss: 0.4569440624780125\n",
      "\n",
      "Epoch 784\n",
      "---------\n",
      "Train loss: 0.04745153201344822\n",
      "Test loss: 0.4554564182957013\n",
      "\n",
      "Epoch 785\n",
      "---------\n",
      "Train loss: 0.04538710279822616\n",
      "Test loss: 0.45354432861010235\n",
      "\n",
      "Epoch 786\n",
      "---------\n",
      "Train loss: 0.045394268219492266\n",
      "Test loss: 0.4639809773200088\n",
      "\n",
      "Epoch 787\n",
      "---------\n",
      "Train loss: 0.04535531833867675\n",
      "Test loss: 0.4416309901409679\n",
      "\n",
      "Epoch 788\n",
      "---------\n",
      "Train loss: 0.04809854168810749\n",
      "Test loss: 0.46517286491062904\n",
      "\n",
      "Epoch 789\n",
      "---------\n",
      "Train loss: 0.04531909415631422\n",
      "Test loss: 0.4503278136253357\n",
      "\n",
      "Epoch 790\n",
      "---------\n",
      "Train loss: 0.04551012999477929\n",
      "Test loss: 0.4656336067451371\n",
      "\n",
      "Epoch 791\n",
      "---------\n",
      "Train loss: 0.04496112309529313\n",
      "Test loss: 0.44617558353477055\n",
      "\n",
      "Epoch 792\n",
      "---------\n",
      "Train loss: 0.04522266511672309\n",
      "Test loss: 0.45045744793282616\n",
      "\n",
      "Epoch 793\n",
      "---------\n",
      "Train loss: 0.04516549598026488\n",
      "Test loss: 0.46611452102661133\n",
      "\n",
      "Epoch 794\n",
      "---------\n",
      "Train loss: 0.04849581335604723\n",
      "Test loss: 0.45145031644238365\n",
      "\n",
      "Epoch 795\n",
      "---------\n",
      "Train loss: 0.05161323684400746\n",
      "Test loss: 0.4456679191854265\n",
      "\n",
      "Epoch 796\n",
      "---------\n",
      "Train loss: 0.04577715295766081\n",
      "Test loss: 0.476591511319081\n",
      "\n",
      "Epoch 797\n",
      "---------\n",
      "Train loss: 0.04515535068432135\n",
      "Test loss: 0.48815083007017773\n",
      "\n",
      "Epoch 798\n",
      "---------\n",
      "Train loss: 0.045588270195626786\n",
      "Test loss: 0.4493681738773982\n",
      "\n",
      "Epoch 799\n",
      "---------\n",
      "Train loss: 0.047518385108560324\n",
      "Test loss: 0.4607488951749272\n",
      "\n",
      "Epoch 800\n",
      "---------\n",
      "Train loss: 0.052496607548424175\n",
      "Test loss: 0.4807400806910462\n",
      "\n",
      "Epoch 801\n",
      "---------\n",
      "Train loss: 0.04651354649104178\n",
      "Test loss: 0.4605875652697351\n",
      "\n",
      "Epoch 802\n",
      "---------\n",
      "Train loss: 0.04558846736992044\n",
      "Test loss: 0.4805612646871143\n",
      "\n",
      "Epoch 803\n",
      "---------\n",
      "Train loss: 0.04666214765581701\n",
      "Test loss: 0.4711676637331645\n",
      "\n",
      "Epoch 804\n",
      "---------\n",
      "Train loss: 0.04384184915293839\n",
      "Test loss: 0.468126290374332\n",
      "\n",
      "Epoch 805\n",
      "---------\n",
      "Train loss: 0.04653092219294714\n",
      "Test loss: 0.4794548149738047\n",
      "\n",
      "Epoch 806\n",
      "---------\n",
      "Train loss: 0.044417123343529444\n",
      "Test loss: 0.48413212680154377\n",
      "\n",
      "Epoch 807\n",
      "---------\n",
      "Train loss: 0.04760354710742831\n",
      "Test loss: 0.47479855600330567\n",
      "\n",
      "Epoch 808\n",
      "---------\n",
      "Train loss: 0.04354147168175716\n",
      "Test loss: 0.4936217587027285\n",
      "\n",
      "Epoch 809\n",
      "---------\n",
      "Train loss: 0.04969089385122061\n",
      "Test loss: 0.4787461840444141\n",
      "\n",
      "Epoch 810\n",
      "---------\n",
      "Train loss: 0.04375262378848025\n",
      "Test loss: 0.4733552568488651\n",
      "\n",
      "Epoch 811\n",
      "---------\n",
      "Train loss: 0.04362361904765878\n",
      "Test loss: 0.4776395186781883\n",
      "\n",
      "Epoch 812\n",
      "---------\n",
      "Train loss: 0.04690169566310942\n",
      "Test loss: 0.4601886437998878\n",
      "\n",
      "Epoch 813\n",
      "---------\n",
      "Train loss: 0.05040374320898471\n",
      "Test loss: 0.46757041580147213\n",
      "\n",
      "Epoch 814\n",
      "---------\n",
      "Train loss: 0.04551478731445968\n",
      "Test loss: 0.49031269715891945\n",
      "\n",
      "Epoch 815\n",
      "---------\n",
      "Train loss: 0.04411790167380657\n",
      "Test loss: 0.4538760268025928\n",
      "\n",
      "Epoch 816\n",
      "---------\n",
      "Train loss: 0.043258160760160536\n",
      "Test loss: 0.4791420598824819\n",
      "\n",
      "Epoch 817\n",
      "---------\n",
      "Train loss: 0.042887327972234095\n",
      "Test loss: 0.48019042362769443\n",
      "\n",
      "Epoch 818\n",
      "---------\n",
      "Train loss: 0.043070819665445015\n",
      "Test loss: 0.479135033984979\n",
      "\n",
      "Epoch 819\n",
      "---------\n",
      "Train loss: 0.042837145841921095\n",
      "Test loss: 0.4663877797623475\n",
      "\n",
      "Epoch 820\n",
      "---------\n",
      "Train loss: 0.050028768966772726\n",
      "Test loss: 0.48269401656256783\n",
      "\n",
      "Epoch 821\n",
      "---------\n",
      "Train loss: 0.04313619891347896\n",
      "Test loss: 0.48568659358554417\n",
      "\n",
      "Epoch 822\n",
      "---------\n",
      "Train loss: 0.04253944991562873\n",
      "Test loss: 0.4718598988321092\n",
      "\n",
      "Epoch 823\n",
      "---------\n",
      "Train loss: 0.044475801255820055\n",
      "Test loss: 0.46505415274037254\n",
      "\n",
      "Epoch 824\n",
      "---------\n",
      "Train loss: 0.04323075656845633\n",
      "Test loss: 0.46650954832633335\n",
      "\n",
      "Epoch 825\n",
      "---------\n",
      "Train loss: 0.043128646278221695\n",
      "Test loss: 0.48157793697383666\n",
      "\n",
      "Epoch 826\n",
      "---------\n",
      "Train loss: 0.04752975682328854\n",
      "Test loss: 0.46179445998536217\n",
      "\n",
      "Epoch 827\n",
      "---------\n",
      "Train loss: 0.042411998113883395\n",
      "Test loss: 0.47138335307439166\n",
      "\n",
      "Epoch 828\n",
      "---------\n",
      "Train loss: 0.04346877067083759\n",
      "Test loss: 0.48530933260917664\n",
      "\n",
      "Epoch 829\n",
      "---------\n",
      "Train loss: 0.044661815968408645\n",
      "Test loss: 0.49164041462871766\n",
      "\n",
      "Epoch 830\n",
      "---------\n",
      "Train loss: 0.043116016197018325\n",
      "Test loss: 0.4586962006158299\n",
      "\n",
      "Epoch 831\n",
      "---------\n",
      "Train loss: 0.04380460372859878\n",
      "Test loss: 0.4741131365299225\n",
      "\n",
      "Epoch 832\n",
      "---------\n",
      "Train loss: 0.045538258173369935\n",
      "Test loss: 0.48178302662240136\n",
      "\n",
      "Epoch 833\n",
      "---------\n",
      "Train loss: 0.043115078238770366\n",
      "Test loss: 0.4810856220622857\n",
      "\n",
      "Epoch 834\n",
      "---------\n",
      "Train loss: 0.042111206600176435\n",
      "Test loss: 0.4888320457604196\n",
      "\n",
      "Epoch 835\n",
      "---------\n",
      "Train loss: 0.041106163855277246\n",
      "Test loss: 0.47486769987477195\n",
      "\n",
      "Epoch 836\n",
      "---------\n",
      "Train loss: 0.048889732537125904\n",
      "Test loss: 0.48564140415853924\n",
      "\n",
      "Epoch 837\n",
      "---------\n",
      "Train loss: 0.04567782945066158\n",
      "Test loss: 0.46835921828945476\n",
      "\n",
      "Epoch 838\n",
      "---------\n",
      "Train loss: 0.043175326335975636\n",
      "Test loss: 0.5032929629087448\n",
      "\n",
      "Epoch 839\n",
      "---------\n",
      "Train loss: 0.042464903747064194\n",
      "Test loss: 0.47311706013149685\n",
      "\n",
      "Epoch 840\n",
      "---------\n",
      "Train loss: 0.042794178040432077\n",
      "Test loss: 0.46944601502683425\n",
      "\n",
      "Epoch 841\n",
      "---------\n",
      "Train loss: 0.04549270836702947\n",
      "Test loss: 0.4872878972027037\n",
      "\n",
      "Epoch 842\n",
      "---------\n",
      "Train loss: 0.04295810222226594\n",
      "Test loss: 0.48308851445714635\n",
      "\n",
      "Epoch 843\n",
      "---------\n",
      "Train loss: 0.04272668079024048\n",
      "Test loss: 0.47519956280787784\n",
      "\n",
      "Epoch 844\n",
      "---------\n",
      "Train loss: 0.04492620180826634\n",
      "Test loss: 0.4852703470322821\n",
      "\n",
      "Epoch 845\n",
      "---------\n",
      "Train loss: 0.04317482341346996\n",
      "Test loss: 0.4892522113190757\n",
      "\n",
      "Epoch 846\n",
      "---------\n",
      "Train loss: 0.04697893447675077\n",
      "Test loss: 0.459615472290251\n",
      "\n",
      "Epoch 847\n",
      "---------\n",
      "Train loss: 0.044495925307273865\n",
      "Test loss: 0.47963439631793237\n",
      "\n",
      "Epoch 848\n",
      "---------\n",
      "Train loss: 0.043532916613392966\n",
      "Test loss: 0.48658734891149735\n",
      "\n",
      "Epoch 849\n",
      "---------\n",
      "Train loss: 0.045713517166274996\n",
      "Test loss: 0.49291329085826874\n",
      "\n",
      "Epoch 850\n",
      "---------\n",
      "Train loss: 0.04198818144920681\n",
      "Test loss: 0.47892708745267654\n",
      "\n",
      "Epoch 851\n",
      "---------\n",
      "Train loss: 0.04170295634084858\n",
      "Test loss: 0.5245786470671495\n",
      "\n",
      "Epoch 852\n",
      "---------\n",
      "Train loss: 0.04256171962645437\n",
      "Test loss: 0.471622772100899\n",
      "\n",
      "Epoch 853\n",
      "---------\n",
      "Train loss: 0.04069371180542346\n",
      "Test loss: 0.48713911241955227\n",
      "\n",
      "Epoch 854\n",
      "---------\n",
      "Train loss: 0.04261708279539432\n",
      "Test loss: 0.49435172975063324\n",
      "\n",
      "Epoch 855\n",
      "---------\n",
      "Train loss: 0.041478305689192245\n",
      "Test loss: 0.48674775742822224\n",
      "\n",
      "Epoch 856\n",
      "---------\n",
      "Train loss: 0.04026225246239586\n",
      "Test loss: 0.4897613533669048\n",
      "\n",
      "Epoch 857\n",
      "---------\n",
      "Train loss: 0.04115823144093156\n",
      "Test loss: 0.48426857797635925\n",
      "\n",
      "Epoch 858\n",
      "---------\n",
      "Train loss: 0.04184771299229136\n",
      "Test loss: 0.4935514372256067\n",
      "\n",
      "Epoch 859\n",
      "---------\n",
      "Train loss: 0.04170445309552763\n",
      "Test loss: 0.48554907821946675\n",
      "\n",
      "Epoch 860\n",
      "---------\n",
      "Train loss: 0.04263604725045817\n",
      "Test loss: 0.4887750181886885\n",
      "\n",
      "Epoch 861\n",
      "---------\n",
      "Train loss: 0.04092736894173348\n",
      "Test loss: 0.4802354855669869\n",
      "\n",
      "Epoch 862\n",
      "---------\n",
      "Train loss: 0.04179846402257681\n",
      "Test loss: 0.484392153720061\n",
      "\n",
      "Epoch 863\n",
      "---------\n",
      "Train loss: 0.04160538305794554\n",
      "Test loss: 0.5194978780216641\n",
      "\n",
      "Epoch 864\n",
      "---------\n",
      "Train loss: 0.04167141136713326\n",
      "Test loss: 0.49780059936973786\n",
      "\n",
      "Epoch 865\n",
      "---------\n",
      "Train loss: 0.04660326435363719\n",
      "Test loss: 0.49062041317423183\n",
      "\n",
      "Epoch 866\n",
      "---------\n",
      "Train loss: 0.04039384637560163\n",
      "Test loss: 0.4922020634015401\n",
      "\n",
      "Epoch 867\n",
      "---------\n",
      "Train loss: 0.04295229553411316\n",
      "Test loss: 0.4969048976070351\n",
      "\n",
      "Epoch 868\n",
      "---------\n",
      "Train loss: 0.04322449931143118\n",
      "Test loss: 0.4878547257847256\n",
      "\n",
      "Epoch 869\n",
      "---------\n",
      "Train loss: 0.04638041612426085\n",
      "Test loss: 0.49741942725247806\n",
      "\n",
      "Epoch 870\n",
      "---------\n",
      "Train loss: 0.04158360753120048\n",
      "Test loss: 0.48762303673558766\n",
      "\n",
      "Epoch 871\n",
      "---------\n",
      "Train loss: 0.04040946005677272\n",
      "Test loss: 0.4848194784588284\n",
      "\n",
      "Epoch 872\n",
      "---------\n",
      "Train loss: 0.038775252741678354\n",
      "Test loss: 0.5116898922456635\n",
      "\n",
      "Epoch 873\n",
      "---------\n",
      "Train loss: 0.04021854469153498\n",
      "Test loss: 0.4884149494270484\n",
      "\n",
      "Epoch 874\n",
      "---------\n",
      "Train loss: 0.03930995608633176\n",
      "Test loss: 0.48958288464281297\n",
      "\n",
      "Epoch 875\n",
      "---------\n",
      "Train loss: 0.040499072222571285\n",
      "Test loss: 0.495899544407924\n",
      "\n",
      "Epoch 876\n",
      "---------\n",
      "Train loss: 0.04008176287672021\n",
      "Test loss: 0.4980630237195227\n",
      "\n",
      "Epoch 877\n",
      "---------\n",
      "Train loss: 0.04222476133145392\n",
      "Test loss: 0.49051255236069363\n",
      "\n",
      "Epoch 878\n",
      "---------\n",
      "Train loss: 0.04072772651228921\n",
      "Test loss: 0.5028580178817114\n",
      "\n",
      "Epoch 879\n",
      "---------\n",
      "Train loss: 0.043317332140369605\n",
      "Test loss: 0.4978100864423646\n",
      "\n",
      "Epoch 880\n",
      "---------\n",
      "Train loss: 0.04540398883234177\n",
      "Test loss: 0.49698417716556126\n",
      "\n",
      "Epoch 881\n",
      "---------\n",
      "Train loss: 0.04169201616397394\n",
      "Test loss: 0.48560121117366684\n",
      "\n",
      "Epoch 882\n",
      "---------\n",
      "Train loss: 0.04535758375589337\n",
      "Test loss: 0.4997350855006112\n",
      "\n",
      "Epoch 883\n",
      "---------\n",
      "Train loss: 0.03959608517990481\n",
      "Test loss: 0.5045619491073821\n",
      "\n",
      "Epoch 884\n",
      "---------\n",
      "Train loss: 0.03960490811316829\n",
      "Test loss: 0.4928513372109996\n",
      "\n",
      "Epoch 885\n",
      "---------\n",
      "Train loss: 0.04123269315875534\n",
      "Test loss: 0.4916425276961591\n",
      "\n",
      "Epoch 886\n",
      "---------\n",
      "Train loss: 0.04347308921361608\n",
      "Test loss: 0.5132623347971175\n",
      "\n",
      "Epoch 887\n",
      "---------\n",
      "Train loss: 0.039505397069400976\n",
      "Test loss: 0.5003971093230777\n",
      "\n",
      "Epoch 888\n",
      "---------\n",
      "Train loss: 0.03865617756465716\n",
      "Test loss: 0.49566757596201366\n",
      "\n",
      "Epoch 889\n",
      "---------\n",
      "Train loss: 0.039192604582889805\n",
      "Test loss: 0.4918239514033\n",
      "\n",
      "Epoch 890\n",
      "---------\n",
      "Train loss: 0.03885043395816216\n",
      "Test loss: 0.508564841416147\n",
      "\n",
      "Epoch 891\n",
      "---------\n",
      "Train loss: 0.0425641542782874\n",
      "Test loss: 0.5026394584112697\n",
      "\n",
      "Epoch 892\n",
      "---------\n",
      "Train loss: 0.04076521471142769\n",
      "Test loss: 0.4767650415500005\n",
      "\n",
      "Epoch 893\n",
      "---------\n",
      "Train loss: 0.040162104747391174\n",
      "Test loss: 0.5371544659137726\n",
      "\n",
      "Epoch 894\n",
      "---------\n",
      "Train loss: 0.04044146177225879\n",
      "Test loss: 0.5230418336060312\n",
      "\n",
      "Epoch 895\n",
      "---------\n",
      "Train loss: 0.0386693677922137\n",
      "Test loss: 0.5054055609636836\n",
      "\n",
      "Epoch 896\n",
      "---------\n",
      "Train loss: 0.038821923496600776\n",
      "Test loss: 0.4942619610163901\n",
      "\n",
      "Epoch 897\n",
      "---------\n",
      "Train loss: 0.0383401715828638\n",
      "Test loss: 0.5150678240590625\n",
      "\n",
      "Epoch 898\n",
      "---------\n",
      "Train loss: 0.03886992244848183\n",
      "Test loss: 0.49827813191546333\n",
      "\n",
      "Epoch 899\n",
      "---------\n",
      "Train loss: 0.039634156348516365\n",
      "Test loss: 0.5032439554731051\n",
      "\n",
      "Epoch 900\n",
      "---------\n",
      "Train loss: 0.040171831029389135\n",
      "Test loss: 0.515148779998223\n",
      "\n",
      "Epoch 901\n",
      "---------\n",
      "Train loss: 0.038540878549351225\n",
      "Test loss: 0.4934813835554653\n",
      "\n",
      "Epoch 902\n",
      "---------\n",
      "Train loss: 0.03930852051624762\n",
      "Test loss: 0.5079758829540677\n",
      "\n",
      "Epoch 903\n",
      "---------\n",
      "Train loss: 0.03770404245421689\n",
      "Test loss: 0.5209024829996957\n",
      "\n",
      "Epoch 904\n",
      "---------\n",
      "Train loss: 0.03879865809410278\n",
      "Test loss: 0.5072177482975854\n",
      "\n",
      "Epoch 905\n",
      "---------\n",
      "Train loss: 0.04022897083112704\n",
      "Test loss: 0.5014591837922732\n",
      "\n",
      "Epoch 906\n",
      "---------\n",
      "Train loss: 0.03762321214474339\n",
      "Test loss: 0.5012939750320382\n",
      "\n",
      "Epoch 907\n",
      "---------\n",
      "Train loss: 0.03927136499467971\n",
      "Test loss: 0.5092340774006314\n",
      "\n",
      "Epoch 908\n",
      "---------\n",
      "Train loss: 0.03848416263437165\n",
      "Test loss: 0.5147099134822687\n",
      "\n",
      "Epoch 909\n",
      "---------\n",
      "Train loss: 0.03749514356357914\n",
      "Test loss: 0.5016597898470031\n",
      "\n",
      "Epoch 910\n",
      "---------\n",
      "Train loss: 0.039951799553819\n",
      "Test loss: 0.5028742767042584\n",
      "\n",
      "Epoch 911\n",
      "---------\n",
      "Train loss: 0.03975861020652311\n",
      "Test loss: 0.5156675113572015\n",
      "\n",
      "Epoch 912\n",
      "---------\n",
      "Train loss: 0.0430096431435751\n",
      "Test loss: 0.46519612272580463\n",
      "\n",
      "Epoch 913\n",
      "---------\n",
      "Train loss: 0.04150441618237112\n",
      "Test loss: 0.5037272514568435\n",
      "\n",
      "Epoch 914\n",
      "---------\n",
      "Train loss: 0.03862093441836935\n",
      "Test loss: 0.50524584742056\n",
      "\n",
      "Epoch 915\n",
      "---------\n",
      "Train loss: 0.03753023865699236\n",
      "Test loss: 0.5203680189119445\n",
      "\n",
      "Epoch 916\n",
      "---------\n",
      "Train loss: 0.039302569132165184\n",
      "Test loss: 0.5167453255918291\n",
      "\n",
      "Epoch 917\n",
      "---------\n",
      "Train loss: 0.04011075225259576\n",
      "Test loss: 0.5025431683493985\n",
      "\n",
      "Epoch 918\n",
      "---------\n",
      "Train loss: 0.03853002726100385\n",
      "Test loss: 0.5098914057016373\n",
      "\n",
      "Epoch 919\n",
      "---------\n",
      "Train loss: 0.0395112289781017\n",
      "Test loss: 0.49920632276270127\n",
      "\n",
      "Epoch 920\n",
      "---------\n",
      "Train loss: 0.03726205389414515\n",
      "Test loss: 0.5163094368245866\n",
      "\n",
      "Epoch 921\n",
      "---------\n",
      "Train loss: 0.03682638130183997\n",
      "Test loss: 0.5169135630130768\n",
      "\n",
      "Epoch 922\n",
      "---------\n",
      "Train loss: 0.044030009116145914\n",
      "Test loss: 0.5144272107217047\n",
      "\n",
      "Epoch 923\n",
      "---------\n",
      "Train loss: 0.038974288485145996\n",
      "Test loss: 0.4862634026341968\n",
      "\n",
      "Epoch 924\n",
      "---------\n",
      "Train loss: 0.040787259382861\n",
      "Test loss: 0.4968487264381515\n",
      "\n",
      "Epoch 925\n",
      "---------\n",
      "Train loss: 0.03661755794850511\n",
      "Test loss: 0.5134222002493011\n",
      "\n",
      "Epoch 926\n",
      "---------\n",
      "Train loss: 0.03721029717209084\n",
      "Test loss: 0.5128737241029739\n",
      "\n",
      "Epoch 927\n",
      "---------\n",
      "Train loss: 0.037207135847503583\n",
      "Test loss: 0.4971219177047412\n",
      "\n",
      "Epoch 928\n",
      "---------\n",
      "Train loss: 0.03844867867883295\n",
      "Test loss: 0.5019717969828181\n",
      "\n",
      "Epoch 929\n",
      "---------\n",
      "Train loss: 0.042958791218032796\n",
      "Test loss: 0.5254809972312715\n",
      "\n",
      "Epoch 930\n",
      "---------\n",
      "Train loss: 0.03793105195342962\n",
      "Test loss: 0.5077674384746287\n",
      "\n",
      "Epoch 931\n",
      "---------\n",
      "Train loss: 0.039703221824830895\n",
      "Test loss: 0.49541900638076997\n",
      "\n",
      "Epoch 932\n",
      "---------\n",
      "Train loss: 0.03675508119548405\n",
      "Test loss: 0.4975437944134076\n",
      "\n",
      "Epoch 933\n",
      "---------\n",
      "Train loss: 0.036571028006229814\n",
      "Test loss: 0.5081169191333983\n",
      "\n",
      "Epoch 934\n",
      "---------\n",
      "Train loss: 0.036130813436882035\n",
      "Test loss: 0.5172575687368711\n",
      "\n",
      "Epoch 935\n",
      "---------\n",
      "Train loss: 0.0364965765121659\n",
      "Test loss: 0.5232705221407943\n",
      "\n",
      "Epoch 936\n",
      "---------\n",
      "Train loss: 0.03639782368346849\n",
      "Test loss: 0.5073676208655039\n",
      "\n",
      "Epoch 937\n",
      "---------\n",
      "Train loss: 0.0366499112985496\n",
      "Test loss: 0.517568998866611\n",
      "\n",
      "Epoch 938\n",
      "---------\n",
      "Train loss: 0.03637812953508858\n",
      "Test loss: 0.5012306107415093\n",
      "\n",
      "Epoch 939\n",
      "---------\n",
      "Train loss: 0.03588818683887699\n",
      "Test loss: 0.505534092999167\n",
      "\n",
      "Epoch 940\n",
      "---------\n",
      "Train loss: 0.03871622861229947\n",
      "Test loss: 0.5045923011170493\n",
      "\n",
      "Epoch 941\n",
      "---------\n",
      "Train loss: 0.04040252902112635\n",
      "Test loss: 0.5122057091858652\n",
      "\n",
      "Epoch 942\n",
      "---------\n",
      "Train loss: 0.03549479570106736\n",
      "Test loss: 0.5026330302158991\n",
      "\n",
      "Epoch 943\n",
      "---------\n",
      "Train loss: 0.03503860284425692\n",
      "Test loss: 0.5142708172400793\n",
      "\n",
      "Epoch 944\n",
      "---------\n",
      "Train loss: 0.03505065350327641\n",
      "Test loss: 0.515174402958817\n",
      "\n",
      "Epoch 945\n",
      "---------\n",
      "Train loss: 0.036249172940318077\n",
      "Test loss: 0.5118128657341003\n",
      "\n",
      "Epoch 946\n",
      "---------\n",
      "Train loss: 0.03568218714125188\n",
      "Test loss: 0.514954666296641\n",
      "\n",
      "Epoch 947\n",
      "---------\n",
      "Train loss: 0.03568281211690711\n",
      "Test loss: 0.5085050927268134\n",
      "\n",
      "Epoch 948\n",
      "---------\n",
      "Train loss: 0.03641862839400086\n",
      "Test loss: 0.5072132796049118\n",
      "\n",
      "Epoch 949\n",
      "---------\n",
      "Train loss: 0.03635496982107205\n",
      "Test loss: 0.4945767753654056\n",
      "\n",
      "Epoch 950\n",
      "---------\n",
      "Train loss: 0.036755339575133154\n",
      "Test loss: 0.5222972946034538\n",
      "\n",
      "Epoch 951\n",
      "---------\n",
      "Train loss: 0.03565223984021161\n",
      "Test loss: 0.5135134822792478\n",
      "\n",
      "Epoch 952\n",
      "---------\n",
      "Train loss: 0.035616511961312165\n",
      "Test loss: 0.4985865437322193\n",
      "\n",
      "Epoch 953\n",
      "---------\n",
      "Train loss: 0.03540637258317604\n",
      "Test loss: 0.5065790630049176\n",
      "\n",
      "Epoch 954\n",
      "---------\n",
      "Train loss: 0.0354253437503108\n",
      "Test loss: 0.5014245733618736\n",
      "\n",
      "Epoch 955\n",
      "---------\n",
      "Train loss: 0.03468077907538308\n",
      "Test loss: 0.514233717487918\n",
      "\n",
      "Epoch 956\n",
      "---------\n",
      "Train loss: 0.035491067924470245\n",
      "Test loss: 0.5063966777589586\n",
      "\n",
      "Epoch 957\n",
      "---------\n",
      "Train loss: 0.035237540161428375\n",
      "Test loss: 0.522543395558993\n",
      "\n",
      "Epoch 958\n",
      "---------\n",
      "Train loss: 0.035064458630846014\n",
      "Test loss: 0.50889124472936\n",
      "\n",
      "Epoch 959\n",
      "---------\n",
      "Train loss: 0.03543307000238981\n",
      "Test loss: 0.5053689305981001\n",
      "\n",
      "Epoch 960\n",
      "---------\n",
      "Train loss: 0.0346125275536906\n",
      "Test loss: 0.5143010078205003\n",
      "\n",
      "Epoch 961\n",
      "---------\n",
      "Train loss: 0.03570630230076079\n",
      "Test loss: 0.50001593430837\n",
      "\n",
      "Epoch 962\n",
      "---------\n",
      "Train loss: 0.04075353528605774\n",
      "Test loss: 0.5043587502506044\n",
      "\n",
      "Epoch 963\n",
      "---------\n",
      "Train loss: 0.03511822310143283\n",
      "Test loss: 0.5097195828954378\n",
      "\n",
      "Epoch 964\n",
      "---------\n",
      "Train loss: 0.03944129835248792\n",
      "Test loss: 0.5372466329071257\n",
      "\n",
      "Epoch 965\n",
      "---------\n",
      "Train loss: 0.03740831965114921\n",
      "Test loss: 0.4920327125324143\n",
      "\n",
      "Epoch 966\n",
      "---------\n",
      "Train loss: 0.03832909426585372\n",
      "Test loss: 0.530355648861991\n",
      "\n",
      "Epoch 967\n",
      "---------\n",
      "Train loss: 0.035961589742718\n",
      "Test loss: 0.522135959731208\n",
      "\n",
      "Epoch 968\n",
      "---------\n",
      "Train loss: 0.03432532534994997\n",
      "Test loss: 0.5302137070231967\n",
      "\n",
      "Epoch 969\n",
      "---------\n",
      "Train loss: 0.035761547391302884\n",
      "Test loss: 0.5235650866395898\n",
      "\n",
      "Epoch 970\n",
      "---------\n",
      "Train loss: 0.0369777794694528\n",
      "Test loss: 0.5179465413093567\n",
      "\n",
      "Epoch 971\n",
      "---------\n",
      "Train loss: 0.03514702202353094\n",
      "Test loss: 0.5186788096196122\n",
      "\n",
      "Epoch 972\n",
      "---------\n",
      "Train loss: 0.035430093453864435\n",
      "Test loss: 0.5192699829737345\n",
      "\n",
      "Epoch 973\n",
      "---------\n",
      "Train loss: 0.033177728262671735\n",
      "Test loss: 0.5137475389573309\n",
      "\n",
      "Epoch 974\n",
      "---------\n",
      "Train loss: 0.033706319263635924\n",
      "Test loss: 0.530234812034501\n",
      "\n",
      "Epoch 975\n",
      "---------\n",
      "Train loss: 0.034039905336352864\n",
      "Test loss: 0.5041584885782666\n",
      "\n",
      "Epoch 976\n",
      "---------\n",
      "Train loss: 0.034182484444629936\n",
      "Test loss: 0.5216215828226672\n",
      "\n",
      "Epoch 977\n",
      "---------\n",
      "Train loss: 0.03390367705807356\n",
      "Test loss: 0.51356247978078\n",
      "\n",
      "Epoch 978\n",
      "---------\n",
      "Train loss: 0.03391873811779078\n",
      "Test loss: 0.524093988041083\n",
      "\n",
      "Epoch 979\n",
      "---------\n",
      "Train loss: 0.03307222115108743\n",
      "Test loss: 0.5282045520014234\n",
      "\n",
      "Epoch 980\n",
      "---------\n",
      "Train loss: 0.03598362573289445\n",
      "Test loss: 0.5183414539529217\n",
      "\n",
      "Epoch 981\n",
      "---------\n",
      "Train loss: 0.038069722458853254\n",
      "Test loss: 0.5137216730250252\n",
      "\n",
      "Epoch 982\n",
      "---------\n",
      "Train loss: 0.03487998897409333\n",
      "Test loss: 0.5139067620038986\n",
      "\n",
      "Epoch 983\n",
      "---------\n",
      "Train loss: 0.03447382157069764\n",
      "Test loss: 0.5146951989995109\n",
      "\n",
      "Epoch 984\n",
      "---------\n",
      "Train loss: 0.03337632749961423\n",
      "Test loss: 0.5063002109527588\n",
      "\n",
      "Epoch 985\n",
      "---------\n",
      "Train loss: 0.03710693367923211\n",
      "Test loss: 0.5081554162833426\n",
      "\n",
      "Epoch 986\n",
      "---------\n",
      "Train loss: 0.03719806928919362\n",
      "Test loss: 0.5015165954828262\n",
      "\n",
      "Epoch 987\n",
      "---------\n",
      "Train loss: 0.039044150816542764\n",
      "Test loss: 0.5134554215603404\n",
      "\n",
      "Epoch 988\n",
      "---------\n",
      "Train loss: 0.033616464115246866\n",
      "Test loss: 0.5079517190655073\n",
      "\n",
      "Epoch 989\n",
      "---------\n",
      "Train loss: 0.03294095789481487\n",
      "Test loss: 0.5182075177629789\n",
      "\n",
      "Epoch 990\n",
      "---------\n",
      "Train loss: 0.04030487650951337\n",
      "Test loss: 0.5000685254732767\n",
      "\n",
      "Epoch 991\n",
      "---------\n",
      "Train loss: 0.03360626198783783\n",
      "Test loss: 0.5229754928085539\n",
      "\n",
      "Epoch 992\n",
      "---------\n",
      "Train loss: 0.032771490685783125\n",
      "Test loss: 0.5150527507066727\n",
      "\n",
      "Epoch 993\n",
      "---------\n",
      "Train loss: 0.03340194740199617\n",
      "Test loss: 0.5100586273603969\n",
      "\n",
      "Epoch 994\n",
      "---------\n",
      "Train loss: 0.03457454014902136\n",
      "Test loss: 0.49894384874237907\n",
      "\n",
      "Epoch 995\n",
      "---------\n",
      "Train loss: 0.03198660707234272\n",
      "Test loss: 0.5250346495045556\n",
      "\n",
      "Epoch 996\n",
      "---------\n",
      "Train loss: 0.0332385694491677\n",
      "Test loss: 0.5097355047861735\n",
      "\n",
      "Epoch 997\n",
      "---------\n",
      "Train loss: 0.033054116797367375\n",
      "Test loss: 0.5191205317775408\n",
      "\n",
      "Epoch 998\n",
      "---------\n",
      "Train loss: 0.035461491622429876\n",
      "Test loss: 0.5120621066954401\n",
      "\n",
      "Epoch 999\n",
      "---------\n",
      "Train loss: 0.04022505834499108\n",
      "Test loss: 0.5058704912662506\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lead1  Model forecast\n",
      "Month                            \n",
      "1949-01-01  118.0      117.441498\n",
      "1949-02-01  132.0      124.121620\n",
      "1949-03-01  129.0      136.214172\n",
      "1949-04-01  121.0      128.806946\n",
      "1949-05-01  135.0      123.692329\n",
      "...           ...             ...\n",
      "1960-07-01  606.0      576.453979\n",
      "1960-08-01  508.0      566.964783\n",
      "1960-09-01  461.0      411.681702\n",
      "1960-10-01  390.0      391.551575\n",
      "1960-11-01  432.0      341.564331\n",
      "\n",
      "[144 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=lead1<br>Date=%{x}<br>Air Passengers=%{y}<extra></extra>",
         "legendgroup": "lead1",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "lead1",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "1949-01-01T00:00:00",
          "1949-02-01T00:00:00",
          "1949-03-01T00:00:00",
          "1949-04-01T00:00:00",
          "1949-05-01T00:00:00",
          "1949-06-01T00:00:00",
          "1949-07-01T00:00:00",
          "1949-08-01T00:00:00",
          "1949-09-01T00:00:00",
          "1949-10-01T00:00:00",
          "1949-11-01T00:00:00",
          "1949-12-01T00:00:00",
          "1950-01-01T00:00:00",
          "1950-02-01T00:00:00",
          "1950-03-01T00:00:00",
          "1950-04-01T00:00:00",
          "1950-05-01T00:00:00",
          "1950-06-01T00:00:00",
          "1950-07-01T00:00:00",
          "1950-08-01T00:00:00",
          "1950-09-01T00:00:00",
          "1950-10-01T00:00:00",
          "1950-11-01T00:00:00",
          "1950-12-01T00:00:00",
          "1951-01-01T00:00:00",
          "1951-02-01T00:00:00",
          "1951-03-01T00:00:00",
          "1951-04-01T00:00:00",
          "1951-05-01T00:00:00",
          "1951-06-01T00:00:00",
          "1951-07-01T00:00:00",
          "1951-08-01T00:00:00",
          "1951-09-01T00:00:00",
          "1951-10-01T00:00:00",
          "1951-11-01T00:00:00",
          "1951-12-01T00:00:00",
          "1952-01-01T00:00:00",
          "1952-02-01T00:00:00",
          "1952-03-01T00:00:00",
          "1952-04-01T00:00:00",
          "1952-05-01T00:00:00",
          "1952-06-01T00:00:00",
          "1952-07-01T00:00:00",
          "1952-08-01T00:00:00",
          "1952-09-01T00:00:00",
          "1952-10-01T00:00:00",
          "1952-11-01T00:00:00",
          "1952-12-01T00:00:00",
          "1953-01-01T00:00:00",
          "1953-02-01T00:00:00",
          "1953-03-01T00:00:00",
          "1953-04-01T00:00:00",
          "1953-05-01T00:00:00",
          "1953-06-01T00:00:00",
          "1953-07-01T00:00:00",
          "1953-08-01T00:00:00",
          "1953-09-01T00:00:00",
          "1953-10-01T00:00:00",
          "1953-11-01T00:00:00",
          "1953-12-01T00:00:00",
          "1954-01-01T00:00:00",
          "1954-02-01T00:00:00",
          "1954-03-01T00:00:00",
          "1954-04-01T00:00:00",
          "1954-05-01T00:00:00",
          "1954-06-01T00:00:00",
          "1954-07-01T00:00:00",
          "1954-08-01T00:00:00",
          "1954-09-01T00:00:00",
          "1954-10-01T00:00:00",
          "1954-11-01T00:00:00",
          "1954-12-01T00:00:00",
          "1955-01-01T00:00:00",
          "1955-02-01T00:00:00",
          "1955-03-01T00:00:00",
          "1955-04-01T00:00:00",
          "1955-05-01T00:00:00",
          "1955-06-01T00:00:00",
          "1955-07-01T00:00:00",
          "1955-08-01T00:00:00",
          "1955-09-01T00:00:00",
          "1955-10-01T00:00:00",
          "1955-11-01T00:00:00",
          "1955-12-01T00:00:00",
          "1956-01-01T00:00:00",
          "1956-02-01T00:00:00",
          "1956-03-01T00:00:00",
          "1956-04-01T00:00:00",
          "1956-05-01T00:00:00",
          "1956-06-01T00:00:00",
          "1956-07-01T00:00:00",
          "1956-08-01T00:00:00",
          "1956-09-01T00:00:00",
          "1956-10-01T00:00:00",
          "1956-11-01T00:00:00",
          "1956-12-01T00:00:00",
          "1957-01-01T00:00:00",
          "1957-02-01T00:00:00",
          "1957-03-01T00:00:00",
          "1957-04-01T00:00:00",
          "1957-05-01T00:00:00",
          "1957-06-01T00:00:00",
          "1957-07-01T00:00:00",
          "1957-08-01T00:00:00",
          "1957-09-01T00:00:00",
          "1957-10-01T00:00:00",
          "1957-11-01T00:00:00",
          "1957-12-01T00:00:00",
          "1958-01-01T00:00:00",
          "1958-01-01T00:00:00",
          "1958-02-01T00:00:00",
          "1958-03-01T00:00:00",
          "1958-04-01T00:00:00",
          "1958-05-01T00:00:00",
          "1958-06-01T00:00:00",
          "1958-07-01T00:00:00",
          "1958-08-01T00:00:00",
          "1958-09-01T00:00:00",
          "1958-10-01T00:00:00",
          "1958-11-01T00:00:00",
          "1958-12-01T00:00:00",
          "1959-01-01T00:00:00",
          "1959-02-01T00:00:00",
          "1959-03-01T00:00:00",
          "1959-04-01T00:00:00",
          "1959-05-01T00:00:00",
          "1959-06-01T00:00:00",
          "1959-07-01T00:00:00",
          "1959-08-01T00:00:00",
          "1959-09-01T00:00:00",
          "1959-10-01T00:00:00",
          "1959-11-01T00:00:00",
          "1959-12-01T00:00:00",
          "1960-01-01T00:00:00",
          "1960-02-01T00:00:00",
          "1960-03-01T00:00:00",
          "1960-04-01T00:00:00",
          "1960-05-01T00:00:00",
          "1960-06-01T00:00:00",
          "1960-07-01T00:00:00",
          "1960-08-01T00:00:00",
          "1960-09-01T00:00:00",
          "1960-10-01T00:00:00",
          "1960-11-01T00:00:00"
         ],
         "xaxis": "x",
         "y": [
          118,
          132,
          129,
          121,
          135,
          148,
          148,
          136,
          119,
          104,
          118,
          115,
          126,
          141,
          135,
          125,
          149,
          170,
          170,
          158,
          133,
          114,
          140,
          145,
          150,
          178,
          163,
          172,
          178,
          199,
          199,
          184,
          162,
          146,
          166,
          171,
          180,
          193,
          181,
          183,
          218,
          230,
          242,
          209,
          191,
          172,
          194,
          196,
          196,
          236,
          235,
          229,
          243,
          264,
          272,
          237,
          211,
          180,
          201,
          204,
          188,
          235,
          227,
          234,
          264,
          302,
          293,
          259,
          229,
          203,
          229,
          242,
          233,
          267,
          269,
          270,
          315,
          364,
          347,
          312,
          274,
          237,
          278,
          284,
          277,
          317,
          313,
          318,
          374,
          413,
          405,
          355,
          306,
          271,
          306,
          315,
          301,
          356,
          348,
          355,
          422,
          465,
          467,
          404,
          347,
          305,
          336,
          340,
          318,
          318,
          362,
          348,
          363,
          435,
          491,
          505,
          404,
          359,
          310,
          337,
          360,
          342,
          406,
          396,
          420,
          472,
          548,
          559,
          463,
          407,
          362,
          405,
          417,
          391,
          419,
          461,
          472,
          535,
          622,
          606,
          508,
          461,
          390,
          432
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Model forecast<br>Date=%{x}<br>Air Passengers=%{y}<extra></extra>",
         "legendgroup": "Model forecast",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Model forecast",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          "1949-01-01T00:00:00",
          "1949-02-01T00:00:00",
          "1949-03-01T00:00:00",
          "1949-04-01T00:00:00",
          "1949-05-01T00:00:00",
          "1949-06-01T00:00:00",
          "1949-07-01T00:00:00",
          "1949-08-01T00:00:00",
          "1949-09-01T00:00:00",
          "1949-10-01T00:00:00",
          "1949-11-01T00:00:00",
          "1949-12-01T00:00:00",
          "1950-01-01T00:00:00",
          "1950-02-01T00:00:00",
          "1950-03-01T00:00:00",
          "1950-04-01T00:00:00",
          "1950-05-01T00:00:00",
          "1950-06-01T00:00:00",
          "1950-07-01T00:00:00",
          "1950-08-01T00:00:00",
          "1950-09-01T00:00:00",
          "1950-10-01T00:00:00",
          "1950-11-01T00:00:00",
          "1950-12-01T00:00:00",
          "1951-01-01T00:00:00",
          "1951-02-01T00:00:00",
          "1951-03-01T00:00:00",
          "1951-04-01T00:00:00",
          "1951-05-01T00:00:00",
          "1951-06-01T00:00:00",
          "1951-07-01T00:00:00",
          "1951-08-01T00:00:00",
          "1951-09-01T00:00:00",
          "1951-10-01T00:00:00",
          "1951-11-01T00:00:00",
          "1951-12-01T00:00:00",
          "1952-01-01T00:00:00",
          "1952-02-01T00:00:00",
          "1952-03-01T00:00:00",
          "1952-04-01T00:00:00",
          "1952-05-01T00:00:00",
          "1952-06-01T00:00:00",
          "1952-07-01T00:00:00",
          "1952-08-01T00:00:00",
          "1952-09-01T00:00:00",
          "1952-10-01T00:00:00",
          "1952-11-01T00:00:00",
          "1952-12-01T00:00:00",
          "1953-01-01T00:00:00",
          "1953-02-01T00:00:00",
          "1953-03-01T00:00:00",
          "1953-04-01T00:00:00",
          "1953-05-01T00:00:00",
          "1953-06-01T00:00:00",
          "1953-07-01T00:00:00",
          "1953-08-01T00:00:00",
          "1953-09-01T00:00:00",
          "1953-10-01T00:00:00",
          "1953-11-01T00:00:00",
          "1953-12-01T00:00:00",
          "1954-01-01T00:00:00",
          "1954-02-01T00:00:00",
          "1954-03-01T00:00:00",
          "1954-04-01T00:00:00",
          "1954-05-01T00:00:00",
          "1954-06-01T00:00:00",
          "1954-07-01T00:00:00",
          "1954-08-01T00:00:00",
          "1954-09-01T00:00:00",
          "1954-10-01T00:00:00",
          "1954-11-01T00:00:00",
          "1954-12-01T00:00:00",
          "1955-01-01T00:00:00",
          "1955-02-01T00:00:00",
          "1955-03-01T00:00:00",
          "1955-04-01T00:00:00",
          "1955-05-01T00:00:00",
          "1955-06-01T00:00:00",
          "1955-07-01T00:00:00",
          "1955-08-01T00:00:00",
          "1955-09-01T00:00:00",
          "1955-10-01T00:00:00",
          "1955-11-01T00:00:00",
          "1955-12-01T00:00:00",
          "1956-01-01T00:00:00",
          "1956-02-01T00:00:00",
          "1956-03-01T00:00:00",
          "1956-04-01T00:00:00",
          "1956-05-01T00:00:00",
          "1956-06-01T00:00:00",
          "1956-07-01T00:00:00",
          "1956-08-01T00:00:00",
          "1956-09-01T00:00:00",
          "1956-10-01T00:00:00",
          "1956-11-01T00:00:00",
          "1956-12-01T00:00:00",
          "1957-01-01T00:00:00",
          "1957-02-01T00:00:00",
          "1957-03-01T00:00:00",
          "1957-04-01T00:00:00",
          "1957-05-01T00:00:00",
          "1957-06-01T00:00:00",
          "1957-07-01T00:00:00",
          "1957-08-01T00:00:00",
          "1957-09-01T00:00:00",
          "1957-10-01T00:00:00",
          "1957-11-01T00:00:00",
          "1957-12-01T00:00:00",
          "1958-01-01T00:00:00",
          "1958-01-01T00:00:00",
          "1958-02-01T00:00:00",
          "1958-03-01T00:00:00",
          "1958-04-01T00:00:00",
          "1958-05-01T00:00:00",
          "1958-06-01T00:00:00",
          "1958-07-01T00:00:00",
          "1958-08-01T00:00:00",
          "1958-09-01T00:00:00",
          "1958-10-01T00:00:00",
          "1958-11-01T00:00:00",
          "1958-12-01T00:00:00",
          "1959-01-01T00:00:00",
          "1959-02-01T00:00:00",
          "1959-03-01T00:00:00",
          "1959-04-01T00:00:00",
          "1959-05-01T00:00:00",
          "1959-06-01T00:00:00",
          "1959-07-01T00:00:00",
          "1959-08-01T00:00:00",
          "1959-09-01T00:00:00",
          "1959-10-01T00:00:00",
          "1959-11-01T00:00:00",
          "1959-12-01T00:00:00",
          "1960-01-01T00:00:00",
          "1960-02-01T00:00:00",
          "1960-03-01T00:00:00",
          "1960-04-01T00:00:00",
          "1960-05-01T00:00:00",
          "1960-06-01T00:00:00",
          "1960-07-01T00:00:00",
          "1960-08-01T00:00:00",
          "1960-09-01T00:00:00",
          "1960-10-01T00:00:00",
          "1960-11-01T00:00:00"
         ],
         "xaxis": "x",
         "y": [
          117.44149780273438,
          124.12162017822266,
          136.21417236328125,
          128.80694580078125,
          123.69232940673828,
          141.45639038085938,
          148.3125,
          146.9440460205078,
          135.64556884765625,
          120.61341094970703,
          108.4957275390625,
          127.65058135986328,
          116.60395050048828,
          132.3543701171875,
          143.04623413085938,
          132.9244384765625,
          127.57423400878906,
          153.84225463867188,
          165.02239990234375,
          166.23114013671875,
          158.71852111816406,
          130.5883026123047,
          119.58171081542969,
          147.40440368652344,
          142.480224609375,
          151.55374145507812,
          175.54718017578125,
          153.84725952148438,
          179.9998779296875,
          178.62454223632812,
          199.51947021484375,
          201.7599334716797,
          192.27587890625,
          168.5884246826172,
          153.4987335205078,
          171.70919799804688,
          167.64108276367188,
          181.81077575683594,
          194.45645141601562,
          182.4192657470703,
          193.99267578125,
          215.89031982421875,
          230.59872436523438,
          253.7277374267578,
          207.47962951660156,
          201.7806854248047,
          184.33070373535156,
          201.865478515625,
          196.68814086914062,
          205.8824005126953,
          233.8423614501953,
          239.79415893554688,
          249.9412384033203,
          265.025146484375,
          273.4342041015625,
          277.03704833984375,
          222.01332092285156,
          188.62855529785156,
          193.61715698242188,
          210.47535705566406,
          207.454833984375,
          196.17984008789062,
          231.97996520996094,
          230.30238342285156,
          257.7080993652344,
          272.63250732421875,
          291.80859375,
          256.66094970703125,
          235.06565856933594,
          225.493408203125,
          202.69631958007812,
          238.30442810058594,
          247.4505615234375,
          243.64891052246094,
          284.7672424316406,
          265.89117431640625,
          318.8111877441406,
          356.5858459472656,
          347.5843811035156,
          318.0451965332031,
          281.9419250488281,
          262.8641357421875,
          250.6491241455078,
          275.541259765625,
          272.7289123535156,
          309.74176025390625,
          336.2362976074219,
          311.9833984375,
          367.35955810546875,
          410.23614501953125,
          408.6163330078125,
          357.8386535644531,
          308.40240478515625,
          287.12640380859375,
          307.9468688964844,
          314.715087890625,
          318.49200439453125,
          339.5742492675781,
          349.27093505859375,
          336.33203125,
          417.044677734375,
          478.81109619140625,
          472.0125732421875,
          406.74200439453125,
          349.4713134765625,
          306.5686340332031,
          338.216552734375,
          344.548095703125,
          360.0658264160156,
          388.46978759765625,
          325.46185302734375,
          392.51348876953125,
          323.1169128417969,
          446.80364990234375,
          477.61895751953125,
          502.430908203125,
          443.55255126953125,
          341.7236022949219,
          332.12493896484375,
          276.19805908203125,
          367.46807861328125,
          389.6539001464844,
          312.5661315917969,
          506.00341796875,
          353.01617431640625,
          425.01580810546875,
          470.6822509765625,
          530.555419921875,
          515.6700439453125,
          385.11077880859375,
          362.1663818359375,
          327.35858154296875,
          544.4492797851562,
          390.395751953125,
          347.2474365234375,
          448.6592102050781,
          452.4351806640625,
          414.97381591796875,
          496.02935791015625,
          576.4539794921875,
          566.9647827148438,
          411.68170166015625,
          391.55157470703125,
          341.5643310546875
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "Test set start",
          "x": 0.75,
          "xref": "paper",
          "y": 0.8,
          "yref": "paper"
         }
        ],
        "legend": {
         "orientation": "h",
         "title": {
          "text": ""
         },
         "tracegroupgap": 0,
         "y": 1.02
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "dash": "dash",
           "width": 4
          },
          "type": "line",
          "x0": "1958-01-01",
          "x1": "1958-01-01",
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "font": {
           "size": 18
          },
          "xaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          },
          "yaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Air Passengers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Create the datasets and data loaders\n",
    "\n",
    "# %%\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(101)\n",
    "\n",
    "batch_size = 4\n",
    "sequence_length = 3\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test,\n",
    "    target=target,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# # The model and learning algorithm\n",
    "\n",
    "# %%\n",
    "from torch import nn\n",
    "\n",
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super().__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# %%\n",
    "learning_rate = 1e-3\n",
    "num_hidden_units = 32\n",
    "\n",
    "model = ShallowRegressionLSTM(hidden_units=num_hidden_units)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train\n",
    "\n",
    "# %%\n",
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "# Store losses per epoch\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"Untrained test\\n--------\")\n",
    "initial_test_loss = test_model(test_loader, model, loss_function)\n",
    "test_losses.append(initial_test_loss)\n",
    "print()\n",
    "\n",
    "for ix_epoch in range(1000):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    test_loss = test_model(test_loader, model, loss_function)\n",
    "    test_losses.append(test_loss)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Plot loss per epoch\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Testing loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Evaluation\n",
    "\n",
    "# %%\n",
    "def predict(data_loader, model):\n",
    "    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n",
    "    function.\n",
    "    \"\"\"\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# %%\n",
    "train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ystar_col = \"Model forecast\"\n",
    "df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n",
    "df_test[ystar_col] = predict(test_loader, model).numpy()\n",
    "\n",
    "df_out = pd.concat((df_train, df_test))[[target, ystar_col]]\n",
    "\n",
    "for c in df_out.columns:\n",
    "    df_out[c] = df_out[c] * target_stdev + target_mean\n",
    "\n",
    "print(df_out)\n",
    "\n",
    "# %%\n",
    "fig = px.line(df_out, labels={'value': \"Air Passengers\", 'Month': 'Date'})\n",
    "fig.add_vline(x=test_start, line_width=4, line_dash=\"dash\")\n",
    "fig.add_annotation(xref=\"paper\", x=0.75, yref=\"paper\", y=0.8, text=\"Test set start\", showarrow=False)\n",
    "fig.update_layout(\n",
    "  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\"air_passengers_forecast.png\", width=1200, height=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.11.2)",
   "language": "python",
   "name": "3.11.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
