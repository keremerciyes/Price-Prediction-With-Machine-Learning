{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('data-sets/air_passengers.csv')\n",
    "\n",
    "# Create lagged features\n",
    "for i in range(1, 3):\n",
    "    df[f'lag_{i}'] = df['Passengers'].shift(i)\n",
    "\n",
    "# Drop NA values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define inputs and target\n",
    "X = df[['lag_1', 'lag_2']].values\n",
    "y = df['Passengers'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)  # 0.2 x 0.8 = 0.16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=2, num_layers=1, lr=0.1, weight_decay=0.01):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq) ,1, -1))\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_lightning' has no attribute 'LightningModule'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cemer\\OneDrive\\Belgeler\\GitHub\\price-prediction-with-machine-learning\\mlp_bayes.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cemer/OneDrive/Belgeler/GitHub/price-prediction-with-machine-learning/mlp_bayes.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m weight_decay \u001b[39min\u001b[39;00m weight_decays:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cemer/OneDrive/Belgeler/GitHub/price-prediction-with-machine-learning/mlp_bayes.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     model \u001b[39m=\u001b[39m LSTM(hidden_layer_size\u001b[39m=\u001b[39mhidden_size, num_layers\u001b[39m=\u001b[39mlayer, lr\u001b[39m=\u001b[39mlr, weight_decay\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cemer/OneDrive/Belgeler/GitHub/price-prediction-with-machine-learning/mlp_bayes.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(max_epochs\u001b[39m=\u001b[39;49mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cemer/OneDrive/Belgeler/GitHub/price-prediction-with-machine-learning/mlp_bayes.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     trainer\u001b[39m.\u001b[39mfit(model, train_loader, val_loader)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\utilities\\argparse.py:69\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m     68\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\trainer.py:421\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccumulate_grad_batches \u001b[39m=\u001b[39m accumulate_grad_batches\n\u001b[0;32m    419\u001b[0m \u001b[39m# init callbacks\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39m# Declare attributes to be set in _callback_connector on_trainer_init\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callback_connector\u001b[39m.\u001b[39;49mon_trainer_init(\n\u001b[0;32m    422\u001b[0m     callbacks,\n\u001b[0;32m    423\u001b[0m     enable_checkpointing,\n\u001b[0;32m    424\u001b[0m     enable_progress_bar,\n\u001b[0;32m    425\u001b[0m     default_root_dir,\n\u001b[0;32m    426\u001b[0m     enable_model_summary,\n\u001b[0;32m    427\u001b[0m     max_time,\n\u001b[0;32m    428\u001b[0m )\n\u001b[0;32m    430\u001b[0m \u001b[39m# init data flags\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_val_every_n_epoch: Optional[\u001b[39mint\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:79\u001b[0m, in \u001b[0;36m_CallbackConnector.on_trainer_init\u001b[1;34m(self, callbacks, enable_checkpointing, enable_progress_bar, default_root_dir, enable_model_summary, max_time)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_model_summary_callback(enable_model_summary)\n\u001b[0;32m     78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mextend(_configure_external_callbacks())\n\u001b[1;32m---> 79\u001b[0m _validate_callbacks_list(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcallbacks)\n\u001b[0;32m     81\u001b[0m \u001b[39m# push all model checkpoint callbacks to the end\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m# it is important that these are the last callbacks to run\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reorder_callbacks(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcallbacks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:252\u001b[0m, in \u001b[0;36m_validate_callbacks_list\u001b[1;34m(callbacks)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_callbacks_list\u001b[39m(callbacks: List[Callback]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     stateful_callbacks \u001b[39m=\u001b[39m [cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks \u001b[39mif\u001b[39;00m is_overridden(\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m, instance\u001b[39m=\u001b[39mcb)]\n\u001b[0;32m    253\u001b[0m     seen_callbacks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    254\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_callbacks_list\u001b[39m(callbacks: List[Callback]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     stateful_callbacks \u001b[39m=\u001b[39m [cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks \u001b[39mif\u001b[39;00m is_overridden(\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m, instance\u001b[39m=\u001b[39;49mcb)]\n\u001b[0;32m    253\u001b[0m     seen_callbacks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    254\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m stateful_callbacks:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:27\u001b[0m, in \u001b[0;36mis_overridden\u001b[1;34m(method_name, instance, parent)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m parent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(instance, pl\u001b[39m.\u001b[39;49mLightningModule):\n\u001b[0;32m     28\u001b[0m         parent \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mLightningModule\n\u001b[0;32m     29\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(instance, pl\u001b[39m.\u001b[39mLightningDataModule):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pytorch_lightning' has no attribute 'LightningModule'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters for grid search\n",
    "learning_rates = [0.01, 0.1]\n",
    "epochs = [50, 100]\n",
    "hidden_sizes = [50, 100]\n",
    "layers = [1, 2]\n",
    "weight_decays = [1e-3, 1e-2]\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Preparing data loaders\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "val_dataset = TensorDataset(torch.Tensor(X_val), torch.Tensor(y_val))\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for epoch in epochs:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            for layer in layers:\n",
    "                for weight_decay in weight_decays:\n",
    "                    model = LSTM(hidden_layer_size=hidden_size, num_layers=layer, lr=lr, weight_decay=weight_decay)\n",
    "                    trainer = Trainer(max_epochs=epoch)\n",
    "                    trainer.fit(model, train_loader, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
